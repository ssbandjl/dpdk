源码分析-大页初始化: https://zzqcn.github.io/opensource/dpdk/code-analysis/mem.html
UIO原理和流程简析: https://blog.csdn.net/ApeLife/article/details/100751359

git remote add upstream https://github.com/DPDK/dpdk
git fetch upstream
git merge upstream/master



mlx5driver:
driver source code: https://blog.csdn.net/leiyanjie8995/article/details/121341828
struct rte_eth_dev { -> 与每个以太网设备关联的通用数据结构。 指向突发数据包接收和发送功能的指针位于该结构的开头，以及指向特定设备的所有数据元素存储在共享内存中的位置的指针。 这种分割允许每个进程使用函数指针和驱动程序数据，而设备的实际配置数据是共享的。

与框架相关的比较重要的，收发报文的接口是 rx_pkt_burst 和 tx_pkt_burst, 还有与网卡相关的初始化、配置等接口都在eth_dev_ops里。还有网卡设备的私有数据，带有硬件相关的各项参数和数据，记录在rte_eth_dev_data结构里，包括网卡名称、收发队列个数及列表、mac地址等等
值得注意的是，为了representor的概念，mellanox在rte_eth_dev_data结构里添加了一个名为representor_id的参数，用作representor设备的id
mellanox的驱动在drivers/common/mlx5和drivers/net/mlx5目录下。common目录下是通用pcie相关，包括pcie驱动、与硬件交互的接口封装；net目录下是更上层的接口，包括eth设备、representor相关的一系列操作






LPM, 最长掩码匹配, Longest Prefix matching: lib\lpm\rte_lpm.h
struct rte_lpm_tbl_entry

rte_lpm_create(const char *name, int socket_id, const struct rte_lpm_config *config)




rte_distributor_process -> 处理一组数据包并将其分发给worker




ptp presision timing protocol


查看大页: cat /proc/meminfo |grep -i huge

配置大页:
echo 16 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
hugepage:
大页信息初始化
eal_hugepage_info_init(void) -> 当我们初始化大页信息时，默认情况下所有内容都会转到套接字 0。 稍后将按内存初始化过程排序. mem：共享主要和次要的大页信息，因为我们需要在主要和辅助进程中映射大页，所以我们需要知道应该在哪里寻找hugetlbfs挂载点。 因此，与辅助进程共享这些，并将它们映射到 init 上
    eal_get_internal_configuration
    hugepage_info_init
        const char dirent_start_text[] = "hugepages-";
        dir = opendir(sys_dir_path) -> static const char sys_dir_path[] = "/sys/kernel/mm/hugepages";
        for (dirent = readdir(dir); dirent != NULL; dirent = readdir(dir))
            rte_str_to_size(&dirent->d_name[dirent_start_len]) -> 大页目录名转整数
            get_hugepage_dir
            get_num_hugepages
            calc_num_pages
    create_shared_memory(eal_hugepage_info_path(),
    memcpy
    munmap

读取 /sys/kernel/mm/hugepages 中的“hugepages-XXX”目录，最多读取 3个。比如读取到 hugepages-2048kB ，将其中的 2048kB 转换为2048*1024，存入internal_config.hugepage_info[num_sizes].hugepage_sz， num_sizes<3
打开 /proc/meminfo 文件，读取 Hugepagesize 项的值，做为大页默认大小。打开 /proc/mounts 文件，找到类似 hugetlbfs /dev/hugepages hugetlbfs rw,seclabel,relatime 0 0 或 nodev /mnt/huge hugetlbfs rw,relatime 0 0 的行，根据选项(rw,relatime)中出现的 pagesize= 项的值(如果有的话)，来返回对应的大页文件系统挂载路径，如 /dev/hugepages 或 /mnt/huge ，将其存入internal_config.hugepage_info[num_sizes].hugedir
锁定hugedir(flock)
打开 sys/kernel/mm/hugepages/hugepages-XXX 目录下面的 resv_hugepages 和 free_hugepages 文件，计算可用大页数量， 存入internal_config.hugepange_info->num_pages[0]，这个0是socket id，在支持NUMA的系统中先在socket 0上进行操作
internal_config.num_hugepage_sizes数设置为num_sizes数，不大于3
将上述过程发现的所有num_sizes个大页信息按从大到小排序，并检查至少有一个可用大页尺寸

get and req hugepage: dpdk-hugepages.py --setup 2G


大页内存初始化
rte_eal_memory_init
    rte_eal_memseg_init
    eal_memalloc_init
    rte_eal_hugepage_init rte_eal_hugepage_attach
        map_all_hugepages
            eal_get_hugefile_path(hf->filepath, sizeof(hf->filepath), -> // 拼接文件名 /dev/hugepages/rte_hugepage_%s
            ...
            fd = open(hf->filepath, O_CREAT | O_RDWR, 0600);
            ...
            virtaddr = mmap(NULL, hugepage_sz, PROT_READ | PROT_WRITE,
                            MAP_SHARED | MAP_POPULATE, fd, 0);



receive pkt:
rte_eth_rx_burst
    eth_igb_recv_pkts
        if (! (staterr & rte_cpu_to_le_32(E1000_RXD_STAT_DD))) -> check dma dd flag



send pkt:
rte_eth_tx_burst
    tx_queues -> rte_eth_tx_burst -> nb_pkts = p->tx_pkt_burst(qd, tx_pkts, nb_pkts)




how huge page init?


//主进程创建/var/run/.rte_config文件
mem_cfg_fd = open(pathname, O_RDWR | O_CREAT, 0660);
//主进程映射/var/run/.rte_config到主进程空间
rte_mem_cfg_addr = mmap(rte_mem_cfg_addr, sizeof(*rte_config.mem_config),PROT_READ | PROT_WRITE, MAP_SHARED, mem_cfg_fd, 0);

static void rte_config_init(void)
{
    switch (rte_config.process_type)
    {
        case RTE_PROC_PRIMARY:
             //主进程创建共享内存配置
            rte_eal_config_create();
            break;
        case RTE_PROC_SECONDARY:
            //从进程打开共享内存配置后，映射到从进程自己的地址空间
            rte_eal_config_attach();
            //睡眠等待主进程设置完成共享内存配置
            rte_eal_mcfg_wait_complete(rte_config.mem_config);
            //从进程重新映射共享内存配置
            rte_eal_config_reattach();
    }
}

//从进程打开/var/run/.rte_config文件
mem_cfg_fd = open(pathname, O_RDWR);
//从进程将/var/run/.rte_config文件内容映射到从进程空间
rte_mem_cfg_addr = mmap(NULL, sizeof(*rte_config.mem_config),PROT_READ | PROT_WRITE, MAP_SHARED, mem_cfg_fd, 0

思考个问题，dpdk如何保证在主从进程模式下，物理地址相同，对应的主从进程的虚拟地址也相同呢？答案是主进程mmap映射后，主进程会将mmap映射后的虚拟地址放到共享内存中rte_config.mem_config。从进程会进行2次共享内存映射，第一次调用mmap进行映射时，第一个参数为空，表示由内核选择一个虚拟地址空间，从进程将会映射到这个由内核选择的虚拟地址空间中，此时从进程就可以从共享内存中获取到主进程mmap后的虚拟地址。之后从进程第二次调用mmap进行映射，传递的第一个参数不为空了，而是主进程mmap映射后的虚拟地址，相当于从进程直接从这个虚拟地址开始映射，从而保证了主从进程的虚拟地址空间一样，对应的物理空间也一样。主从进程的映射逻辑，都在rte_config_init函数中
原文链接：https://blog.csdn.net/ApeLife/article/details/99700882



rte_malloc


eth_igb_dev_init
read register or write:
E1000_PCI_REG_ADDR
E1000_READ_REG
E1000_WRITE_REG

e1000_init_nvm_ops_generic
e1000_init_mbx_ops_generic -> mailbox

cb:
rx_pkt_burst



core:
rte_flow_create


从 find_physaddr() 中提取 rte_mem_virt2phy()。 该函数允许获取映射到调用该函数的当前进程的任何虚拟地址的物理地址。 请注意，此函数非常慢，不应在初始化后调用，以避免性能瓶颈
#define RTE_BAD_PHYS_ADDR ((phys_addr_t)-1)
#define RTE_BAD_IOVA ((rte_iova_t)-1)

rte_mem_virt2phy(const void *virtaddr) -> 获取当前进程中任意映射虚拟地址的物理地址, 整理了DPDK中实现在用户态分配巨页和获取巨页物理地址的代码，可以作为参考，需要简化代码
    page_size = getpagesize()


rte_iova_t rte_mem_virt2iova(const void *virt);

内存缓冲区
struct rte_mbuf {
    ...
    buf_iova -> 段缓冲区的物理地址。 如果构建配置为仅使用虚拟地址作为 IOVA（即 RTE_IOVA_AS_PA 为 0），则该字段未定义。 强制对齐到 8 字节，以确保 32 位和 64 位具有完全相同的 mbuf cacheline0 布局。 这使得矢量驱动程序的工作变得更加容易. buf_iova 是给设备用的地址，在 iova_mod=PA 的情况，buf_iova 就是物理地址
    ...
}


rte_mempool_obj_iter(mp, rte_pktmbuf_init, NULL) -> mbuf：使用配置中的默认内存池处理程序，默认情况下，用于 mbuf 分配的内存池操作是多生产者和多消费者环。 我们可以想象一个提供硬件辅助池机制的目标（也许是一些网络处理器？）。 在这种情况下，该架构的默认配置将包含不同的 RTE_MBUF_DEFAULT_MEMPOOL_OPS 值
    rte_pktmbuf_priv_size
    rte_pktmbuf_data_room_size
    rte_mbuf_iova_set(m, rte_mempool_virt2iova(m) + mbuf_size) -> mbuf：添加帮助程序来获取/设置 IOVA 地址，添加 API rte_mbuf_iova_set 和 rte_mbuf_iova_get 分别用于设置和获取 mbuf 的物理地址。 更新了应用程序和库以使用相同的


rte_mempool_populate_default


struct hugepage_info {



./dpdk-helloworld -l 0-3 -n 4





eal_get_virtual_area

echo 16 >/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
cat /proc/meminfo |grep -i huge

helloworld -> examples/helloworld/main.c
main(int argc, char **argv)
rte_eal_init(int argc, char **argv)
    rte_eal_get_configuration
    eal_get_internal_configuration
    rte_cpu_is_supported
        RTE_COMPILE_TIME_CPUFLAGS -> #define RTE_COMPILE_TIME_CPUFLAGS RTE_CPUFLAG_SSE,RTE_CPUFLAG_SSE2,RTE_CPUFLAG_SSE3,RTE_CPUFLAG_SSSE3,RTE_CPUFLAG_SSE4_1,RTE_CPUFLAG_SSE4_2,RTE_CPUFLAG_AES,RTE_CPUFLAG_AVX,RTE_CPUFLAG_AVX2,RTE_CPUFLAG_AVX512BW,RTE_CPUFLAG_AVX512CD,RTE_CPUFLAG_AVX512DQ,RTE_CPUFLAG_AVX512F,RTE_CPUFLAG_AVX512VL,RTE_CPUFLAG_PCLMULQDQ,RTE_CPUFLAG_RDRAND,RTE_CPUFLAG_RDSEED
        rte_cpu_get_flag_enabled
            rte_cpu_feature_table
            __get_cpuid_max(feat->leaf & 0x80000000, NULL)
            const struct feature_entry rte_cpu_feature_table[]
    __atomic_compare_exchange_n
    eal_reset_internal_config
        internal_cfg->iova_mode = RTE_IOVA_DC -> default memory mode
    eal_log_level_parse -> set_log demo: ./app/test-pmd --log-level='pmd\.i40e.*,8' -> 现在正确设置 --log-level=7 不会打印来自 rte_eal_cpu_init() 例程的消息
    eal_save_args -> Connecting to /var/run/dpdk/rte/dpdk_telemetry.v2
        handle_eal_info_request
            rte_tel_data_start_array
            rte_tel_data_add_array_string
    rte_eal_cpu_init -> eal：不要对CPU检测感到恐慌，可能没有办法优雅地恢复，但是应该通知应用程序发生了故障，而不是完全中止。 这允许用户继续使用“慢路径”类型的解决方案。 进行此更改后，EAL CPU NUMA 节点解析步骤不再发出 rte_panic。 这与 rte_eal_init 中的代码一致，该代码期望失败返回错误代码 -> 使用物理和逻辑处理器的数量填充配置 此函数是 EAL 专用的。 解析 /proc/cpuinfo 以获取计算机上的物理和逻辑处理器的数量, /sys/devices/system/cpu
        eal_cpu_socket_id -> NUMA_NODE_PATH "/sys/devices/system/node"
        eal_cpu_detected -> eal：不缓存 lcore 检测状态，我们仅在服务核心和 -c/-l 选项的控制路径中使用此状态。 使用--lcores 时，该值不会更新。 在需要的地方使用内部助手
    eal_parse_args
        while ((opt = getopt_long
        eal_parse_common_option
            -l 和 -c 选项是选择 DPDK 使用的内核的两种方法。 它们的格式不同，但对所选核心的检查是相同的。 使用中间数组将特定的解析检查与常见的一致性检查分开。 解析函数现在专注于验证传递的字符串，而不执行其他操作。 我们可以报告所有无效的核心索引，而不仅仅是第一个错误。 在错误日志消息中，当核心列表不连续时，将 [0, cfg->lcore_count - 1] 报告为有效范围是错误的
            eal_service_cores_parsed
            rte_eal_parse_coremask
            update_lcore_config
            -n -> conf->force_nchannel = atoi(optarg)
        eal_create_runtime_dir -> eal：即使不使用共享数据，也创建运行时目录，当不需要多进程并且DPDK使用“no-shconf”标志运行时，遥测库仍然需要一个运行时目录来放置用于遥测连接的unix套接字。 因此，我们可以更改代码以尝试创建目录，而不是在设置此标志时不创建目录，但如果失败则不会出错。 如果成功，则遥测将可用，但如果失败，DPDK 的其余部分将在没有遥测的情况下运行。 这确保了“内存中”标志将允许 DPDK 运行，即使整个文件系统是只读的
            /var/run/dpdk
            eal_get_hugefile_prefix
            eal_set_runtime_dir
                strlcpy(runtime_dir, run_dir, PATH_MAX)
        eal_adjust_config
            eal_auto_detect_cores -> eal：限制核心自动检测，当未指定以下选项时，此补丁使用 pthread_getaffinity_np() 来缩小使用的核心范围： * coremask (-c) * corelist (-l) * 和 coremap (--lcores) 这样做的目的 patch的目的是在容器环境下部署DPDK应用程序时省略这些核心相关选项，以便用户在开发应用程序时不需要决定核心相关参数。 相反，当应用程序部署在容器中时，请使用 cpu-set 来限制可以在该容器实例内使用哪些核心。 而容器内的DPDK应用程序只是依靠这种自动检测机制来启动轮询线程。 注意：之前有部分用户使用隔离CPU，默认可以排除。 请添加任务集等命令来使用这些核心。 测试示例： $taskset 0xc0000 ./examples/helloworld/build/helloworld -m 1024
                rte_thread_get_affinity_by_id
            eal_proc_type_detect
            main_lcore_parsed -> eal：重命名lcore master和slave，将master lcore替换为main lcore，并将slave lcore替换为worker lcore。 保留旧函数和宏，但将它们标记为在此版本中已弃用。 “--master-lcore”命令行选项也已弃用，任何使用都会打印警告并使用“--main-lcore”作为替换
            compute_ctrl_threads_cpuset -> eal：限制控制线程启动 CPU 亲和力，在不属于 eal coremask 的任何内容上生成 ctrl 线程对系统的其余部分来说不太礼貌，尤其是当您非常小心地使用工具将进程固定在 cpu 资源上时 像任务集（linux）/cpuset（freebsd）。 我们不再引入另一个 eal 选项来控制在哪个 cpu 上创建这些 ctrl 线程，而是以启动 cpu 亲和力作为参考并从中删除 eal coremask。 如果没有剩下 cpu，那么我们默认为主核心。 cpuset 在 init 时计算一次，然后原始 cpu 关联性就会丢失。 引入了一个RTE_CPU_AND宏来抽象linux和freebsd各自宏之间的差异 -> taskset -c 7  ./master/app/testpmd --master-lcore 0 --lcores '(0,7)@(7,4,5)'  --no-huge --no-pci -m 512 -- -i --total-num-mbufs=2048
                RTE_CPU_AND
        eal_check_common_options -> sanity checks -> eal：分解选项健全性检查，无需对常见选项进行重复检查。 为选项 -c 和 -m 设置一些标志以简化检查
            Main lcore
            mbuf_pool ...
        eal_usage
            eal_common_usage -> help
    eal_plugins_init -> eal: 在设备解析之前调用插件 init，默认 eal_init 代码调用 0. eal_plugins_init 1. eal_option_device_parse 2. rte_bus_scan IOVA 提交：cf408c224 错过了在 eal_option_device_parse、rte_bus_scan 之前调用 eal_plugins_init 以及下面引入的共享模式回归：使用 CONFIG_RTE_BUILD_SHARED_LIB=y: 'net_vhost 0 ,iface=/tmp/vhost-user2' -d ./install/lib/librte_pmd_vhost.so -- --portmask=1 --disable-hw-vlan -i --rxq=1 --txq=1 --nb -cores=1 --eth-peer=0,52:54:00:11:22:12 EAL：检测到 4 个 lcore 错误：无法解析设备“net_vhost0”EAL：无法解析设备“net_vhost0,iface” =/tmp/vhost-user2' main() 中发生恐慌：无法初始化 EAL
        如果我们不是静态链接，请添加默认驱动程序加载路径（如果它作为目录存在）。 （在 EAL 上使用带有 NOLOAD 标志的 dlopen，如果 EAL 共享库尚未加载，即它是静态链接的，则将返回 NULL
        is_shared_build
            #define EAL_SO "librte_eal.so
            handle = dlopen(soname, RTLD_LAZY | RTLD_NOLOAD)
        eal_plugin_add -> eal：支持从目录加载驱动程序，添加对目录的支持作为 -d 的参数，以从给定目录加载所有驱动程序。 此外，可以在构建时配置中设置默认驱动程序目录，在这种情况下，在初始化 EAL 时将始终使用该目录。 与使用 -d 手动加载单个驱动程序相比，这大大简化了共享库配置的使用，并允许发行版建立一个嵌入式驱动程序目录，以便与第 3 方驱动程序等无缝集成
            -> #define RTE_EAL_PMD_PATH "/usr/local/lib/x86_64-linux-gnu/dpdk/pmds-23.1"
        TAILQ_FOREACH
            eal_plugindir_init
            eal_dlopen
    rte_config_init
        rte_eal_config_create
            mem_cfg_fd = open(pathname, O_RDWR | O_CREAT, 0600)
            retval = ftruncate(mem_cfg_fd, cfg_len)
            retval = fcntl(mem_cfg_fd, F_SETLK, &wr_lock)
            eal_get_virtual_area -> eal：修复多进程的内存配置分配，目前，内存配置将在不使用虚拟区域预留基础设施的情况下进行映射，这意味着它将被映射到任意位置。 这可能会导致在辅助进程中映射共享配置失败，因为 PCI 白名单参数在主进程已分配共享内存配置的空间中分配内存。 通过使用虚拟区域预留来为内存配置预留空间来修复此问题，从而避免该问题并保留共享配置（希望如此）远离任何正常的内存分配
                rte_mem_page_size -> eal：引入内存管理包装器，引入独立于操作系统的包装器，用于跨DPDK使用的内存管理操作，特别是在EAL的公共代码中： * rte_mem_map() * rte_mem_unmap() * rte_mem_page_size() * rte_mem_lock() Windows使用不同的API进行内存映射 和保留，而 Unices 通过映射来保留内存。 引入 EAL 私有函数以支持公共代码中的内存预留： * eal_mem_reserve() * eal_mem_free() * eal_mem_set_dump() 包装器遵循仅限于 DPDK 任务的 POSIX 语义，但它们的签名故意与 POSIX 签名不同，以更加安全和更具表现力。 新符号是内部的。 由于包装很薄，因此不需要特殊维护
                eal_get_baseaddr -> Linux 内核使用一个非常高的地址作为服务 mmap 调用的起始地址。 如果存在寻址限制并且 IOVA 模式为 VA，则该起始地址对于这些设备来说可能太高。 但是，可以在进程虚拟地址空间中使用较低的地址，因为 64 位有大量可用空间。 当前已知的限制是 39 或 40 位。 将起始地址设置为 4GB 意味着有 508GB 或 1020GB 用于映射可用的大页。 这对于大多数系统来说可能已经足够了，尽管具有寻址限制的设备应该调用 rte_mem_check_dma_mask 以确保所有内存都在支持的范围内
                    return 0x7000000000ULL -> 28GB = int("0x700000000", 16)/(1<<30)
                    or
                    return 0x100000000ULL -> 4GB
                eal_mem_reserve
                    int sys_flags = MAP_PRIVATE | MAP_ANONYMOUS;
                    sys_flags |= MAP_HUGETLB
                     mem_map(requested_addr, size, PROT_NONE, sys_flags, -1, 0)
                RTE_PTR_ALIGN
            mapped_mem_cfg_addr = mmap(rte_mem_cfg_addr,
            cfg_len_aligned, PROT_READ | PROT_WRITE,
            MAP_SHARED | MAP_FIXED, mem_cfg_fd, 0);
        eal_mcfg_update_from_internal
            mcfg->single_file_segments = internal_conf->single_file_segments
        rte_eal_config_attach
        eal_mcfg_wait_complete
        __rte_mp_enable
    rte_eal_using_phys_addrs -> eal：根据PA可用性计算IOVA模式，目前，如果总线选择IOVA作为PA，则在缺乏对物理地址的访问时，内存初始化可能会失败。 对于普通用户来说，这可能很难理解出了什么问题，因为这是默认行为。 通过验证物理地址可用性，在 eal init 中尽早发现这种情况，或者在没有表达明确的偏好时选择 IOVA。 总线代码已更改，以便它在不关心 IOVA 模式时进行报告，并让 eal init 决定。 在Linux实现中，重新设计rte_eal_using_phys_addrs()，以便可以更早地调用它，但仍然避免与rte_mem_virt2phys()的循环依赖。 在 FreeBSD 实现中，rte_eal_using_phys_addrs() 始终返回 false，因此检测部分保持原样。 如果编译了librte_kni并加载了KNI kmod， - 如果总线请求VA，如果物理地址可用，则强制使用PA，就像之前所做的那样， - 否则，将iova保留为VA，KNI init稍后将失败
        rte_eal_has_hugepages -> no_hugetlbfs -> default use hugepage
        rte_mem_virt2phy
            return RTE_BAD_IOVA
    rte_bus_get_iommu_class
    if (internal_conf->no_hugetlbfs == 0)
        hugepage_info_init
        create_shared_memory
            map_sharee_memory
    ...
    RTE_LOG(DEBUG, EAL, "IOMMU is not available, selecting IOVA as PA mode.\n")
    rte_eal_get_configuration
    eal_hugepage_info_init
    eal_log_init
    rte_eal_vfio_setup
    rte_eal_memzone_init
    eal_hugedirs_unlock
    rte_eal_malloc_heap_init
    rte_eal_tailqs_init
    rte_eal_timer_init
    eal_check_mem_on_local_socket
    rte_thread_set_affinity_by_id
    eal_thread_dump_current_affinity
    RTE_LCORE_FOREACH_WORKER(i)
        eal_worker_thread_create(i)
    rte_eal_mp_remote_launch sync_func
    rte_eal_mp_wait_lcore
    rte_service_init
    if (rte_bus_probe())
    rte_vfio_is_enabled
    rte_service_start_with_defaults
    eal_clean_runtime_dir
    rte_log_register_type_and_pick_level
    rte_telemetry_init
    eal_mcfg_complete
rte_eal_remote_launch(lcore_hello, NULL, lcore_id)
lcore_hello




args/option:
eal_usage
eal_common_usage(void)
{
    printf("[options]\n\n"
           "EAL common options:\n"
           "  -c COREMASK         Hexadecimal bitmask of cores to run on\n"
           "  -l CORELIST         List of cores to run on\n"
           "                      The argument format is <c1>[-c2][,c3[-c4],...]\n"
           "                      where c1, c2, etc are core indexes between 0 and %d\n"
           "  --"OPT_LCORES" COREMAP    Map lcore set to physical cpu set\n"
           "                      The argument format is\n"
           "                            '<lcores[@cpus]>[<,lcores[@cpus]>...]'\n"
           "                      lcores and cpus list are grouped by '(' and ')'\n"
           "                      Within the group, '-' is used for range separator,\n"
           "                      ',' is used for single number separator.\n"
           "                      '( )' can be omitted for single element group,\n"
           "                      '@' can be omitted if cpus and lcores have the same value\n"
           "  -s SERVICE COREMASK Hexadecimal bitmask of cores to be used as service cores\n"
           "  --"OPT_MAIN_LCORE" ID     Core ID that is used as main\n"
           "  --"OPT_MBUF_POOL_OPS_NAME" Pool ops name for mbuf to use\n"
           "  -n CHANNELS         Number of memory channels\n" -> 内存通道是内存单元和 CPU 之间用于数据移动的走线。 内存通道的数量充当以更快的速率传输数据的路径。 对于 OVS-DPDK，参数 OVSDpdkMemoryChannels 保存活动使用的通道数, 内存通道 获取正确的内存通道数（-n 参数）很棘手，因为它取决于系统主板支持的通道数、内存芯片的数量和类型以及它们在系统中的物理安装方式，并且没有简单或简单的方法 甚至可以通过可靠的方式来判断正在运行的系统。 此信息应该在 BIOS 内存检查阶段的启动期间可用，在运行时 dmidecode 可以帮助至少做出有根据的猜测。 除非已经安装，否则您现在需要这样做： # yum install dmidecode 这通常会提供足够的信息来查找系统和/或主板手册，以了解主板支持什么以及在哪些配置中内存插槽数量 对于多通道支持至关重要： # dmidecode -t system # dmidecode -t baseboard 这会输出系统上已填充的内存插槽： # dmidecode -t memory | grep 'Size: [0-9]' 如果只有一个，则不能使用多通道内存。 如果有两个或其倍数，则可能是双通道，如果有三个或其倍数，则可能是三通道，如果有四个或其倍数，则可能是四通道。 或者双通道...此外，内存设备的定位器字段中可能还有进一步的提示，例如 ChannelA-DIMM0 和 ChannelB-DIMM0： grep 定位器：
           "  -m MB               Memory to allocate (see also --"OPT_SOCKET_MEM")\n"
           "  -r RANKS            Force number of memory ranks (don't detect)\n"
           "  -b, --block         Add a device to the blocked list.\n"
           "                      Prevent EAL from using this device. The argument\n"
           "                      format for PCI devices is <domain:bus:devid.func>.\n"
           "  -a, --allow         Add a device to the allow list.\n"
           "                      Only use the specified devices. The argument format\n"
           "                      for PCI devices is <[domain:]bus:devid.func>.\n"
           "                      This option can be present several times.\n"
           "                      [NOTE: " OPT_DEV_ALLOW " cannot be used with "OPT_DEV_BLOCK" option]\n"
           "  --"OPT_VDEV"              Add a virtual device.\n"
           "                      The argument format is <driver><id>[,key=val,...]\n"
           "                      (ex: --vdev=net_pcap0,iface=eth2).\n"
           "  --"OPT_IOVA_MODE"   Set IOVA mode. 'pa' for IOVA_PA\n"
           "                      'va' for IOVA_VA\n"
           "  -d LIB.so|DIR       Add a driver or driver directory\n"
           "                      (can be used multiple times)\n"
           "  --"OPT_VMWARE_TSC_MAP"    Use VMware TSC map instead of native RDTSC\n"
           "  --"OPT_PROC_TYPE"         Type of this process (primary|secondary|auto)\n"
#ifndef RTE_EXEC_ENV_WINDOWS
           "  --"OPT_SYSLOG"            Set syslog facility\n"
#endif
           "  --"OPT_LOG_LEVEL"=<level> Set global log level\n"
           "  --"OPT_LOG_LEVEL"=<type-match>:<level>\n"
           "                      Set specific log level\n"
           "  --"OPT_LOG_LEVEL"=help    Show log types and levels\n"
#ifndef RTE_EXEC_ENV_WINDOWS
           "  --"OPT_TRACE"=<regex-match>\n"
           "                      Enable trace based on regular expression trace name.\n"
           "                      By default, the trace is disabled.\n"
           "		      User must specify this option to enable trace.\n"
           "  --"OPT_TRACE_DIR"=<directory path>\n"
           "                      Specify trace directory for trace output.\n"
           "                      By default, trace output will created at\n"
           "                      $HOME directory and parameter must be\n"
           "                      specified once only.\n"
           "  --"OPT_TRACE_BUF_SIZE"=<int>\n"
           "                      Specify maximum size of allocated memory\n"
           "                      for trace output for each thread. Valid\n"
           "                      unit can be either 'B|K|M' for 'Bytes',\n"
           "                      'KBytes' and 'MBytes' respectively.\n"
           "                      Default is 1MB and parameter must be\n"
           "                      specified once only.\n"
           "  --"OPT_TRACE_MODE"=<o[verwrite] | d[iscard]>\n"
           "                      Specify the mode of update of trace\n"
           "                      output file. Either update on a file can\n"
           "                      be wrapped or discarded when file size\n"
           "                      reaches its maximum limit.\n"
           "                      Default mode is 'overwrite' and parameter\n"
           "                      must be specified once only.\n"
#endif /* !RTE_EXEC_ENV_WINDOWS */
           "  -v                  Display version information on startup\n"
           "  -h, --help          This help\n"
           "  --"OPT_IN_MEMORY"   Operate entirely in memory. This will\n"
           "                      disable secondary process support\n"
           "  --"OPT_BASE_VIRTADDR"     Base virtual address\n"
           "  --"OPT_TELEMETRY"   Enable telemetry support (on by default)\n"
           "  --"OPT_NO_TELEMETRY"   Disable telemetry support\n"
           "  --"OPT_FORCE_MAX_SIMD_BITWIDTH" Force the max SIMD bitwidth\n"
           "\nEAL options for DEBUG use only:\n"
           "  --"OPT_HUGE_UNLINK"[=existing|always|never]\n"
           "                      When to unlink files in hugetlbfs\n"
           "                      ('existing' by default, no value means 'always')\n"
           "  --"OPT_NO_HUGE"           Use malloc instead of hugetlbfs\n"
           "  --"OPT_NO_PCI"            Disable PCI\n"
           "  --"OPT_NO_HPET"           Disable HPET\n"
           "  --"OPT_NO_SHCONF"         No shared config (mmap'd files)\n"
           "\n", RTE_MAX_LCORE);
}



CPU特性
const struct feature_entry rte_cpu_feature_table[] = {
    FEAT_DEF(SSE3, 0x00000001, 0, RTE_REG_ECX,  0)
    FEAT_DEF(PCLMULQDQ, 0x00000001, 0, RTE_REG_ECX,  1)
    FEAT_DEF(DTES64, 0x00000001, 0, RTE_REG_ECX,  2)
    FEAT_DEF(MONITOR, 0x00000001, 0, RTE_REG_ECX,  3)
    FEAT_DEF(DS_CPL, 0x00000001, 0, RTE_REG_ECX,  4)
    FEAT_DEF(VMX, 0x00000001, 0, RTE_REG_ECX,  5)
    FEAT_DEF(SMX, 0x00000001, 0, RTE_REG_ECX,  6)
    FEAT_DEF(EIST, 0x00000001, 0, RTE_REG_ECX,  7)
    FEAT_DEF(TM2, 0x00000001, 0, RTE_REG_ECX,  8)
    FEAT_DEF(SSSE3, 0x00000001, 0, RTE_REG_ECX,  9)
    FEAT_DEF(CNXT_ID, 0x00000001, 0, RTE_REG_ECX, 10)
    FEAT_DEF(FMA, 0x00000001, 0, RTE_REG_ECX, 12)
    FEAT_DEF(CMPXCHG16B, 0x00000001, 0, RTE_REG_ECX, 13)
    FEAT_DEF(XTPR, 0x00000001, 0, RTE_REG_ECX, 14)
    FEAT_DEF(PDCM, 0x00000001, 0, RTE_REG_ECX, 15)
    FEAT_DEF(PCID, 0x00000001, 0, RTE_REG_ECX, 17)
    FEAT_DEF(DCA, 0x00000001, 0, RTE_REG_ECX, 18)
    FEAT_DEF(SSE4_1, 0x00000001, 0, RTE_REG_ECX, 19)
    FEAT_DEF(SSE4_2, 0x00000001, 0, RTE_REG_ECX, 20)
    FEAT_DEF(X2APIC, 0x00000001, 0, RTE_REG_ECX, 21)
    FEAT_DEF(MOVBE, 0x00000001, 0, RTE_REG_ECX, 22)
    FEAT_DEF(POPCNT, 0x00000001, 0, RTE_REG_ECX, 23)
    FEAT_DEF(TSC_DEADLINE, 0x00000001, 0, RTE_REG_ECX, 24)
    FEAT_DEF(AES, 0x00000001, 0, RTE_REG_ECX, 25)
    FEAT_DEF(XSAVE, 0x00000001, 0, RTE_REG_ECX, 26)
    FEAT_DEF(OSXSAVE, 0x00000001, 0, RTE_REG_ECX, 27)
    FEAT_DEF(AVX, 0x00000001, 0, RTE_REG_ECX, 28)
    FEAT_DEF(F16C, 0x00000001, 0, RTE_REG_ECX, 29)
    FEAT_DEF(RDRAND, 0x00000001, 0, RTE_REG_ECX, 30)
    FEAT_DEF(HYPERVISOR, 0x00000001, 0, RTE_REG_ECX, 31)

    FEAT_DEF(FPU, 0x00000001, 0, RTE_REG_EDX,  0)
    FEAT_DEF(VME, 0x00000001, 0, RTE_REG_EDX,  1)
    FEAT_DEF(DE, 0x00000001, 0, RTE_REG_EDX,  2)
    FEAT_DEF(PSE, 0x00000001, 0, RTE_REG_EDX,  3)
    FEAT_DEF(TSC, 0x00000001, 0, RTE_REG_EDX,  4)
    FEAT_DEF(MSR, 0x00000001, 0, RTE_REG_EDX,  5)
    FEAT_DEF(PAE, 0x00000001, 0, RTE_REG_EDX,  6)
    FEAT_DEF(MCE, 0x00000001, 0, RTE_REG_EDX,  7)
    FEAT_DEF(CX8, 0x00000001, 0, RTE_REG_EDX,  8)
    FEAT_DEF(APIC, 0x00000001, 0, RTE_REG_EDX,  9)
    FEAT_DEF(SEP, 0x00000001, 0, RTE_REG_EDX, 11)
    FEAT_DEF(MTRR, 0x00000001, 0, RTE_REG_EDX, 12)
    FEAT_DEF(PGE, 0x00000001, 0, RTE_REG_EDX, 13)
    FEAT_DEF(MCA, 0x00000001, 0, RTE_REG_EDX, 14)
    FEAT_DEF(CMOV, 0x00000001, 0, RTE_REG_EDX, 15)
    FEAT_DEF(PAT, 0x00000001, 0, RTE_REG_EDX, 16)
    FEAT_DEF(PSE36, 0x00000001, 0, RTE_REG_EDX, 17)
    FEAT_DEF(PSN, 0x00000001, 0, RTE_REG_EDX, 18)
    FEAT_DEF(CLFSH, 0x00000001, 0, RTE_REG_EDX, 19)
    FEAT_DEF(DS, 0x00000001, 0, RTE_REG_EDX, 21)
    FEAT_DEF(ACPI, 0x00000001, 0, RTE_REG_EDX, 22)
    FEAT_DEF(MMX, 0x00000001, 0, RTE_REG_EDX, 23)
    FEAT_DEF(FXSR, 0x00000001, 0, RTE_REG_EDX, 24)
    FEAT_DEF(SSE, 0x00000001, 0, RTE_REG_EDX, 25)
    FEAT_DEF(SSE2, 0x00000001, 0, RTE_REG_EDX, 26)
    FEAT_DEF(SS, 0x00000001, 0, RTE_REG_EDX, 27)
    FEAT_DEF(HTT, 0x00000001, 0, RTE_REG_EDX, 28)
    FEAT_DEF(TM, 0x00000001, 0, RTE_REG_EDX, 29)
    FEAT_DEF(PBE, 0x00000001, 0, RTE_REG_EDX, 31)

    FEAT_DEF(DIGTEMP, 0x00000006, 0, RTE_REG_EAX,  0)
    FEAT_DEF(TRBOBST, 0x00000006, 0, RTE_REG_EAX,  1)
    FEAT_DEF(ARAT, 0x00000006, 0, RTE_REG_EAX,  2)
    FEAT_DEF(PLN, 0x00000006, 0, RTE_REG_EAX,  4)
    FEAT_DEF(ECMD, 0x00000006, 0, RTE_REG_EAX,  5)
    FEAT_DEF(PTM, 0x00000006, 0, RTE_REG_EAX,  6)

    FEAT_DEF(MPERF_APERF_MSR, 0x00000006, 0, RTE_REG_ECX,  0)
    FEAT_DEF(ACNT2, 0x00000006, 0, RTE_REG_ECX,  1)
    FEAT_DEF(ENERGY_EFF, 0x00000006, 0, RTE_REG_ECX,  3)

    FEAT_DEF(FSGSBASE, 0x00000007, 0, RTE_REG_EBX,  0)
    FEAT_DEF(BMI1, 0x00000007, 0, RTE_REG_EBX,  3)
    FEAT_DEF(HLE, 0x00000007, 0, RTE_REG_EBX,  4)
    FEAT_DEF(AVX2, 0x00000007, 0, RTE_REG_EBX,  5)
    FEAT_DEF(SMEP, 0x00000007, 0, RTE_REG_EBX,  7)
    FEAT_DEF(BMI2, 0x00000007, 0, RTE_REG_EBX,  8)
    FEAT_DEF(ERMS, 0x00000007, 0, RTE_REG_EBX,  9)
    FEAT_DEF(INVPCID, 0x00000007, 0, RTE_REG_EBX, 10)
    FEAT_DEF(RTM, 0x00000007, 0, RTE_REG_EBX, 11)
    FEAT_DEF(AVX512F, 0x00000007, 0, RTE_REG_EBX, 16)
    FEAT_DEF(AVX512DQ, 0x00000007, 0, RTE_REG_EBX, 17)
    FEAT_DEF(RDSEED, 0x00000007, 0, RTE_REG_EBX, 18)
    FEAT_DEF(AVX512IFMA, 0x00000007, 0, RTE_REG_EBX, 21)
    FEAT_DEF(AVX512CD, 0x00000007, 0, RTE_REG_EBX, 28)
    FEAT_DEF(AVX512BW, 0x00000007, 0, RTE_REG_EBX, 30)
    FEAT_DEF(AVX512VL, 0x00000007, 0, RTE_REG_EBX, 31)

    FEAT_DEF(AVX512VBMI, 0x00000007, 0, RTE_REG_ECX,  1)
    FEAT_DEF(WAITPKG, 0x00000007, 0, RTE_REG_ECX,  5)
    FEAT_DEF(AVX512VBMI2, 0x00000007, 0, RTE_REG_ECX,  6)
    FEAT_DEF(GFNI, 0x00000007, 0, RTE_REG_ECX,  8)
    FEAT_DEF(VAES, 0x00000007, 0, RTE_REG_ECX,  9)
    FEAT_DEF(VPCLMULQDQ, 0x00000007, 0, RTE_REG_ECX, 10)
    FEAT_DEF(AVX512VNNI, 0x00000007, 0, RTE_REG_ECX, 11)
    FEAT_DEF(AVX512BITALG, 0x00000007, 0, RTE_REG_ECX, 12)
    FEAT_DEF(AVX512VPOPCNTDQ, 0x00000007, 0, RTE_REG_ECX, 14)
    FEAT_DEF(CLDEMOTE, 0x00000007, 0, RTE_REG_ECX, 25)
    FEAT_DEF(MOVDIRI, 0x00000007, 0, RTE_REG_ECX, 27)
    FEAT_DEF(MOVDIR64B, 0x00000007, 0, RTE_REG_ECX, 28)

    FEAT_DEF(AVX512VP2INTERSECT, 0x00000007, 0, RTE_REG_EDX,  8)

    FEAT_DEF(LAHF_SAHF, 0x80000001, 0, RTE_REG_ECX,  0)
    FEAT_DEF(LZCNT, 0x80000001, 0, RTE_REG_ECX,  4)

    FEAT_DEF(SYSCALL, 0x80000001, 0, RTE_REG_EDX, 11)
    FEAT_DEF(XD, 0x80000001, 0, RTE_REG_EDX, 20)
    FEAT_DEF(1GB_PG, 0x80000001, 0, RTE_REG_EDX, 26)
    FEAT_DEF(RDTSCP, 0x80000001, 0, RTE_REG_EDX, 27)
    FEAT_DEF(EM64T, 0x80000001, 0, RTE_REG_EDX, 29)

    FEAT_DEF(INVTSC, 0x80000007, 0, RTE_REG_EDX,  8)
};



长短选项:
enum {
    /* long options mapped to a short option */
#define OPT_HELP              "help"
    OPT_HELP_NUM            = 'h',
#define OPT_DEV_ALLOW	      "allow"
    OPT_DEV_ALLOW_NUM       = 'a',
#define OPT_DEV_BLOCK         "block"
    OPT_DEV_BLOCK_NUM      = 'b',

    /* first long only option value must be >= 256, so that we won't
     * conflict with short options */
    OPT_LONG_MIN_NUM = 256,
#define OPT_BASE_VIRTADDR     "base-virtaddr"
    OPT_BASE_VIRTADDR_NUM,
#define OPT_CREATE_UIO_DEV    "create-uio-dev"
    OPT_CREATE_UIO_DEV_NUM,
#define OPT_FILE_PREFIX       "file-prefix"
    OPT_FILE_PREFIX_NUM,
#define OPT_HUGE_DIR          "huge-dir"
    OPT_HUGE_DIR_NUM,
#define OPT_HUGE_UNLINK       "huge-unlink"
    OPT_HUGE_UNLINK_NUM,
#define OPT_LCORES            "lcores"
    OPT_LCORES_NUM,
#define OPT_LOG_LEVEL         "log-level"
    OPT_LOG_LEVEL_NUM,
#define OPT_TRACE             "trace"
    OPT_TRACE_NUM,
#define OPT_TRACE_DIR         "trace-dir"
    OPT_TRACE_DIR_NUM,
#define OPT_TRACE_BUF_SIZE    "trace-bufsz"
    OPT_TRACE_BUF_SIZE_NUM,
#define OPT_TRACE_MODE        "trace-mode"
    OPT_TRACE_MODE_NUM,
#define OPT_MAIN_LCORE        "main-lcore"
    OPT_MAIN_LCORE_NUM,
#define OPT_MBUF_POOL_OPS_NAME "mbuf-pool-ops-name"
    OPT_MBUF_POOL_OPS_NAME_NUM,
#define OPT_PROC_TYPE         "proc-type"
    OPT_PROC_TYPE_NUM,
#define OPT_NO_HPET           "no-hpet"
    OPT_NO_HPET_NUM,
#define OPT_NO_HUGE           "no-huge"
    OPT_NO_HUGE_NUM,
#define OPT_NO_PCI            "no-pci"
    OPT_NO_PCI_NUM,
#define OPT_NO_SHCONF         "no-shconf"
    OPT_NO_SHCONF_NUM,
#define OPT_IN_MEMORY         "in-memory"
    OPT_IN_MEMORY_NUM,
#define OPT_SOCKET_MEM        "socket-mem"
    OPT_SOCKET_MEM_NUM,
#define OPT_SOCKET_LIMIT        "socket-limit"
    OPT_SOCKET_LIMIT_NUM,
#define OPT_SYSLOG            "syslog"
    OPT_SYSLOG_NUM,
#define OPT_VDEV              "vdev"
    OPT_VDEV_NUM,
#define OPT_VFIO_INTR         "vfio-intr"
    OPT_VFIO_INTR_NUM,
#define OPT_VFIO_VF_TOKEN     "vfio-vf-token"
    OPT_VFIO_VF_TOKEN_NUM,
#define OPT_VMWARE_TSC_MAP    "vmware-tsc-map"
    OPT_VMWARE_TSC_MAP_NUM,
#define OPT_LEGACY_MEM    "legacy-mem"
    OPT_LEGACY_MEM_NUM,
#define OPT_SINGLE_FILE_SEGMENTS    "single-file-segments"
    OPT_SINGLE_FILE_SEGMENTS_NUM,
#define OPT_IOVA_MODE          "iova-mode"
    OPT_IOVA_MODE_NUM,
#define OPT_MATCH_ALLOCATIONS  "match-allocations"
    OPT_MATCH_ALLOCATIONS_NUM,
#define OPT_TELEMETRY         "telemetry"
    OPT_TELEMETRY_NUM,
#define OPT_NO_TELEMETRY      "no-telemetry"
    OPT_NO_TELEMETRY_NUM,
#define OPT_FORCE_MAX_SIMD_BITWIDTH  "force-max-simd-bitwidth"
    OPT_FORCE_MAX_SIMD_BITWIDTH_NUM,
#define OPT_HUGE_WORKER_STACK  "huge-worker-stack"
    OPT_HUGE_WORKER_STACK_NUM,

    OPT_LONG_MAX_NUM
};



设置日志级别:
export RTE_LOG_LEVEL=8


eal_dynmem_memseg_lists_init
计算出我们将拥有的内存量是一个漫长且非常复杂的过程。 我们操作的基本元素是内存类型，定义为 NUMA 节点 ID 和页面大小的组合（例如，具有 2 个页面大小的 2 个套接字总共产生 4 个内存类型）。 决定每种内存类型的内存量是每种类型的最大段、每种类型的最大内存和检测到的 NUMA 节点数量之间的平衡行为。 目标是确保每种内存类型至少获得一个 memseg 列表。 内存总量受 RTE_MAX_MEM_MB 值限制。 每种类型的内存总量受 RTE_MAX_MEM_MB_PER_TYPE 或 RTE_MAX_MEM_MB 除以检测到的 NUMA 节点数的限制。 此外，每种类型的最大段数也受到 RTE_MAX_MEMSEG_PER_TYPE 的限制。 这是因为对于较小的页面大小，可能需要数十万个段才能达到上述指定的每种类型的内存限制。 此外，每种类型可能有多个与其关联的 memseg 列表，每个列表受 RTE_MAX_MEM_MB_PER_LIST（对于较大的页面大小）或 RTE_MAX_MEMSEG_PER_LIST 段（对于较小的页面大小）的限制。 每种类型的 memseg 列表的数量是根据上述限制决定的，并且还考虑检测到的 NUMA 节点的数量，以确保在用内存填充所有 NUMA 节点之前我们不会用完 memseg 列表。 我们分三个阶段进行。 首先，我们收集类型的数量。 然后，我们找出内存限制并填充可能的 memseg 列表。 然后，我们继续分配 memseg 列表





helloworld -> examples/helloworld/main.c
main(int argc, char **argv)
rte_eal_init
    rte_eal_get_configuration
    eal_get_internal_configuration
    rte_cpu_is_supported
    eal_reset_internal_config
    eal_log_level_parse
    eal_save_args
    rte_eal_cpu_init
    eal_parse_args
    eal_plugins_init
    ...
    rte_config_init
        ...
        pathname -> /run/user/1020/dpdk/rte/config
        ...
        eal_mem_reserve
            addr:0x100000000 size: 28672
            ...
            mmap(requested_addr, size, prot, flags, fd, offset);
    rte_eal_using_phys_addrs
        rte_eal_has_hugepages -> no_hugetlbfs -> default use hugepage
        rte_mem_virt2phy
            return RTE_BAD_IOVA
    rte_bus_get_iommu_class
    if (internal_conf->no_hugetlbfs == 0)
        hugepage_info_init
        create_shared_memory
rte_eal_remote_launch(lcore_hello, NULL, lcore_id)
lcore_hello



rte_eal_malloc_heap_init
    malloc_add_seg


RTE_LCORE_FOREACH_WORKER(i)
    pipe(lcore_config[i].pipe_main2worker
    eal_worker_thread_create
        if (pthread_create(&lcore_config[lcore_id].thread_id, attrp, eal_thread_loop, (void *)(uintptr_t)lcore_id) == 0)
            eal_thread_wait_command
                n = read(m2w, &c, 1);
            eal_thread_ack_command
                n = write(w2m, &c, 1) <- eal_thread_wake_worker
    pthread_setaffinity_np
    rte_eal_mp_remote_launch(sync_func, NULL, SKIP_MAIN)
        rte_eal_remote_launch(f, arg, lcore_id)
            __atomic_store_n(&lcore_config[worker_id].f, f, __ATOMIC_RELEASE)
            eal_thread_wake_worker
    rte_eal_mp_wait_lcore


rte_service_init
    rte_services = rte_calloc("rte_services",



vfio_mp_sync_setup
    rte_mp_action_register
        name: eal_vfio_mp_sync
        entry = malloc(sizeof(struct action_entry));
        find_action_entry_by_name
        TAILQ_INSERT_TAIL(&action_entry_list, entry, next)


rte_service_start_with_defaults

eal_clean_runtime_dir





EAL: Multi-process socket /var/run/dpdk/rte/mp_socket

static struct rte_pci_driver mlx5_common_pci_driver = {
    .driver = {
           .name = MLX5_PCI_DRIVER_NAME,
    },
    .probe = mlx5_common_pci_probe,
    .remove = mlx5_common_pci_remove,
    .dma_map = mlx5_common_pci_dma_map,
    .dma_unmap = mlx5_common_pci_dma_unmap,
};


pci_probe(void)
    FOREACH_DEVICE_ON_PCIBUS <- RTE_PMD_REGISTER_PCI -> register pci device
        pci_probe_all_drivers
            FOREACH_DRIVER_ON_PCIBUS
                rte_pci_probe_one_driver
                    rte_pci_match
                    ...
                    hisi_dma_pmd_drv
                    idxd_pmd_drv_pci
                    ioat_pmd_drv
                    rte_ark_pmd
                    rte_cxgbe_pmd
                    rte_igb_pmd
                    rte_ice_pmd
                    mlx5_common_pci_driver <- rte_pci_register(&mlx5_common_pci_driver);
                    rte_virtio_net_pci_pmd
                    ...
                    ret = dr->probe(dr, dev) -> mlx5_common_pci_probe
                        mlx5_common_dev_probe
                            mlx5_kvargs_prepare
                            parse_class_options
                            mlx5_common_dev_create
                                mlx5_malloc_mem_select -> 选择 PMD 内存分配首选项。一旦设置了 sys_mem_en，则仅当设置了明确标志以从 rte hugepage 内存中排序内存时，才会从系统分配默认内存
                                    mlx5_sys_mem.enable = 1 -> SYS_MEM or RTE_MEM
                                mlx5_dev_hw_global_prepare
                                    mlx5_os_open_device(cdev, classes)
                                        struct ibv_context *ctx = NULL
                                        ctx = mlx5_open_device(cdev, classes)
                                        or ctx = mlx5_import_device(cdev)
                                        mlx5_set_context_attr(cdev->dev, ctx)
                                            .alloc = &mlx5_alloc_verbs_buf -> verbs回调以分配内存。此函数应根据大页面内提供的大小分配空间。请注意，所有分配都必须遵守 libmlx5 的对齐方式（即当前的 rte_mem_page_size()）
                                                size_t alignment = rte_mem_page_size()
                                                mlx5_malloc(0, size, alignment, dev->numa_node)
                                            mlx5_glue->dv_set_context_attr
                                    mlx5_devx_cmd_query_hca_attr -> Query HCA attributes, use kernel api query hw attr
                                        mlx5_devx_cmd_query_hca_parse_graph_node_cap
                                        mlx5_devx_cmd_query_hca_vdpa_attr
                                            hcattr = mlx5_devx_get_hca_cap(ctx, in, out, NULL, MLX5_GET_HCA_CAP_OP_MOD_VDPA_EMULATION | MLX5_HCA_CAP_OPMOD_GET_CUR)
                                                MLX5_SET(query_hca_cap_in, in, opcode, MLX5_CMD_OP_QUERY_HCA_CAP)
                                                mlx5_glue->devx_general_cmd(ctx, in, size_in, out, size_out)
                                            vdpa_attr->valid = 1
                                            vdpa_attr->virtio_version_1_0 =
                                            vdpa_attr->doorbell_bar_offset
                                            ...
                                        mlx5_devx_cmd_query_nic_vport_context
                                        mlx5_devx_get_hca_cap
                                    mlx5_os_pd_prepare
                                mlx5_mr_create_cache -> global share MR cache resources
                                    mlx5_os_set_reg_mr_cb(&share_cache->reg_mr_cb, &share_cache->dereg_mr_cb)
                                    mlx5_mr_btree_init(&share_cache->cache, MLX5_MR_BTREE_CACHE_N * 2, socket) -> Initialize B-tree and allocate memory for global MR cache table
                                        struct mlx5_mr_btree cache
                                        bt->table = mlx5_malloc
                                        // First entry must be NULL for binary search
                                        (*bt->table)[bt->len++] = (struct mr_cache_entry) {
                                            .lkey = UINT32_MAX,
                                        };
                                rte_mem_event_callback_register("MLX5_MEM_EVENT_CB", mlx5_mr_mem_event_cb, NULL)
                                    case RTE_MEM_EVENT_FREE
                                        TAILQ_FOREACH(cdev, &devices_list, next)
                                            mlx5_free_mr_by_addr(&cdev->mr_scache, mlx5_os_get_ctx_device_name(cdev->ctx), addr, len)
                            is_valid_class_combination -> 校验组合
                            drivers_probe
                                ...
                                mlx5_os_pci_probe
                                    mlx5_os_parse_eth_devargs
                                        rte_eth_devargs_parse
                                            eth_dev_devargs_tokenise
                                            rte_eth_devargs_parse_representor_ports
                                    if (eth_da.nb_ports > 0)
                                        mlx5_os_pci_probe_pf
                                            ibv_list = mlx5_glue->get_device_list(&ret)
                                            NETLINK_ROUTE
                                            NETLINK_RDMA
                                            mlx5_device_bond_pci_match
                                            np = mlx5_nl_portnum(nl_rdma, ibv_match[0]->name)
                                            mlx5_dev_spawn
                                            rte_eth_copy_pci_info
                                            rte_intr_instance_alloc
                                    else mlx5_os_pci_probe_pf
                                        mlx5_dev_spawn
                                            rte_eth_switch_domain_alloc



drivers/common/mlx5/mlx5_common_pci.c


mlx5_os_get_ibv_dev
mlx5_os_get_ibv_device


rte_telemetry_init
    telemetry_v2_init
        rte_telemetry_register_cmd("/"
            callbacks[i].fn = fn
        rte_telemetry_register_cmd("/info"
        v2_socket.sock = create_socket(v2_socket.path)
        pthread_create(&t_new, NULL, socket_listener, &v2_socket)
            int s_accepted = accept(s->sock, NULL, NULL)
            pthread_create(&th, NULL, s->fn,
    or telemetry_legacy_init



rte_log_register_type_and_pick_level


eal_mcfg_complete
    internal_conf->init_complete = 1;



RTE_PMD_REGISTER_VDEV(net_virtio_user, virtio_user_driver)
static struct rte_vdev_driver virtio_user_driver = {
    .probe = virtio_user_pmd_probe,
    .remove = virtio_user_pmd_remove,
    .dma_map = virtio_user_pmd_dma_map,
    .dma_unmap = virtio_user_pmd_dma_unmap,
};

vdpa
https://www.redhat.com/en/blog/vdpa-kernel-framework-part-3-usage-vms-and-containers
host模拟出/dev/vhost-vdpa0设备，容器中的testpmd接管设备，使用virtio-user driver
testpmd接管参考命令如下
./dpdk-testpmd -l 2-3 -n 4 --file-prefix=test --vdev=net_virtio_user0,path=/dev/vhost-vdpa-0,queues=1 – --forward-mode=sim -i -a

testpmd_funcs.rst, https://dpdk.readthedocs.io/en/v1.8.0/testpmd_app_ug/testpmd_funcs.html
user guider: https://doc.dpdk.org/guides/testpmd_app_ug/
run app: https://doc.dpdk.org/guides/testpmd_app_ug/run_app.html
./dpdk-testpmd -l 0-3 -n 4 -- -i --portmask=0x1 --nb-cores=2
app\test-pmd\testpmd.c -> main, https://blog.csdn.net/hhd1988/article/details/123237172, virtio-user pmd driver 跟 host 协商过程: https://blog.csdn.net/jyshappy/article/details/126718739
    rte_eal_init
        rte_bus_probe
            ret = bus->probe()
            vbus->probe() -> vdev_probe
                vdev_probe_all_drivers
                    driver->probe(dev) -> virtio_user_pmd_probe
                        eth_virtio_dev_init
                            eth_dev->dev_ops = &virtio_eth_dev_ops
                            virtio_init_device
                                virtio_set_status(hw, VIRTIO_CONFIG_STATUS_ACK)
                                virtio_set_status(hw, VIRTIO_CONFIG_STATUS_DRIVER)
                                virtio_ethdev_negotiate_features
                                    host_features = VIRTIO_OPS(hw)->get_features(hw)
                        virtio_user_dev_init
                            virtio_dev_construct(vdev, name, &virtio_user_ops, dev)
    register_eth_event_callback
    init_port
    set_def_fwd_config
    launch_args_parse
    rte_dev_hotplug_handle_enable
    rte_dev_event_monitor_start
    rte_dev_event_callback_register dev_event_callback
    start_port
    rte_eth_promiscuous_enable
    start_packet_forwarding
    rte_eal_cleanup


static const struct virtio_dev_ops virtio_user_ops = {
    .read_dev_cfg	= virtio_user_read_dev_config,
    .write_dev_cfg	= virtio_user_write_dev_config,
    .get_status	= virtio_user_get_status,
    .set_status	= virtio_user_set_status,
        virtio_user_dev_set_status(dev, status)
            ret = dev->ops->set_status(dev, status)
    .get_features	= virtio_user_get_features,
    .set_features	= virtio_user_set_features,
    .destruct_dev	= virtio_user_destroy,
    .get_queue_size	= virtio_user_get_queue_size,
    .setup_queue	= virtio_user_setup_queue,
    .del_queue	= virtio_user_del_queue,
    .notify_queue	= virtio_user_notify_queue,
    .dump_json_info = virtio_user_dump_json_info,
    .write_json_config = virtio_user_write_json_config,
};


struct fwd_engine io_fwd_engine = {
    .fwd_mode_name  = "io",
    .port_fwd_begin = NULL,
    .port_fwd_end   = NULL,
    .stream_init    = stream_init_forward,
    .packet_fwd     = pkt_burst_io_forward,
};

struct fwd_engine mac_fwd_engine = {
    .fwd_mode_name  = "mac",
    .port_fwd_begin = NULL,
    .port_fwd_end   = NULL,
    .stream_init    = stream_init_mac_forward,
    .packet_fwd     = pkt_burst_mac_forward,
};



struct fwd_engine mac_swap_engine = {
    .fwd_mode_name  = "macswap",
    .port_fwd_begin = NULL,
    .port_fwd_end   = NULL,
    .stream_init    = stream_init_mac_swap,
    .packet_fwd     = pkt_burst_mac_swap,
};


testpmd> show port info 0
cmdline_parse_inst_t cmd_showport = {
               .f =cmd_showport_parsed,
               .data =NULL,
               .help_str= "show|clear port "
                              "info|stats|xstats|fdir|stat_qmap|dcb_tc|cap"
                              "<port_id>",
               .tokens= {
                              (void*)&cmd_showport_show,
                              (void*)&cmd_showport_port,
                              (void*)&cmd_showport_what,
                              (void*)&cmd_showport_portnum,
                              NULL,
               },
};



# bind vfio-pci
modprobe vfio-pci
./usertools/dpdk-devbind.py -b vfio-pci 06:00.3 06:00.4

-a Add a device to the allow list
./dpdk-vdpa -c 0x2 -n 4 --socket-mem 1024,1024 \
        -a 0000:06:00.3,vdpa=1 -a 0000:06:00.4,vdpa=1 \
        -- --interactive
or
./dpdk-vdpa -c 0x2 -n 4 --socket-mem 1024,1024 -w 0000:06:00.3,vdpa=1 -w 0000:06:00.4,vdpa=1

dpdk/examples/vdpa/main.c
    ret = rte_eal_init(argc, argv)
    ret = parse_args(argc, argv)
    cl = cmdline_stdin_new(main_ctx, "vdpa> ")
    cmdline_interact(cl) -> create -> cmd_create_vdpa_port_parsed
        cmdline_read_char
        cmdline_in
            rdline_add_history(&cl->rdl, buffer)
                cirbuf_add_buf_tail(&rdl->history, buf, len) -> vhost_user_msg_handler
                cirbuf_add_tail(&rdl->history, 0)
    or start_vdpa(&vports[devcnt]) -> examples/vdpa：引入 vDPA 的新示例，vdpa 示例应用程序使用 vDPA 后端创建 vhost-user 套接字。vDPA 代表 vhost 数据路径加速，它利用 virtio 环兼容设备直接为 virtio 驱动程序提供服务以启用数据路径加速。由于 vDPA 驱动程序可以帮助设置 vhost 数据路径，因此此应用程序不需要为 vhost 入队/出队操作启动专用的工作线程
        access(socket_path, F_OK)
        rte_vhost_driver_register(socket_path, vport->flags)
        rte_vhost_driver_callback_register(socket_path,	&vdpa_sample_devops)
        rte_vhost_driver_attach_vdpa_device(socket_path, vport->dev)
            vsocket->vdpa_dev = dev -> 将vDPA的deviceid记录在vsocket结构中，这样就将vhost和vDPA设备关联起来
        rte_vhost_driver_get_vdpa_dev_type
        RTE_VHOST_VDPA_DEVICE_TYPE_BLK
            RTE_LOG(NOTICE, VDPA, "%s is a blk device\n", socket_path)
            vdpa_blk_device_set_features_and_protocol
                rte_vhost_driver_set_features(path, VHOST_BLK_FEATURES) -> VIRTIO_BLK_F_SIZE_MAX, VIRTIO_BLK_F_SEG_MAX, VIRTIO_BLK_F_SCSI, VIRTIO_BLK_F_CONFIG_WCE, VIRTIO_BLK_F_MQ...
                rte_vhost_driver_disable_features
                rte_vhost_driver_get_protocol_features
                    vdpa_dev->ops->get_protocol_features
                rte_vhost_driver_set_protocol_features
                    vsocket->protocol_features = protocol_features
        rte_vhost_driver_start(socket_path)

static const struct rte_vhost_device_ops vdpa_sample_devops = {
        rte_vhost_get_ifname(vid, ifname, sizeof(ifname))
    .new_device = new_device,
        dev = rte_vdpa_get_rte_device(vports[i].dev)
            return vdpa_dev->device
    .destroy_device = destroy_device,
};


vdpa> list
device id       device address  queue num       supported features
0               0000:06:00.3    1               0x14c238020
1               0000:06:00.4    1               0x14c238020
2               0000:06:00.5    1               0x14c238020
vdpa> create /tmp/vdpa-socket0 0000:06:00.3


/tmp/build/examples/dpdk-vdpa -w auxiliary:mlx5_core.sf.4,class=vdpa -- --iface /tmp/vhost-user-
Option -w, --pci-whitelist is deprecated, use -a, --allow option instead

# 我们还需要启动一个DPDK的程序，做vdpa的功能，并接到vf上去。
/data/soft/dpdk-stable-20.11.3/examples/vdpa/build/vdpa -w ${PCINUM%%.*}.2,class=vdpa --log-level=pmd,info -- -i
create /tmp/sock-virtio0 0000:43:00.2

nvidia use vdpa step:
Create the ASAP2 environment:
Create the VFs.
Enter switchdev mode.
Set up OVS.

Run the vDPA application:
cd $RTE_SDK/examples/vdpa/build
./vdpa -w <VF PCI BDF>,class=vdpa --log-level=pmd,info -- -i

Create a vDPA port via the vDPA application CLI:
create /tmp/sock-virtio0 <PCI DEVICE BDF>

drivers/vdpa/mlx5/mlx5_vdpa.c
RTE_INIT(rte_mlx5_vdpa_init)
    mlx5_common_init()
        mlx5_glue_constructor
            setenv("RDMAV_HUGEPAGES_SAFE", "1", 1)
            setenv("MLX5_CQE_SIZE", "128", 0)
            setenv("MLX5_DEVICE_FATAL_CLEANUP", "1", 1)
        mlx5_common_driver_init
            mlx5_common_pci_init
            #ifdef RTE_EXEC_ENV_LINUX
                mlx5_common_auxiliary_init()
                    rte_auxiliary_register(&mlx5_auxiliary_driver)
    mlx5_class_driver_register(&mlx5_vdpa_driver)
static struct mlx5_class_driver mlx5_vdpa_driver = {
    .drv_class = MLX5_CLASS_VDPA,
    .name = RTE_STR(MLX5_VDPA_DRIVER_NAME),
    .id_table = mlx5_vdpa_pci_id_map,
    .probe = mlx5_vdpa_dev_probe,
        priv = rte_zmalloc("mlx5 vDPA device private", sizeof(*priv) + sizeof(struct mlx5_vdpa_virtq) *attr->vdpa.max_num_virtio_queues, RTE_CACHE_LINE_SIZE)
        mlx5_vdpa_config_get(mkvlist, priv)
            const char **params = (const char *[]){
                "event_core",
                "event_mode",
                "event_us",
                "hw_latency_mode",
                "hw_max_latency_us",
                "hw_max_pending_comp",
                "no_traffic_time",
                "queue_size",
                "queues",
                "max_conf_threads",
                NULL,
            };
            mlx5_kvargs_process(mkvlist, params, mlx5_vdpa_args_check_handler, priv)
        mlx5_vdpa_create_dev_resources
            priv->var = mlx5_glue->dv_alloc_var(ctx, 0)
            priv->virtq_db_addr = mmap(NULL, priv->var->length, PROT_READ | PROT_WRITE, MAP_SHARED, ctx->cmd_fd, priv->var->mmap_off)
            priv->virtq_db_addr = (char *)priv->virtq_db_addr + ((rte_mem_page_size() - 1) & priv->caps.doorbell_bar_offset)
            priv->td = mlx5_devx_cmd_create_td(ctx)
                td->obj = mlx5_glue->devx_obj_create
            priv->tiss[i] = mlx5_devx_cmd_create_tis(ctx, &tis_attr) -> Create TIS using DevX API
                opcode, MLX5_CMD_OP_CREATE_TIS
            priv->null_mr = mlx5_glue->alloc_null_mr(priv->cdev->pd)
            priv->steer.domain = mlx5_glue->dr_create_domain(ctx, MLX5DV_DR_DOMAIN_TYPE_NIC_RX)
            priv->steer.tbl = mlx5_glue->dr_create_flow_tbl(priv->steer.domain, 0)
            mlx5_vdpa_err_event_setup
                priv->err_chnl = mlx5_glue->devx_create_event_channel(priv->cdev->ctx, 0)
                fcntl(priv->err_chnl->fd, F_SETFL, flags | O_NONBLOCK)
                priv->err_intr_handle = rte_intr_instance_alloc(RTE_INTR_INSTANCE_F_SHARED)
                rte_intr_fd_set(priv->err_intr_handle, priv->err_chnl->fd)
                rte_intr_callback_register(priv->err_intr_handle, mlx5_vdpa_err_interrupt_handler, priv)
                    mlx5_vdpa_virtq_query(priv, vq_index)
                        mlx5_devx_cmd_query_virtq(virtq->virtq, &attr) -> MLX5_CMD_OP_QUERY_GENERAL_OBJECT
                        rte_vhost_set_vring_base(priv->vid, index, attr.hw_available_index, attr.hw_used_index)
                            if (vq_is_packed(dev)) {
                                vq->last_avail_idx = last_avail_idx & 0x7fff;
                                vq->avail_wrap_counter = !!(last_avail_idx & (1 << 15));
                                vq->last_used_idx = last_used_idx & 0x7fff;
                                vq->used_wrap_counter = !!(last_used_idx & (1 << 15));
                            } else {
                                vq->last_avail_idx = last_avail_idx;
                                vq->last_used_idx = last_used_idx;
                            }
                    mlx5_vdpa_virtq_enable(priv, vq_index, 0) ->  Disable vq
                        if (virtq->enable == !!enable) -> same status
                            mlx5_vdpa_virtq_is_modified(priv, virtq)
                                if (vq.size != virtq->vq_size || vq.kickfd != rte_intr_fd_get(virtq->intr_handle))
                            mlx5_vdpa_steer_update(priv, false)
                        else mlx5_vdpa_is_pre_created_vq_mismatch(priv, virtq)
                        if (enable)
                            mlx5_vdpa_virtq_setup(priv, index, true)
                                rte_vhost_get_vhost_vring(priv->vid, index, &vq)
                                mlx5_vdpa_virtq_sub_objs_prepare(priv, &attr, &vq, index, false)
                                virtq->virtq = mlx5_devx_cmd_create_virtq(priv->cdev->ctx, &attr) -> MLX5_CMD_OP_CREATE_GENERAL_OBJECT
                                    devx_obj_create
                                attr.state = MLX5_VIRTQ_STATE_RDY
                                mlx5_devx_cmd_modify_virtq(virtq->virtq, &attr) -> MLX5_CMD_OP_MODIFY_GENERAL_OBJECT
                                claim_zero(rte_vhost_enable_guest_notification(priv->vid, index, 1))
                                    vq = dev->virtqueue[queue_id]
                                    vhost_enable_guest_notification
                                        vhost_enable_notify_packed
                                            flags = VRING_EVENT_F_ENABLE
                                            if (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX))
                                                flags = VRING_EVENT_F_DESC
                                            vq->device_event->flags = flags
                                        or vhost_enable_notify_split
                                virtq->virtio_version_1_0 = attr.virtio_version_1_0
                                mlx5_vdpa_virtq_doorbell_setup(virtq, &vq, index) -> Setup doorbell mapping -> HW virtq 对象表示 VIRTIO_NET virtqueue 的模拟上下文，该上下文由 VIRTIO_NET 驱动程序创建和管理，如 VIRTIO 规范中定义。添加支持以根据 rte_vhost 配置准备和释放用户 virtqs 模拟所需的所有基本硬件资源。此补丁准备了 DevX 命令创建 virtq 所需的基本配置。添加新文件 mlx5_vdpa_virtq.c 来管理 virtq 操作
                                    virtq->intr_handle = mlx5_os_interrupt_handler_create(RTE_INTR_INSTANCE_F_SHARED, false,vq->kickfd, mlx5_vdpa_virtq_kick_handler, virtq)
                                            rte_intr_callback_register(tmp_intr_handle, cb, cb_arg)
                                        mlx5_vdpa_virtq_kick_handler
                                            rte_intr_fd_get(virtq->intr_handle)
                                            nbytes = read(rte_intr_fd_get(virtq->intr_handle), &buf, 8)
                                            rte_write32(virtq->index, priv->virtq_db_addr)
                                            rte_vhost_host_notifier_ctrl(priv->vid, virtq->index, true) -> Enable/Disable host notifier mapping for a vdpa port
                                Subscribe virtq error event
                                mlx5_glue->devx_subscribe_devx_event(priv->err_chnl, virtq->virtq->obj,sizeof(event_num), &event_num, cookie)
                                if (virtq->eqp.cq.callfd != -1)
                                    eventfd_write(virtq->eqp.cq.callfd, (eventfd_t)1) -> Initial notification to ask Qemu handling completed buffers
                            mlx5_vdpa_steer_update(priv, false)
                                mlx5_vdpa_rqt_prepare
                                    priv->steer.rqt = mlx5_devx_cmd_create_rqt(priv->cdev->ctx,attr) -> MLX5_CMD_OP_CREATE_RQT
                                    else mlx5_devx_cmd_modify_rqt(priv->steer.rqt, attr) -> MLX5_CMD_OP_MODIFY_RQT
                                mlx5_vdpa_rss_flows_create
                                    dv_create_flow_matcher
                                    dv_create_flow
                    mlx5_vdpa_virtq_enable(priv, vq_index, 1) -> enable vq
            mlx5_vdpa_event_qp_global_prepare
                mlx5_os_devx_create_event_channel
                mlx5_devx_uar_prepare
                    const size_t page_size = rte_mem_page_size()
                    uar_obj = mlx5_devx_alloc_uar(cdev)
                    uar_mmap_offset = mlx5_os_get_devx_uar_mmap_offset(uar_obj);
                    base_addr = mlx5_os_get_devx_uar_base_addr(uar_obj);
                    uar->dbnc = mlx5_db_map_type_get(uar_mmap_offset, page_size);
                    uar->bf_db.db = mlx5_os_get_devx_uar_reg_addr(uar_obj);
            mlx5_vdpa_virtq_resource_prepare
                (mlx5_vdpa_task_add(priv, thrd_idx, MLX5_VDPA_TASK_PREPARE_VIRTQ, &remaining_cnt, &err_cnt, (void **)&data, 1)
                    mlx5_vdpa_c_thrd_ring_enqueue_bulk
                        rte_ring_enqueue_bulk_elem_start(r, n, free)
                        rte_ring_enqueue_elem_finish(r, obj, sizeof(struct mlx5_vdpa_task), n)
                mlx5_vdpa_virtq_single_resource_prepare(priv, main_task_idx[index])
                    mlx5_vdpa_virtq_sub_objs_prepare
                        mlx5_vdpa_event_qp_prepare
                        (mlx5_vdpa_cq_create(priv, log_desc_n, callfd, virtq) || !eqp->cq.cq_obj.cq)
                        mlx5_ts_format_conv
                        mlx5_devx_cmd_create_qp
                        mlx5_os_get_devx_uar_page_id
                        mlx5_devx_qp_create
                        rte_write32(rte_cpu_to_be_32(RTE_BIT32(log_desc_n)), &eqp->sw_qp.db_rec[0])
                    mlx5_vdpa_is_modify_virtq_supported
                    mlx5_devx_cmd_create_virtq
                (mlx5_vdpa_c_thread_wait_bulk_tasks_done(&remaining_cnt, &err_cnt, 2000)
                    rte_delay_us_sleep(sleep_time)
        priv->vdev = rte_vdpa_register_device(cdev->dev, &mlx5_vdpa_ops)
    .remove = mlx5_vdpa_dev_remove,
};



vhost-user -> vdpa, 如: 通过virtio-net的vdpa_dev_id获取到对应的vDPA设备，并调用对应的vDPA的set_features函数
static struct rte_vdpa_dev_ops mlx5_vdpa_ops = { -> vhost api wrap vdpa api
    .get_queue_num = mlx5_vdpa_get_queue_num,   <- rte_vhost_driver_get_queue_num -> if (vdpa_dev->ops->get_queue_num(vdpa_dev, &vdpa_queue_num) < 0)
        *queue_num = priv->caps.max_num_virtio_queues / 2
    .get_features = mlx5_vdpa_get_vdpa_features,
    .get_protocol_features = mlx5_vdpa_get_protocol_features,
    .dev_conf = mlx5_vdpa_dev_config,
        mlx5_vdpa_mem_register(priv) -> 这里的目标是将 virtio 设备的所有物理内存区域分组到一个间接 mkey 中。对于 KLM 固定缓冲区大小模式（HW 根据客户物理地址在一次读取中找到转换条目）：它的所有子直接 mkey 必须具有相同的大小，因此，它们中的每一个都应该在所有 virtio 内存区域和它们之间的空洞的 GCD 大小内。对于 KLM 模式（每个条目的大小可能不同，因此 HW 必须迭代条目）：每个 virtio 内存区域和它们之间的每个空洞都有一个条目，只需通过拆分其关联内存区域大于 2G 的条目来覆盖最大允许大小（2G）。这意味着每个 virtio 内存区域可以在 2 种模式下映射到多个直接 mkey。出于安全考虑，virtio 内存区域之间的所有无效内存空洞都将映射到空内存区域
            struct rte_vhost_memory *mem = mlx5_vdpa_vhost_mem_regions_prepare
            mlx5_vdpa_mem_cmp(mem, priv->vmem_info.vmem)
            priv->vmem_info.vmem = mem
            mrs = rte_zmalloc("mlx5 vDPA memory regions"
            mlx5_vdpa_task_add(priv, thrd_idx, MLX5_VDPA_TASK_REG_MR,
        mlx5_vdpa_virtqs_prepare
        mlx5_vdpa_steer_setup
        mlx5_vdpa_cqe_event_setup
            pthread_create(&priv->timer_tid, attrp, mlx5_vdpa_event_handle, (void *)priv) -> vdpa/mlx5：改进中断管理，驱动程序应通知客户机 CQ 轮询检测到的每个流量突发。CQ 轮询触发器由“event_mode”设备参数定义，要么通过对所有 CQ 进行忙轮询，要么通过使用 DevX 通道对 HW 完成事件进行阻塞调用。此外，当流量速率较低时，轮询事件模式可以转移到阻塞调用。当前阻塞调用使用 EAL 中断 API，在 API 管理中承受大量开销，并且仅使用单个线程为所有驱动程序和库提供服务。使用 DevX 通道的阻塞 FD 以便通过 DevX 通道 FD 机制直接进行阻塞调用
                switch (priv->event_mode) {
                case MLX5_VDPA_EVENT_MODE_DYNAMIC_TIMER:
                case MLX5_VDPA_EVENT_MODE_FIXED_TIMER:
                    while (1)
                        max = mlx5_vdpa_queues_complete(priv)
                        mlx5_vdpa_arm_all_cqs
                case MLX5_VDPA_EVENT_MODE_ONLY_INTERRUPT
                    do
                        virtq = mlx5_vdpa_event_wait(priv)
                            mlx5_glue->devx_get_event(priv->eventc, &out.event_resp, sizeof(out.buf))
                        mlx5_vdpa_queues_complete
                            mlx5_vdpa_queue_complete
                                comp = mlx5_vdpa_cq_poll(cq)
                                    last_word.word = rte_read32(&cq->cq_obj.cqes[0].wqe_counter)
                                    cq->cq_obj.db_rec[0] = rte_cpu_to_be_32(cq->cq_ci)
                                    eqp->sw_qp.db_rec[0] = rte_cpu_to_be_32(eqp->qp_pi + cq_size)
                                eventfd_write(cq->callfd, (eventfd_t)1)
                        mlx5_vdpa_cq_arm(priv, &virtq->eqp.cq)
        priv->state = MLX5_VDPA_STATE_CONFIGURED
    .dev_close = mlx5_vdpa_dev_close,
    .dev_cleanup = mlx5_vdpa_dev_cleanup,
    .set_vring_state = mlx5_vdpa_set_vring_state,
    .set_features = mlx5_vdpa_features_set,
    .migration_done = NULL,
    .get_vfio_group_fd = NULL,
    .get_vfio_device_fd = mlx5_vdpa_get_device_fd,
    .get_notify_area = mlx5_vdpa_get_notify_area,
        *offset = priv->var->mmap_off;
        *size = priv->var->length;
    .get_stats_names = mlx5_vdpa_get_stats_names,
    .get_stats = mlx5_vdpa_get_stats,
        mlx5_vdpa_virtq_stats_get(priv, qid, stats, n)
            mlx5_devx_cmd_query_virtio_q_counters(virtq->counters, attr)
                opcode MLX5_CMD_OP_QUERY_GENERAL_OBJECT
    .reset_stats = mlx5_vdpa_reset_stats,
};





helloworld -> examples/helloworld/main.c
main(int argc, char **argv)
rte_eal_init
    rte_eal_get_configuration
    eal_get_internal_configuration
    rte_cpu_is_supported
    eal_reset_internal_config
    eal_log_level_parse
    eal_save_args
    rte_eal_cpu_init
    eal_parse_args
    eal_plugins_init
    ...
    rte_config_init
        ...
        pathname -> /run/user/1020/dpdk/rte/config
        ...
        eal_mem_reserve
            addr:0x100000000 size: 28672
            ...
            mmap(requested_addr, size, prot, flags, fd, offset);
    rte_eal_using_phys_addrs
        rte_eal_has_hugepages -> no_hugetlbfs -> default use hugepage
        rte_mem_virt2phy
            return RTE_BAD_IOVA
    rte_bus_get_iommu_class
    if (internal_conf->no_hugetlbfs == 0)
        hugepage_info_init
        create_shared_memory
rte_eal_remote_launch(lcore_hello, NULL, lcore_id)
lcore_hello


map_all_hugepages




map_shared_memory
    open(filename, flags, 0600)
    ftruncate(fd, mem_size)
    retval = mmap(NULL, mem_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);


eal_log_init
    log_stream = fopencookie(NULL, "w+", console_log_func)
    openlog(id, LOG_NDELAY | LOG_PID, facility)
    eal_log_set_default


rte_eal_vfio_setup
    rte_vfio_enable("vfio")
        for (i = 0; i < VFIO_MAX_CONTAINERS; i++) -> 64
            for (j = 0; j < VFIO_MAX_GROUPS; j++) -> 64
        vfio_available = rte_eal_check_module(modname) -> vfio：避免在模块未加载时启用，当内核支持 vfio 功能时未加载 vfio 模块时，例程仍尝试打开容器以获取文件描述。 此操作不安全，当然会收到错误消息： EAL: Detected 40 lcore(s) EAL: unsupported IOMMU type! EAL: VFIO 支持无法初始化 EAL: 设置内存...这可能会让用户感到困惑，这个补丁使用户更合理、更流畅
        /sys/module/vfio
        default_vfio_cfg->vfio_container_fd = rte_vfio_get_container_fd()
            open(VFIO_CONTAINER_PATH, O_RDWR) -> open /dev/vfio/vfio
        or vfio_get_default_container_fd
        static const struct vfio_iommu_type iommu_types[]




rte_eal_memzone_init
    rte_fbarray_init(&mcfg->memzones, "memzone",
    fully_validate
    rte_mem_page_size
    eal_get_virtual_area
    eal_get_fbarray_path
    eal_file_open
    resize_and_map
    map_addr = rte_mem_map(addr, len, RTE_PROT_READ | RTE_PROT_WRITE, RTE_MAP_SHARED | RTE_MAP_FORCE_ADDRESS, fd, 0);
    TAILQ_INSERT_TAIL(&mem_area_tailq, ma, next)


rte_eal_memory_init
    rte_eal_memseg_init
    #ifndef RTE_ARCH_64
        memseg_primary_init_32
            get_mem_amount
        memseg_primary_init -> eal_dynmem_memseg_lists_init
            eal_memseg_list_init
                eal_memseg_list_init_named
            eal_memseg_list_alloc
    eal_memalloc_init
        (rte_memseg_list_walk(fd_list_create_walk, NULL)
            alloc_list

    rte_eal_hugepage_init
        eal_legacy_hugepage_init or
        eal_dynmem_hugepage_init
            eal_dynmem_calc_num_pages_per_socket
        eal_memalloc_alloc_seg_bulk
            rte_memseg_list_walk_thread_unsafe(alloc_seg_walk, &wa)
            alloc_seg
                addr = eal_mem_alloc_socket(alloc_sz, socket_id)
                fallocate ?
    or rte_eal_hugepage_attach
    rte_eal_memdevice_init






malloc_socket
    malloc_heap_alloc
        malloc_heap_alloc_on_heap_id
            alloc_more_mem_on_socket
                try_expand_heap
                    try_expand_heap_primary
                        alloc_pages_on_heap
                            eal_memalloc_alloc_seg_bulk
                            rte_mem_check_dma_mask_thread_unsafe
                                check_dma_mask
                                    rte_memseg_walk_thread_unsafe(check_iova, &mask)
                                        iova = (ms->iova + ms->len) - 1
                                        if (!(iova & *mask))
                        eal_memalloc_mem_event_notify(RTE_MEM_EVENT_ALLOC, map_addr, alloc_sz)
                    or try_expand_heap_secondary


rte_memzone_reserve
rte_memzone_reserve_aligned
rte_memzone_reserve_bounded
    rte_memzone_reserve_thread_safe
        memzone_reserve_aligned_thread_unsafe
            malloc_heap_alloc



rte_eal_malloc_heap_init
    malloc_add_seg


RTE_LCORE_FOREACH_WORKER(i)
    pipe(lcore_config[i].pipe_main2worker
    eal_worker_thread_create
        if (pthread_create(&lcore_config[lcore_id].thread_id, attrp, eal_thread_loop, (void *)(uintptr_t)lcore_id) == 0)
            eal_thread_wait_command
                n = read(m2w, &c, 1);
            eal_thread_ack_command
                n = write(w2m, &c, 1) <- eal_thread_wake_worker
    pthread_setaffinity_np
    rte_eal_mp_remote_launch(sync_func, NULL, SKIP_MAIN)
        rte_eal_remote_launch(f, arg, lcore_id)
            __atomic_store_n(&lcore_config[worker_id].f, f, __ATOMIC_RELEASE)
            eal_thread_wake_worker
    rte_eal_mp_wait_lcore


rte_service_init
    rte_services = rte_calloc("rte_services",


rte_bus_probe
    TAILQ_FOREACH(bus, &rte_bus_list, next) <- rte_bus_register <- RTE_REGISTER_BUS
        bus->probe()
    vbus->probe()
        auxiliary_probe(void)
        rte_dpaa_bus_probe
        rte_fslmc_probe
        ifpga_probe
        pci_probe(void)

vfio_mp_sync_setup
    rte_mp_action_register
        name: eal_vfio_mp_sync
        entry = malloc(sizeof(struct action_entry));
        find_action_entry_by_name
        TAILQ_INSERT_TAIL(&action_entry_list, entry, next)


rte_service_start_with_defaults

eal_clean_runtime_dir





EAL: Multi-process socket /var/run/dpdk/rte/mp_socket



struct rte_pci_bus rte_pci_bus = {
    .bus = {
        .scan = rte_pci_scan,
        .probe = pci_probe,
        .cleanup = pci_cleanup,
        .find_device = pci_find_device,
        .plug = pci_plug,
        .unplug = pci_unplug,
        .parse = pci_parse,
        .devargs_parse = rte_pci_devargs_parse,
        .dma_map = pci_dma_map,
        .dma_unmap = pci_dma_unmap,
        .get_iommu_class = rte_pci_get_iommu_class,
        .dev_iterate = rte_pci_dev_iterate,
        .hot_unplug_handler = pci_hot_unplug_handler,
        .sigbus_handler = pci_sigbus_handler,
    },
    .device_list = TAILQ_HEAD_INITIALIZER(rte_pci_bus.device_list),
    .driver_list = TAILQ_HEAD_INITIALIZER(rte_pci_bus.driver_list),
};


drivers/common/mlx5/mlx5_common_pci.c


dpdk init module,
drivers/net/mlx5/mlx5.c
RTE_INIT(rte_mlx5_pmd_init)
    mlx5_common_init
        mlx5_glue_constructor
            mlx5_glue->fork_init() -> ibv_fork_init
        mlx5_common_driver_init -> common/mlx5：添加总线无关层，为了支持辅助总线，引入通用设备驱动程序和回调，应该取代mlx5通用PCI总线驱动程序。 Mlx5 类驱动程序，即 eth、vDPA、regex 和 compress 通常使用单个 Verbs 设备上下文来探测设备。 如果设备是 PCI 总线设备，则 Verbs 设备来自 PCI 地址；如果设备是辅助总线设备，则来自 Auxiliary sysfs。 目前仅支持 PCI 总线。 通用设备驱动程序是 mlx5 类驱动程序和总线之间的中间层，为类驱动程序解析和抽象总线信息到 Verbs 设备。 PCI总线驱动程序和辅助总线驱动程序都可以利用公共驱动程序层将总线操作转换为mlx5类驱动程序。 mlx5 eth、vDPA、正则表达式和压缩 PMD 仍在使用旧版 mlx5 通用 PCI 总线驱动程序，一旦所有 PMD 驱动程序迁移到新的通用驱动程序，该驱动程序将被删除
            mlx5_common_pci_init
                pci_ids_table_update -> 所有 mlx5 PMD 构造函数都以相同的优先级运行。 所以任何一个PMD，包括这个，都可以先注册PCI表。 如果任何其他 PMD 已注册 PCI ID 表，则无需注册空的默认表
                    pci_id_table_size_get
                    pci_id_insert(updated_table, &i, driver_id_table)
                    mlx5_pci_id_table = updated_table
                rte_pci_register(&mlx5_common_pci_driver)
                    TAILQ_INSERT_TAIL(&rte_pci_bus.driver_list, driver, next)
            mlx5_common_auxiliary_init
                rte_auxiliary_register(&mlx5_auxiliary_driver)
    mlx5_set_ptype_table -> Build a table to translate Rx completion flags to packet type
        uint32_t (*p)[RTE_DIM(mlx5_ptype_table)] = &mlx5_ptype_table
        bit[1:0] = l3_hdr_type
        bit[4:2] = l4_hdr_type
        bit[5] = ip_frag
        bit[6] = tunneled
        bit[7] = outer_l3_type
    mlx5_set_cksum_table -> Build a table to translate packet to checksum type of Verbs
        bit[0] = RTE_MBUF_F_TX_TCP_SEG
        bit[2:3] = RTE_MBUF_F_TX_UDP_CKSUM, RTE_MBUF_F_TX_TCP_CKSUM
        bit[4] = RTE_MBUF_F_TX_IP_CKSUM
        bit[8] = RTE_MBUF_F_TX_OUTER_IP_CKSUM
        bit[9] = tunnel
    mlx5_set_swp_types_table -> Build a table to translate packet type of mbuf to SWP type of Verbs
        bit[0:1] = RTE_MBUF_F_TX_L4_MASK
        bit[4] = RTE_MBUF_F_TX_IPV6
        bit[8] = RTE_MBUF_F_TX_OUTER_IPV6
        bit[9] = RTE_MBUF_F_TX_OUTER_UDP
    mlx5_class_driver_register(&mlx5_net_driver)


static struct rte_auxiliary_driver mlx5_auxiliary_driver = {
    .driver = {
           .name = MLX5_AUXILIARY_DRIVER_NAME,
    },
    .match = mlx5_common_auxiliary_match,
    .probe = mlx5_common_auxiliary_probe,
        dev->device.numa_node = mlx5_auxiliary_get_numa(dev)
        mlx5_common_dev_probe
    .remove = mlx5_common_auxiliary_remove,
    .dma_map = mlx5_common_auxiliary_dma_map,
    .dma_unmap = mlx5_common_auxiliary_dma_unmap,
};

static struct mlx5_class_driver mlx5_net_driver = {
    .drv_class = MLX5_CLASS_ETH,
    .name = RTE_STR(MLX5_ETH_DRIVER_NAME),
    .id_table = mlx5_pci_id_map,
    .probe = mlx5_os_net_probe,
    .remove = mlx5_net_remove,
    .probe_again = 1,
    .intr_lsc = 1,
    .intr_rmv = 1,
};



__rte_cache_aligned
const struct mlx5_glue *mlx5_glue = &(const struct mlx5_glue) {
    .version = MLX5_GLUE_VERSION,
    .fork_init = mlx5_glue_fork_init,
    .alloc_pd = mlx5_glue_alloc_pd,
    .dealloc_pd = mlx5_glue_dealloc_pd,
    .import_pd = mlx5_glue_import_pd,
    .unimport_pd = mlx5_glue_unimport_pd,
    .get_device_list = mlx5_glue_get_device_list,
    .free_device_list = mlx5_glue_free_device_list,
    .open_device = mlx5_glue_open_device,
    .import_device = mlx5_glue_import_device,
        #ifdef HAVE_MLX5_IBV_IMPORT_CTX_PD_AND_MR
            return ibv_import_device(cmd_fd);
    .close_device = mlx5_glue_close_device,
    .query_device = mlx5_glue_query_device,
    .query_device_ex = mlx5_glue_query_device_ex,
    .get_device_name = mlx5_glue_get_device_name,
    .query_rt_values_ex = mlx5_glue_query_rt_values_ex,
    .query_port = mlx5_glue_query_port,
    .create_comp_channel = mlx5_glue_create_comp_channel,
    .destroy_comp_channel = mlx5_glue_destroy_comp_channel,
    .create_cq = mlx5_glue_create_cq,
    .destroy_cq = mlx5_glue_destroy_cq,
    .get_cq_event = mlx5_glue_get_cq_event,
    .ack_cq_events = mlx5_glue_ack_cq_events,
    .create_rwq_ind_table = mlx5_glue_create_rwq_ind_table,
    .destroy_rwq_ind_table = mlx5_glue_destroy_rwq_ind_table,
    .create_wq = mlx5_glue_create_wq,
    .destroy_wq = mlx5_glue_destroy_wq,
    .modify_wq = mlx5_glue_modify_wq,
    .create_flow = mlx5_glue_create_flow,
    .destroy_flow = mlx5_glue_destroy_flow,
    .destroy_flow_action = mlx5_glue_destroy_flow_action,
    .create_qp = mlx5_glue_create_qp,
    .create_qp_ex = mlx5_glue_create_qp_ex,
    .destroy_qp = mlx5_glue_destroy_qp,
    .modify_qp = mlx5_glue_modify_qp,
    .reg_mr = mlx5_glue_reg_mr,
    .reg_mr_iova = mlx5_glue_reg_mr_iova,
    .alloc_null_mr = mlx5_glue_alloc_null_mr,
        #ifdef HAVE_IBV_DEVX_OBJ
            ibv_alloc_null_mr
        (void)pd
    .dereg_mr = mlx5_glue_dereg_mr,
    .create_counter_set = mlx5_glue_create_counter_set,
    .destroy_counter_set = mlx5_glue_destroy_counter_set,
    .describe_counter_set = mlx5_glue_describe_counter_set,
    .query_counter_set = mlx5_glue_query_counter_set,
    .create_counters = mlx5_glue_create_counters,
    .destroy_counters = mlx5_glue_destroy_counters,
    .attach_counters = mlx5_glue_attach_counters,
    .query_counters = mlx5_glue_query_counters,
    .ack_async_event = mlx5_glue_ack_async_event,
    .get_async_event = mlx5_glue_get_async_event,
    .port_state_str = mlx5_glue_port_state_str,
    .cq_ex_to_cq = mlx5_glue_cq_ex_to_cq,
    .dr_create_flow_action_dest_flow_tbl =
        mlx5_glue_dr_create_flow_action_dest_flow_tbl,
    .dr_create_flow_action_dest_port =
        mlx5_glue_dr_create_flow_action_dest_port,
    .dr_create_flow_action_drop =
        mlx5_glue_dr_create_flow_action_drop,
    .dr_create_flow_action_push_vlan =
        mlx5_glue_dr_create_flow_action_push_vlan,
    .dr_create_flow_action_pop_vlan =
        mlx5_glue_dr_create_flow_action_pop_vlan,
    .dr_create_flow_tbl = mlx5_glue_dr_create_flow_tbl,
        mlx5dv_dr_table_create(domain, level)
    .dr_destroy_flow_tbl = mlx5_glue_dr_destroy_flow_tbl,
    .dr_create_domain = mlx5_glue_dr_create_domain,
        mlx5dv_dr_domain_create(ctx, domain)
    .dr_destroy_domain = mlx5_glue_dr_destroy_domain,
    .dr_sync_domain = mlx5_glue_dr_sync_domain,
    .dv_create_cq = mlx5_glue_dv_create_cq,
    .dv_create_wq = mlx5_glue_dv_create_wq,
    .dv_query_device = mlx5_glue_dv_query_device,
    .dv_set_context_attr = mlx5_glue_dv_set_context_attr,
        mlx5dv_set_context_attr
    .dv_init_obj = mlx5_glue_dv_init_obj,
    .dv_create_qp = mlx5_glue_dv_create_qp,
    .dv_create_flow_matcher = mlx5_glue_dv_create_flow_matcher,
        mlx5dv_dr_matcher_create
    .dv_create_flow_matcher_root = __mlx5_glue_dv_create_flow_matcher,
    .dv_create_flow = mlx5_glue_dv_create_flow,
        mlx5dv_create_flow
    .dv_create_flow_root = __mlx5_glue_dv_create_flow,
    .dv_create_flow_action_counter =
        mlx5_glue_dv_create_flow_action_counter,
    .dv_create_flow_action_dest_ibv_qp =
        mlx5_glue_dv_create_flow_action_dest_ibv_qp,
    .dv_create_flow_action_dest_devx_tir =
        mlx5_glue_dv_create_flow_action_dest_devx_tir,
    .dv_create_flow_action_modify_header =
        mlx5_glue_dv_create_flow_action_modify_header,
    .dv_create_flow_action_modify_header_root =
        __mlx5_glue_dv_create_flow_action_modify_header,
    .dv_create_flow_action_packet_reformat =
        mlx5_glue_dv_create_flow_action_packet_reformat,
    .dv_create_flow_action_packet_reformat_root =
        __mlx5_glue_dv_create_flow_action_packet_reformat,
    .dv_create_flow_action_tag =  mlx5_glue_dv_create_flow_action_tag,
    .dv_create_flow_action_meter = mlx5_glue_dv_create_flow_action_meter,
    .dv_modify_flow_action_meter = mlx5_glue_dv_modify_flow_action_meter,
    .dv_create_flow_action_aso = mlx5_glue_dv_create_flow_action_aso,
    .dr_create_flow_action_default_miss =
        mlx5_glue_dr_create_flow_action_default_miss,
    .dv_destroy_flow = mlx5_glue_dv_destroy_flow,
    .dv_destroy_flow_matcher = mlx5_glue_dv_destroy_flow_matcher,
    .dv_destroy_flow_matcher_root = __mlx5_glue_dv_destroy_flow_matcher,
    .dv_open_device = mlx5_glue_dv_open_device,
    .devx_obj_create = mlx5_glue_devx_obj_create,
        devx_obj->obj = devx_obj_create -> mlx5dv_devx_obj_create -> libibverbs-dev api
    .devx_obj_destroy = mlx5_glue_devx_obj_destroy,
    .devx_obj_query = mlx5_glue_devx_obj_query,
    .devx_obj_modify = mlx5_glue_devx_obj_modify,
    .devx_general_cmd = mlx5_glue_devx_general_cmd,
    .devx_create_cmd_comp = mlx5_glue_devx_create_cmd_comp,
    .devx_destroy_cmd_comp = mlx5_glue_devx_destroy_cmd_comp,
    .devx_obj_query_async = mlx5_glue_devx_obj_query_async,
    .devx_get_async_cmd_comp = mlx5_glue_devx_get_async_cmd_comp,
    .devx_umem_reg = mlx5_glue_devx_umem_reg,
    .devx_umem_dereg = mlx5_glue_devx_umem_dereg,
    .devx_qp_query = mlx5_glue_devx_qp_query,
    .devx_wq_query = mlx5_glue_devx_wq_query,
    .devx_port_query = mlx5_glue_devx_port_query,
    .dr_dump_domain = mlx5_glue_dr_dump_domain,
    .dr_dump_rule = mlx5_glue_dr_dump_single_rule,
    .dr_reclaim_domain_memory = mlx5_glue_dr_reclaim_domain_memory,
    .dr_create_flow_action_sampler =
        mlx5_glue_dr_create_flow_action_sampler,
    .dr_create_flow_action_dest_array =
        mlx5_glue_dr_action_create_dest_array,
    .dr_allow_duplicate_rules = mlx5_glue_dr_allow_duplicate_rules,
    .devx_query_eqn = mlx5_glue_devx_query_eqn,
    .devx_create_event_channel = mlx5_glue_devx_create_event_channel,
        mlx5dv_devx_create_event_channel(ctx, flags)
    .devx_destroy_event_channel = mlx5_glue_devx_destroy_event_channel,
    .devx_subscribe_devx_event = mlx5_glue_devx_subscribe_devx_event,
    .devx_subscribe_devx_event_fd = mlx5_glue_devx_subscribe_devx_event_fd,
    .devx_get_event = mlx5_glue_devx_get_event,
    .devx_alloc_uar = mlx5_glue_devx_alloc_uar,
    .devx_free_uar = mlx5_glue_devx_free_uar,
    .dv_alloc_var = mlx5_glue_dv_alloc_var, -> var: Virtio access region(VAR)
        mlx5dv_alloc_var(context, flags)
    .dv_free_var = mlx5_glue_dv_free_var,
    .dv_alloc_pp = mlx5_glue_dv_alloc_pp,
    .dv_free_pp = mlx5_glue_dv_free_pp,
    .dr_create_flow_action_send_to_kernel =
        mlx5_glue_dr_create_flow_action_send_to_kernel,
};


mlx5_os_get_ibv_dev
mlx5_os_get_ibv_device


rte_telemetry_init
    telemetry_v2_init
        rte_telemetry_register_cmd("/"
            callbacks[i].fn = fn
        rte_telemetry_register_cmd("/info"
        v2_socket.sock = create_socket(v2_socket.path)
        pthread_create(&t_new, NULL, socket_listener, &v2_socket)
            int s_accepted = accept(s->sock, NULL, NULL)
            pthread_create(&th, NULL, s->fn,
    or telemetry_legacy_init



rte_log_register_type_and_pick_level


eal_mcfg_complete
    internal_conf->init_complete = 1;





verification/st/dpdk/st_dpdk.c -> main -> rx
rte_eal_init
sprintf(dut2dpi_addr, "/home/common/st_socket/%s/dut2dpi", argv[1]);
sprintf(dpi2dut_addr, "/home/common/st_socket/%s/dpi2dut", argv[1]);
rte_eth_dev_count_avail
rte_pktmbuf_pool_create("MBUF_POOL", NUM_MBUFS * nb_ports,
RTE_ETH_FOREACH_DEV
    port_init
        rte_eth_dev_info_get
        rte_eth_dev_configure
        rte_eth_dev_adjust_nb_rx_tx_desc
        rte_eth_rx_queue_setup
        rte_eth_dev_socket_id
        rte_eth_tx_queue_setup
        rte_eth_dev_socket_id
        rte_eth_dev_start
        rte_eth_macaddr_get
        rte_eth_promiscuous_enable
RTE_LCORE_FOREACH_WORKER
    rte_eal_remote_launch(lcore_dpdk2eda_rx, NULL, lcore_id)
        init_server
        RTE_ETH_FOREACH_DEV(port)
            rte_eth_rx_burst(port, 0,
            deal_dpdk2eda_msg -> deal_eda_dpdk_msg
                epoll_wait
                rqe_alloc_db
                deal_listenfd_epollin
                deal_clientfd_epollin
                edamsg2pkt
                    rte_pktmbuf_reset_headroom
                    rte_memcpy(rte_pktmbuf_mtod_offset(pkt, char *, 0),
                pkt2edamsg
                epoll_ctl
lcore_eda2dpdk_tx(mbuf_pool) -> tx
    RTE_ETH_FOREACH_DEV(port)
        init_server(&listenfd, &epollfd, dut2dpi_addr) ->
    for (;;)
        RTE_ETH_FOREACH_DEV(port)
            deal_eda2dpdk_msg(listenfd, epollfd, bufs, mbuf_pool) -> deal_eda_dpdk_msg
                mbufs[trans_cnt] = rte_pktmbuf_alloc(mbuf_pool)
                edamsg2pkt
            rte_eth_tx_burst
rte_eal_mp_wait_lcore




struct rte_mbuf {
    ...
    void *buf_addr;
    ...
}



doca,
I used the following commands on the DPU to install dpdk:
wget https://fast.dpdk.org/rel/dpdk-22.11.2.tar.xz 1
tar xf dpdk-22.11.2.tar.xz
cd dpdk-stable-22.11.2
meson build
meson configure -Ddisable_drivers=regex/cn9k build
meson setup soc_build -Dplatform=bluefield
ninja -C build
ninja -C build install
pkg-config --modversion libdpdk

And then you can set the hugepages:
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages



vfio
./usertools/dpdk-devbind.py -b vfio-pci 06:00.3 06:00.4
usertools/dpdk-devbind.py -> main()
    check lspci
    parse_args()
    check_modules() <- dpdk_drivers = ["igb_uio", "vfio-pci", "uio_pci_generic"]
        module_is_loaded(mod["Name"])
            /sys/module/
    clear_data()
    get_device_details(network_devices)
    get_device_details(baseband_devices)
    get_device_details(crypto_devices)
    get_device_details(dma_devices)
    get_device_details(eventdev_devices)
    get_device_details(mempool_devices)
    get_device_details(compress_devices)
    get_device_details(regex_devices)
    get_device_details(misc_devices)
    do_arg_actions()
        bind_all(args, b_flag, force_flag) -> -b
            dev_id_from_dev_name(driver)
            for d in dev_list
                bind_one
                    if has_driver(dev_id)
                        unbind_one
                    /sys/bus/pci/devices/%s/driver_override
                    /sys/bus/pci/drivers/%s/new_id -> /sys/bus/pci/drivers/vfio-pci/new_id
                    /sys/bus/pci/drivers/%s/bind
                    f = open(filename, "a")
                    f.write(dev_id)
                if not exists("/sys/bus/pci/devices/%s/driver_override" % d)




mlx5_os_net_probe(struct mlx5_common_device *cdev, struct mlx5_kvargs_ctrl *mkvlist)
    mlx5_pmd_socket_init
        socket, fcntl, bind, listen -> #define MLX5_SOCKET_PATH "/var/tmp/dpdk_net_mlx5_%d"
        server_intr_handle = mlx5_os_interrupt_handler_create(RTE_INTR_INSTANCE_F_PRIVATE, false, server_socket, mlx5_pmd_socket_handle, NULL)
            tmp_intr_handle = rte_intr_instance_alloc(mode)
                intr_handle->efds = rte_zmalloc(NULL, RTE_MAX_RXTX_INTR_VEC_ID * sizeof(int), 0)
                intr_handle->elist = rte_zmalloc(NULL, RTE_MAX_RXTX_INTR_VEC_ID * sizeof(struct rte_epoll_event), 0)
                intr_handle->alloc_flags = flags
                intr_handle->nb_intr = RTE_MAX_RXTX_INTR_VEC_ID
            rte_intr_fd_set(tmp_intr_handle, fd) -> fd = kickfd/server_socket
                intr_handle->fd = fd
            rte_intr_type_set(tmp_intr_handle, RTE_INTR_HANDLE_EXT)
            rte_intr_callback_register
                callback->cb_fn = cb -> mlx5_pmd_socket_handle
                TAILQ_INSERT_TAIL(&(src->callbacks), callback, next)
                TAILQ_INSERT_TAIL(&intr_sources, src, next)
            return tmp_intr_handle
    mlx5_init_once
        mlx5_init_shared_data
        mlx5_mp_init_primary(MLX5_MP_NAME, mlx5_mp_os_primary_handle)
            rte_mp_action_register(name, primary_action)
                entry = malloc(sizeof(struct action_entry))
                TAILQ_INSERT_TAIL(&action_entry_list, entry, next)
        mlx5_mp_init_secondary(MLX5_MP_NAME, mlx5_mp_os_secondary_handle)
    mlx5_probe_again_args_validate
        mlx5_shared_dev_ctx_args_config(sh, mkvlist, config)
            config->cnt_svc.service_core = rte_get_main_lcore()
            mlx5_kvargs_process(mkvlist, params, mlx5_dev_args_check_handler, config)
            mlx5_devx_obj_ops_en
    mlx5_os_pci_probe
    or mlx5_os_auxiliary_probe
        mlx5_os_parse_eth_devargs
        mlx5_auxiliary_get_ifindex
        mlx5_dev_spawn
            rte_eth_dev_get_port_by_name
            mlx5_alloc_shared_dev_ctx
            mlx5_proc_priv_init
            eth_dev->rx_pkt_burst = mlx5_select_rx_function(eth_dev)
            eth_dev->tx_pkt_burst = mlx5_select_tx_function(eth_dev)
            priv->representor_id = mlx5_representor_id_encode(switch_info, eth_da->type)
            rte_eth_switch_domain_alloc
            mlx5_port_args_config
            rte_eth_dev_allocate
            mlx5_get_mtu
            eth_dev->rx_pkt_burst = rte_eth_pkt_burst_dummy;
            eth_dev->dev_ops = &mlx5_dev_ops
        rte_eth_dev_probing_finish



mlx5_pmd_socket_handle
    conn_sock = accept(server_socket, NULL, NULL)
    recvmsg(conn_sock, &msg, MSG_WAITALL)
    mlx5_flow_dev_dump
    ret = sendmsg(conn_sock, &msg, 0)


mlx5_mp_os_primary_handle
    mlx5_mp_os_handle_port_agnostic
    rte_eth_dev_is_valid_port
    case MLX5_MP_REQ_QUEUE_TX_START
        mlx5_tx_queue_start_primary
            txq_obj_modify -> mlx5_txq_devx_modify
                mlx5_devx_cmd_modify_sq
                    MLX5_SET(modify_sq_in, in, opcode, MLX5_CMD_OP_MODIFY_SQ)
                    ret = mlx5_glue->devx_obj_modify(sq->obj, in, sizeof(in), out, sizeof(out)) -> mlx5_glue_devx_obj_modify
                        return devx_cmd(GET_OBJ_CTX(obj), in, inlen, out, outlen) -> return execute_ioctl(((struct devx_context *)ctx)->cmd_fd, cmd) -> https://github.com/Mellanox/devx/blob/master/src/devx.c



rte_eth_dev_tx_queue_start(uint16_t port_id, uint16_t tx_queue_id)
    eth_dev_validate_tx_queue
    rte_eth_dev_is_tx_hairpin_queue
    dev->dev_ops->tx_queue_start(dev, tx_queue_id) -> mlx5_tx_queue_start
        txq_obj_modify




REGISTER_TEST_COMMAND(latencystats_autotest, test_latencystats)
    test_latency_packet_forward



dpdk test plan: https://doc.dpdk.org/dts/test_plans/index.html

e810 rx timestamp: https://doc.dpdk.org/dts/test_plans/rx_timestamp_perf_test_plan.html
 <build_dir>/app/dpdk-testpmd -l 5,6 -n 8 --force-max-simd-bitwidth=64 \
 -- -i --portmask=0x3 --rxq=1 --txq=1 --txd=1024 --rxd=1024 --forward=mac \
 --nb-cores=1 --enable-rx-timestamp

Note:
  -force-max-simd-bitwidth: Set 64, the feature only support 64.
  -enable-rx-timestamp: enable rx-timestamp.


使用 E810 对标头分割转发的性能进行基准测试: https://doc.dpdk.org/dts/test_plans/ice_header_split_perf_test_plan.html
bind: ./usertools/dpdk-devbind.py -b vfio-pci 17:00.0 4b:00.0
<build_dir>/app/dpdk-testpmd -l 5,6 -n 8 --force-max-simd-bitwidth=64 \
 -- -i --portmask=0x3 --rxq=1 --txq=1 --txd=1024 --rxd=1024 --forward=rxonly \
 --nb-cores=1 --mbuf-size=2048,2048
port start all
start


fs_tx_queue_start


testpmd + pdump

rte_ctrl_thread_create
    pthread_create(thread, attr, ctrl_thread_init, (void *)params) <- qemu connect vhost.socket
        return start_routine(routine_arg) -> fdset_event_dispatch
            rcb(fd, dat, &remove1) -> vhost_user_server_new_connection
                fd = accept(fd, NULL, NULL)
                vhost_user_add_connection(fd, vsocket)
                    vid = vhost_new_device()
                    vhost_setup_virtio_net
                    vhost_attach_vdpa_device
                        dev->vdpa_dev = vdpa_dev
                    ret = vsocket->notify_ops->new_connection(vid) -> vhost_blk_device_ops -> new_connection -> vhost_session_install_rte_compat_hooks
                        rte_vhost_extern_callback_register(vid, &g_extern_vhost_ops, NULL)
                    ret = fdset_add(&vhost_user.fdset, fd, vhost_user_read_cb, NULL, conn) -> vhost_user_read_cb
                        vhost_user_msg_handler
                            ret = read_vhost_message(dev, fd, &ctx)
                                read_fd_message
                                read(sockfd, &ctx->msg.payload, ctx->msg.size)
                            msg_handler = &vhost_message_handlers[request]
                            "read message %s\n", msg_handler->descriptionread -> message VHOST_USER_GET_FEATURES
                            vhost_user_check_and_alloc_queue_pair(dev, &ctx)
                                alloc_vring_queue(dev, vring_idx) -> vhost：将队列对转换为 vring，队列对是 virtio-net 特有的，其他设备没有这样的概念。为了使其通用，我们应该记录 vring 的数量而不是队列对的数量。此补丁只是进行简单的转换，后续补丁会将 vring 的数量导出到应用程序
                                    for (i = 0; i <= vring_idx; i++)
                                        vq = rte_zmalloc(NULL, sizeof(struct vhost_virtqueue), 0)
                                        dev->virtqueue[i] = vq
                            switch (request)
                            case VHOST_USER_SET_FEATURES:
                            case VHOST_USER_SET_PROTOCOL_FEATURES:
                            case VHOST_USER_SET_OWNER:
                            case VHOST_USER_SET_MEM_TABLE:
                            case VHOST_USER_SET_LOG_BASE:
                            case VHOST_USER_SET_LOG_FD:
                            case VHOST_USER_SET_VRING_NUM:
                            case VHOST_USER_SET_VRING_ADDR:
                            case VHOST_USER_SET_VRING_BASE:
                            case VHOST_USER_SET_VRING_KICK:
                            case VHOST_USER_SET_VRING_CALL:
                            case VHOST_USER_SET_VRING_ERR:
                            case VHOST_USER_SET_VRING_ENABLE:
                            case VHOST_USER_SEND_RARP:
                            case VHOST_USER_NET_SET_MTU:
                            case VHOST_USER_SET_SLAVE_REQ_FD:
                            msg_result = (*dev->extern_ops.pre_msg_handle)(dev->vid, &ctx)
                            msg_result = msg_handler->callback(&dev, &ctx, fd) -> vhost cb -> VHOST_MESSAGE_HANDLERS
                                -> vhost_user_get_features
                                    rte_vhost_driver_get_features(dev->ifname, &features)
                                        vdpa_dev->ops->get_features(vdpa_dev, &vdpa_features)
                            case RTE_VHOST_MSG_RESULT_REPLY:
                                send_vhost_reply(dev, fd, &ctx)
                            msg_result = (*dev->extern_ops.post_msg_handle)(dev->vid, &ctx)
                            if (dev->notify_ops->new_device(dev->vid) -> config vdpa device -> vdpa new device
                            if (vdpa_dev->ops->dev_conf(dev->vid)
                    TAILQ_INSERT_TAIL(&vsocket->conn_list, conn, next)



struct rte_vhost_user_extern_ops g_extern_vhost_ops = {
    .pre_msg_handle = extern_vhost_pre_msg_handler,
        rte_vhost_get_ifname(vid, path, PATH_MAX)
        ctrlr = vhost_blk_ctrlr_find(path)
        switch ((int)msg->request)
    .post_msg_handle = extern_vhost_post_msg_handler,
};


vhost protocol seq:
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) new vhost user connection is 1319
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) new device, handle is 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_GET_FEATURES
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_GET_PROTOCOL_FEATURES
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_PROTOCOL_FEATURES
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) negotiated Vhost-user protocol features: 0x11ebf
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_GET_QUEUE_NUM
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_SLAVE_REQ_FD
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_OWNER
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_GET_FEATURES
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_VRING_CALL
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) vring call idx:0 file:1321
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_VRING_ERR
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_FEATURES
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) negotiated Virtio features: 0x140000000
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_GET_STATUS
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_STATUS
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) new device status(0x00000008):
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket)      -RESET: 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket)      -ACKNOWLEDGE: 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket)      -DRIVER: 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket)      -FEATURES_OK: 1
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket)      -DRIVER_OK: 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket)      -DEVICE_NEED_RESET: 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket)      -FAILED: 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_INFLIGHT_FD
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) set_inflight_fd mmap_size: 2112
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) set_inflight_fd mmap_offset: 0
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) set_inflight_fd num_queues: 1
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) set_inflight_fd queue_size: 128
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) set_inflight_fd fd: 1322
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) set_inflight_fd pervq_inflight_size: 2112
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_VRING_CALL
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) vring call idx:0 file:1323
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_FEATURES
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) negotiated Virtio features: 0x140000000
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_GET_STATUS
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) read message VHOST_USER_SET_VRING_CALL
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) vring call idx:0 file:1321
VHOST_CONFIG: (/root/project/net/dpdk/build/examples/vhost.socket) vhost peer closed


usage(char* progname)
    --latencystats



testpmd -> main
    signal_handler
    rte_log_register
    rte_eal_init
    Selected IOVA mode 'VA'
    init_port
    register_eth_event_callback
    rte_eth_dev_callback_register(RTE_ETH_ALL, event, eth_event_callback,
        eth_event_callback

    rte_pdump_init
        rte_memzone_reserve(MZ_RTE_PDUMP_STATS, sizeof(*pdump_stats),
        rte_mp_action_register(PDUMP_MP, pdump_server)
    set_def_fwd_config
        set_default_fwd_lcores_config
        set_def_peer_eth_addrs
        set_default_fwd_ports_config
            rte_eth_dev_socket_id
    launch_args_parse
    init_config
        init_config_port_offloads
        mbuf_pool_create(mbuf_data_size[j],
        init_port_config


./dpdk-vhost_blk -m 1024
examples/vhost_blk/vhost_blk.c -> main
    rte_eal_init
    g_vhost_ctrlr = vhost_blk_ctrlr_construct(CTRLR_NAME) -> "vhost.socket" -> /root/project/net/dpdk/build/examples/vhost.socket
        rte_vhost_driver_register(dev_pathname, 0) -> vhost：添加 vhost-user 客户端模式，向 rte_vhost_driver_register() 添加新参数（标志）。当设置 RTE_VHOST_USER_CLIENT 标志时，DPDK vhost-user 充当客户端模式。这些标志还将允许未来的扩展而不会再次破坏 API。剩下的就是：分配一个 unix 套接字，绑定/监听服务器，连接客户端。此扩展仅适用于 vhost-user，因此当为 vhost-cuse 提供任何标志时，我们只需退出并报告错误 -> vhost：引入异步入队注册 API，执行大型内存复制通常占用大部分 CPU 周期，并成为 vhost-user 入队操作中的热点。为了将大型复制从 CPU 卸载到 DMA 设备，引入了异步 API，CPU 只需将复制作业提交给 DMA，而无需等待其复制完成。因此，数据传输期间无需 CPU 干预。我们可以节省宝贵的 CPU 周期并提高基于 vhost-user 的应用程序的整体吞吐量。此补丁引入了用于 vhost 异步数据入队操作的注册/取消注册 API。除了注册 API 实现外，还定义了异步入队数据路径所需的数据结构和异步回调函数的原型
            vsocket = malloc(sizeof(struct vhost_user_socket))
            1ULL << VIRTIO_NET_F_HOST_TSO4
            VIRTIO_NET_F_HOST_TSO6
            VIRTIO_NET_F_HOST_UFO
            if ((flags & RTE_VHOST_USER_CLIENT) -> client
                vhost_user_reconnect_init
                    rte_ctrl_thread_create(&reconn_tid, "vhost_reconn", NULL, vhost_user_client_reconnect, NULL)
                        vhost_user_connect_nonblock
                            connect(fd, un, sz)
                        vhost_user_add_connection
                            vid = vhost_new_device() -> 当建立新的 vhost-user 连接时(连接新的 virtio 设备)调用该函数来创建vhost通信设备
                                dev->vid = i
                                dev = rte_zmalloc(NULL, sizeof(struct virtio_net), 0)
                                dev->flags = VIRTIO_DEV_BUILTIN_VIRTIO_NET
                            vhost_attach_vdpa_device(vid, vsocket->vdpa_dev)
                            vsocket->notify_ops->new_connection(vid)
                            fdset_add(&vhost_user.fdset, fd, vhost_user_read_cb, NULL, conn)
                            TAILQ_INSERT_TAIL(&vsocket->conn_list, conn, next)
            or vsocket->is_server = true -> Server
            create_unix_socket(vsocket)
            vhost_user.vsockets[vhost_user.vsocket_cnt++] = vsocket
        rte_vhost_driver_set_features(dev_pathname, VHOST_BLK_FEATURES)
        ctrlr = rte_zmalloc(NULL, sizeof(*ctrlr), RTE_CACHE_LINE_SIZE)
        vhost_dev_install_rte_compat_hooks(dev_pathname)
        ctrlr->bdev = vhost_blk_bdev_construct("malloc0", "vhost_blk_malloc0", 4096, 32768, 0) -> 128MB -> in qemu, lsblk show vda size = 128MB
            bdev = rte_zmalloc(NULL, sizeof(*bdev), RTE_CACHE_LINE_SIZE)
            bdev->data = rte_zmalloc(NULL, blk_cnt * blk_size, 0) -> use memory as disk storage space
            VHOST_USER_PROTOCOL_F_CONFIG
            VHOST_USER_PROTOCOL_F_INFLIGHT_SHMFD
        ctrlr->bdev = vhost_blk_bdev_construct("malloc0", "vhost_blk_malloc0", 4096, 32768, 0) -> 128MB -> in qemu, lsblk show vda size = 128MB
            bdev = rte_zmalloc(NULL, sizeof(*bdev), RTE_CACHE_LINE_SIZE)
            bdev->blocklen = blk_size
            bdev->blockcnt = blk_cnt
            bdev->data = rte_zmalloc(NULL, blk_cnt * blk_size, 0) -> use huge share memory as disk storage space
        rte_vhost_driver_callback_register(dev_pathname, &vhost_blk_device_ops)
            vsocket->notify_ops = ops
    rte_vhost_driver_start(dev_pathname)
        fdset_pipe_init(&vhost_user.fdset)
        rte_ctrl_thread_create(&fdset_tid, "vhost-events", NULL, fdset_event_dispatch, &vhost_user.fdset)
        vhost_user_start_server(vsocket) -> bind -> listen
            fdset_add(&vhost_user.fdset, fd, vhost_user_server_new_connection, NULL, vsocket)
                fdset_add_fd(pfdset, i, fd, rcb, wcb, dat)
                    pfdentry->rcb = rcb
        or vhost_user_start_client(vsocket)
            vhost_user_connect_nonblock
            vhost_user_add_connection(fd, vsocket)
            TAILQ_INSERT_TAIL(&reconn_list.head, reconn, next)



struct rte_vhost_device_ops vhost_blk_device_ops = {
    .new_device =  new_device,
        ret = rte_vhost_get_ifname(vid, path, PATH_MAX)
        ctrlr = vhost_blk_ctrlr_find(path)
        rte_vhost_get_negotiated_features
        rte_vhost_get_negotiated_protocol_features
        for (i = 0; i < NUM_OF_BLK_QUEUES; i++)
            rte_vhost_get_vhost_vring
            rte_vhost_get_vring_base
            rte_vhost_get_vhost_ring_inflight
            rte_vhost_get_vring_base_from_inflight
            rte_vhost_enable_guest_notification
        rte_vhost_get_mem_table
        alloc_task_pool
        rte_ctrl_thread_create(&tid, "vhostblk-ctrlr", NULL, &ctrlr_worker, ctrlr)
        rte_ctrl_thread_create(&tid, "vhostblk-ctrlr", NULL, &ctrlr_worker, ctrlr) -> io thread, loop deal with io -> ctrlr_worker(void *arg)
            for (i = 0; i < NUM_OF_BLK_QUEUES; i++)
                submit_inflight_vq(&ctrlr->queues[i])
                    blk_task_init(task)
                    process_blk_task(task)
                        vhost_bdev_process_blk_commands
                            switch (task->req->type)
                            case VIRTIO_BLK_T_IN: -> read
                                vhost_bdev_blk_readwrite
                                    memcpy(bdev->data + offset, task->iovs[i].iov_base, task->iovs[i].iov_len)
                                    or memcpy(task->iovs[i].iov_base, bdev->data + offset, task->iovs[i].iov_len)
                            case VIRTIO_BLK_T_OUT: -> write
                                vhost_bdev_blk_readwrite
                        if (task->vq->packed_ring)
                            enqueue_task_packed(task)
                        else
                            enqueue_task(task)
                                rte_vhost_clr_inflight_desc_split(task->ctrlr->vid, vq->id, used->idx, task->req_idx)
                                rte_vhost_vring_call(task->ctrlr->vid, vq->id) -> send interrupt back to guest vm, completion is ready to be process
                                    vhost_vring_call_packed(dev, vq)
                                        if (vhost_need_event(off, new, old)) -> kick = true
                                            return (uint16_t)(new_idx - event_idx - 1) < (uint16_t)(new_idx - old)
                                        if (kick)
                                            eventfd_write(vq->callfd, (eventfd_t)1)
                                            dev->notify_ops->guest_notified(dev->vid)
                                    or vhost_vring_call_split
                                        if (dev->features & (1ULL << VIRTIO_RING_F_EVENT_IDX)) -> Don't kick guest if we don't reach index specified by guest
                                        else -> Kick the guest if necessary -> vring_avail flags是用来告诉后端是否要发中断, 当avail flags为0时后端就后发中断到guest里面
                                            if (!(vq->avail->flags & VRING_AVAIL_F_NO_INTERRUPT)
                                                eventfd_write(vq->callfd, (eventfd_t)1)
            while (worker_thread_status != WORKER_STATE_STOP)
                for (i = 0; i < NUM_OF_BLK_QUEUES; i++)
                    process_vq(&ctrlr->queues[i])
                        if (vq->packed_ring)
                            blk_task_init
                            vhost_blk_vq_get_desc_chain_buffer_id
                            rte_vhost_set_inflight_desc_packed
                            process_blk_task
                        else split
                            rte_vhost_set_inflight_desc_split
                            process_blk_task
        ctrlr->started = 1
    .destroy_device = destroy_device,
    .new_connection = new_connection,
        vhost_session_install_rte_compat_hooks
};



vhost_kernel_get_features
    vhost_kernel_ioctl(data->vhostfds[0], VHOST_GET_FEATURES, features)



struct virtio_user_backend_ops virtio_ops_vdpa = {
    .setup = vhost_vdpa_setup,
    .destroy = vhost_vdpa_destroy,
    .get_backend_features = vhost_vdpa_get_backend_features,
    .set_owner = vhost_vdpa_set_owner,
    .get_features = vhost_vdpa_get_features,
        vhost_vdpa_ioctl(data->vhostfd, VHOST_GET_FEATURES, features) -> kernel -> .unlocked_ioctl	= vhost_vdpa_unlocked_ioctl
    .set_features = vhost_vdpa_set_features,
        vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_FEATURES, &features)
    .set_memory_table = vhost_vdpa_set_memory_table,
        vhost_vdpa_iotlb_batch_begin
            msg.type = VHOST_IOTLB_MSG_V2;
            msg.iotlb.type = VHOST_IOTLB_BATCH_BEGIN
            write(data->vhostfd, &msg, sizeof(msg)
        vhost_vdpa_dma_unmap
        rte_memseg_contig_walk_thread_unsafe(vhost_vdpa_map_contig, dev)
            vhost_vdpa_dma_map(dev, ms->addr, ms->iova, len)
        rte_memseg_walk_thread_unsafe(vhost_vdpa_map, dev)
        vhost_vdpa_iotlb_batch_end
    .set_vring_num = vhost_vdpa_set_vring_num,
    .set_vring_base = vhost_vdpa_set_vring_base, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_BASE, state)
    .get_vring_base = vhost_vdpa_get_vring_base,
    .set_vring_call = vhost_vdpa_set_vring_call, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_CALL, file)
    .set_vring_kick = vhost_vdpa_set_vring_kick, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_KICK, file)
    .set_vring_addr = vhost_vdpa_set_vring_addr, -> vhost_vdpa_ioctl(data->vhostfd, VHOST_SET_VRING_ADDR, addr)
    .get_status = vhost_vdpa_get_status,
    .set_status = vhost_vdpa_set_status,
    .get_config = vhost_vdpa_get_config,
    .set_config = vhost_vdpa_set_config,
    .enable_qp = vhost_vdpa_enable_queue_pair,
    .dma_map = vhost_vdpa_dma_map_batch,
        vhost_vdpa_iotlb_batch_begin
        vhost_vdpa_dma_map
            msg.type = VHOST_IOTLB_MSG_V2
            msg.iotlb.type = VHOST_IOTLB_UPDATE -> to kernel
            msg.iotlb.iova = iova;
            msg.iotlb.uaddr = (uint64_t)(uintptr_t)addr;
            msg.iotlb.size = len;
            msg.iotlb.perm = VHOST_ACCESS_RW;
            write(data->vhostfd, &msg, sizeof(msg)
        vhost_vdpa_iotlb_batch_end
    .dma_unmap = vhost_vdpa_dma_unmap_batch,
    .update_link_state = vhost_vdpa_update_link_state,
    .get_intr_fd = vhost_vdpa_get_intr_fd,
};



struct virtio_user_backend_ops virtio_ops_user = {
    .setup = vhost_user_setup,
    .destroy = vhost_user_destroy,
    .get_backend_features = vhost_user_get_backend_features,
    .set_owner = vhost_user_set_owner,
    .get_features = vhost_user_get_features,
    .set_features = vhost_user_set_features,
    .set_memory_table = vhost_user_set_memory_table,
        .request = VHOST_USER_SET_MEM_TABLE
        rte_memseg_walk_thread_unsafe(update_memory_region, &wa) -> 此函数不执行任何锁定，并且只能在与内存相关的回调函数中安全调用
            mr->guest_phys_addr = start_addr;
            mr->userspace_addr = start_addr;
            mr->memory_size = ms->len;
            mr->mmap_offset = offset;
        vhost_user_write(data->vhostfd, &msg, fds, fd_num)
    .set_vring_num = vhost_user_set_vring_num,
    .set_vring_base = vhost_user_set_vring_base, -> virtio device sends us the available ring last used index
        vq->last_avail_idx = val & 0x7fff
    .get_vring_base = vhost_user_get_vring_base, -> when virtio is stopped, qemu will send us the GET_VRING_BASE message
        vhost_destroy_device_notify(dev)
            vdpa_dev->ops->dev_close(dev->vid)
        vhost_user_iotlb_flush_all(vq) -> vhost：在 vring 停止时清理 IOTLB 缓存，当 VM 中的 virtio 驱动程序发生更改时，会留下旧的 IOVA 缓存条目。如果所有这些旧条目的 iova 地址都小于新的 iova 条目，则 vhost 代码将需要迭代所有缓存以查找新条目。如果新转换只需要一个新的 iova 条目，这种情况将永远持续下去。在 virtio-net 到 testpmd 的 vfio-pci 驱动程序转换中已经观察到这种情况，如果大页面地址高于网络缓冲区，则性能会从超过 10Mpps 降低到不到 0.07Mpps。由于所有新缓冲区都包含在这个新的巨型页面中，因此 vhost 最坏情况下需要为每个转换扫描 IOTLB_CACHE_SIZE - 1
            vhost_user_iotlb_cache_remove_all
                for
                    TAILQ_REMOVE(&vq->iotlb_list, node, next)
                    vhost_user_iotlb_pool_put(vq, node)
                        SLIST_INSERT_HEAD(&vq->iotlb_free_list, node, next_free)
            vhost_user_iotlb_pending_remove_all
        vring_invalidate(dev, vq)
            vq->access_ok = false;
            vq->desc = NULL;
            vq->avail = NULL;
            vq->used = NULL;
            vq->log_guest_addr = 0;
    .set_vring_call = vhost_user_set_vring_call,
        file.index = ctx->msg.payload.u64 & VHOST_USER_VRING_IDX_MASK
        vq = dev->virtqueue[file.index]
        vhost_user_notify_queue_state(dev, vq, 0)
        if (vq->ready) -> vhost：通知 virtq 文件描述符更新，当队列就绪时，virtq call 或 kick 文件描述符在设备配置中发生更改时，应通知应用程序和 vDPA 驱动程序以与新的文件描述符对齐。在文件描述符更新之前通知状态为禁用，并在更新后将其恢复为启用
            vhost_user_notify_queue_state(dev, vq, 0)
                vdpa_dev->ops->set_vring_state(dev->vid, vq->index, enable)
                dev->notify_ops->vring_state_changed(dev->vid, vq->index, enable)
        vq->callfd = file.fd -> is qemu eventfd
    .set_vring_kick = vhost_user_set_vring_kick,
        vq = dev->virtqueue[file.index]
        translate_ring_addresses(&dev, &vq)
        vq->kickfd = file.fd
        vhost_check_queue_inflights_packed
            resubmit = rte_zmalloc_socket("resubmit", sizeof(struct rte_vhost_resubmit_info), 0, vq->numa_node)
    .set_vring_addr = vhost_user_set_vring_addr, -> virtio 设备向我们发送 desc、used 和 avail 环地址。然后此函数将它们转换为我们的地址空间
        struct vhost_vring_addr *addr = &ctx->msg.payload.addr
        vq = dev->virtqueue[ctx->msg.payload.addr.index]
        memcpy(&vq->ring_addrs, addr, sizeof(*addr))
        vring_invalidate
        translate_ring_addresses(&dev, &vq)
            if (vq->ring_addrs.flags & (1 << VHOST_VRING_F_LOG))
                vq->log_guest_addr = log_addr_to_gpa(dev, vq)
            if (vq_is_packed(dev))
                vq->desc_packed = (struct vring_packed_desc *)(uintptr_t)ring_addr_to_vva(dev, vq, vq->ring_addrs.desc_user_addr, &len) -> 将环地址转换为 Vhost 虚拟地址。如果启用了 IOMMU，则环地址为客户机 IO 虚拟地址，否则为 QEMU 虚拟地址
                    if (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM))
                        vhost_iova_to_vva(dev, vq, ra, size, VHOST_ACCESS_RW) -> vhost：确保在转换 QVA 时映射所有范围，此补丁确保在将地址从主地址（例如 QEMU 主机地址）转换为进程 VA 时映射所有地址范围
                            rte_vhost_va_from_guest_pa(dev->mem, iova, len) -> Convert guest physical address to host virtual address
                            or __vhost_iova_to_vva(dev, vq, iova, len, perm)
                                vva = vhost_user_iotlb_cache_find(vq, iova, &tmp_size, perm)
                                    TAILQ_FOREACH(node, &vq->iotlb_list, next)
                                if (!vhost_user_iotlb_pending_miss(vq, iova, perm))
                                    vhost_user_iotlb_pending_insert
                                        node = vhost_user_iotlb_pool_get(vq)
                                            node = SLIST_FIRST(&vq->iotlb_free_list)
                                            SLIST_REMOVE_HEAD(&vq->iotlb_free_list, next_free)
                                        TAILQ_INSERT_TAIL(&vq->iotlb_pending_list, node, next)
                                    vhost_user_iotlb_miss
                                        .request.slave = VHOST_USER_SLAVE_IOTLB_MSG
                                        send_vhost_message(dev, dev->slave_req_fd, &ctx)
                                            send_fd_message(dev->ifname, sockfd, (char *)&ctx->msg, VHOST_USER_HDR_SIZE + ctx->msg.size, ctx->fds, ctx->fd_num)
                    qva_to_vva(dev, ra, size) ->  Converts QEMU virtual address to Vhost virtual address
                        for (i = 0; i < dev->mem->nregions; i++)
                numa_realloc(&dev, &vq) -> vhost：保留对 virtqueue 索引的引用，在 dev->virtqueue[] 数组中拥有对 vq 索引的反向引用，可以统一内部 API，只需传递 dev 和 vq。它还允许在日志消息中显示 vq 索引。删除不需要的 virtqueue 索引检查（例如在所有可用 virtqueue 上循环调用的静态帮助程序）。尽快移动 virtqueue 索引有效性检查
    .get_status = vhost_user_get_status,
    .set_status = vhost_user_set_status,
    .enable_qp = vhost_user_enable_queue_pair,
    .update_link_state = vhost_user_update_link_state,
    .server_disconnect = vhost_user_server_disconnect,
    .server_reconnect = vhost_user_server_reconnect,
    .get_intr_fd = vhost_user_get_intr_fd,
};



dpdk/drivers/net/virtio/virtio_user_ethdev.c
RTE_PMD_REGISTER_VDEV(net_virtio_user, virtio_user_driver);
static struct rte_vdev_driver virtio_user_driver = {
    .probe = virtio_user_pmd_probe,
    .remove = virtio_user_pmd_remove,
    .dma_map = virtio_user_pmd_dma_map,
    .dma_unmap = virtio_user_pmd_dma_unmap,
};
virtio_user_pmd_probe
    virtio_user_dev_setup
        switch (dev->backend_type)
        case VIRTIO_USER_BACKEND_VHOST_USER:
            dev->ops = &virtio_ops_user
        case VIRTIO_USER_BACKEND_VHOST_KERNEL
            dev->ops = &virtio_ops_kernel
        case VIRTIO_USER_BACKEND_VHOST_VDPA:
            dev->ops = &virtio_ops_vdpa
        dev->ops->setup(dev)
        virtio_user_dev_init_notify
        virtio_user_fill_intr_handle
    virtio_user_dev_init
        dev->ops->get_features(dev, &dev->device_features)


struct virtio_user_backend_ops virtio_ops_kernel = {
    .setup = vhost_kernel_setup,
    .destroy = vhost_kernel_destroy,
    .get_backend_features = vhost_kernel_get_backend_features,
    .set_owner = vhost_kernel_set_owner,
    .get_features = vhost_kernel_get_features,
    .set_features = vhost_kernel_set_features,
    .set_memory_table = vhost_kernel_set_memory_table,
    .set_vring_num = vhost_kernel_set_vring_num,
    .set_vring_base = vhost_kernel_set_vring_base,
    .get_vring_base = vhost_kernel_get_vring_base,
    .set_vring_call = vhost_kernel_set_vring_call,
    .set_vring_kick = vhost_kernel_set_vring_kick,
    .set_vring_addr = vhost_kernel_set_vring_addr,
    .get_status = vhost_kernel_get_status,
    .set_status = vhost_kernel_set_status,
    .enable_qp = vhost_kernel_enable_queue_pair,
    .update_link_state = vhost_kernel_update_link_state,
    .get_intr_fd = vhost_kernel_get_intr_fd,
};


vhost_kernel_set_memory_table
    vhost_kernel_ioctl(data->vhostfds[i], VHOST_SET_MEM_TABLE, vm)


virtio_user_set_status
    virtio_user_start_device
        dev->ops->set_memory_table(dev)


#define VHOST_MESSAGE_HANDLERS \
VHOST_MESSAGE_HANDLER(VHOST_USER_NONE, NULL, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_GET_FEATURES, vhost_user_get_features, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_FEATURES, vhost_user_set_features, false) \
    if (vdpa_dev)
        vdpa_dev->ops->set_features(dev->vid)
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_OWNER, vhost_user_set_owner, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_RESET_OWNER, vhost_user_reset_owner, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_MEM_TABLE, vhost_user_set_mem_table, true) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_LOG_BASE, vhost_user_set_log_base, true) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_LOG_FD, vhost_user_set_log_fd, true) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_VRING_NUM, vhost_user_set_vring_num, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_VRING_ADDR, vhost_user_set_vring_addr, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_VRING_BASE, vhost_user_set_vring_base, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_GET_VRING_BASE, vhost_user_get_vring_base, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_VRING_KICK, vhost_user_set_vring_kick, true) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_VRING_CALL, vhost_user_set_vring_call, true) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_VRING_ERR, vhost_user_set_vring_err, true) \
VHOST_MESSAGE_HANDLER(VHOST_USER_GET_PROTOCOL_FEATURES, vhost_user_get_protocol_features, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_PROTOCOL_FEATURES, vhost_user_set_protocol_features, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_GET_QUEUE_NUM, vhost_user_get_queue_num, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_VRING_ENABLE, vhost_user_set_vring_enable, false) \
    dev->virtqueue[index]->enabled = enable
VHOST_MESSAGE_HANDLER(VHOST_USER_SEND_RARP, vhost_user_send_rarp, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_NET_SET_MTU, vhost_user_net_set_mtu, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_SLAVE_REQ_FD, vhost_user_set_req_fd, true) \
    dev->slave_req_fd = fd
VHOST_MESSAGE_HANDLER(VHOST_USER_IOTLB_MSG, vhost_user_iotlb_msg, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_GET_CONFIG, vhost_user_get_config, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_CONFIG, vhost_user_set_config, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_POSTCOPY_ADVISE, vhost_user_set_postcopy_advise, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_POSTCOPY_LISTEN, vhost_user_set_postcopy_listen, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_POSTCOPY_END, vhost_user_postcopy_end, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_GET_INFLIGHT_FD, vhost_user_get_inflight_fd, false) \

VHOST_MESSAGE_HANDLER(VHOST_USER_SET_INFLIGHT_FD, vhost_user_set_inflight_fd, true) \
    pervq_inflight_size = get_pervq_shm_size_packed(queue_size)
    or pervq_inflight_size = get_pervq_shm_size_split(queue_size)
    addr = mmap(0, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, mmap_offset)
    addr = (void *)((char *)addr + pervq_inflight_size)
VHOST_MESSAGE_HANDLER(VHOST_USER_SET_STATUS, vhost_user_set_status, false) \
VHOST_MESSAGE_HANDLER(VHOST_USER_GET_STATUS, vhost_user_get_status, false)

static vhost_message_handler_t vhost_message_handlers[] = {
    VHOST_MESSAGE_HANDLERS
};



msg_handler = &vhost_message_handlers[request]



vhost_user_set_mem_table(struct virtio_net **pdev, struct vhu_msg_context *ctx, int main_fd)
    VHOST_MEMORY_MAX_NREGIONS -> max regions = 8
    vhost_memory_changed(memory, dev->mem) -> compare new and old mem addr, size
        if (new_r->guest_phys_addr != old_r->guest_phys_addr)
    dev->notify_ops->vring_state_changed(dev->vid, i, 0) -> stop dma
        vring_conf_update(vid, eth_dev, vring)
        update_queuing_status(eth_dev, false)
        rte_eth_dev_callback_process(eth_dev, RTE_ETH_EVENT_QUEUE_STATE, NULL)
    if (dev->features & (1ULL << VIRTIO_F_IOMMU_PLATFORM))
        vhost_user_iotlb_flush_all(dev->virtqueue[i])
    dev->guest_pages = rte_zmalloc_socket(NULL, dev->max_guest_pages *sizeof(struct guest_page), RTE_CACHE_LINE_SIZE, numa_node) -> malloc 8 pages from hugepage
    dev->mem = rte_zmalloc_socket("vhost-mem-table", sizeof(struct rte_vhost_memory) + sizeof(struct rte_vhost_mem_region) * memory->nregions, 0, numa_node)
    for (i = 0; i < memory->nregions; i++) -> map muti mem region
        reg->guest_phys_addr = memory->regions[i].guest_phys_addr
        reg->guest_user_addr = memory->regions[i].userspace_addr;
        reg->size            = memory->regions[i].memory_size;
        reg->fd              = ctx->fds[i];
        mmap_offset = memory->regions[i].mmap_offset
        vhost_user_mmap_region(dev, reg, mmap_offset)
            mmap_size = region->size + mmap_offset
            mmap_size = RTE_ALIGN_CEIL(mmap_size, alignment)
            mmap_addr = mmap(NULL, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED | populate, region->fd, 0) <- vhost_user_set_vring_call -> file.fd -> userspace vaddr, used by vm(hva, maped by mmu)
            region->host_user_addr = (uint64_t)(uintptr_t)mmap_addr + mmap_offset
            region->mmap_addr = mmap_addr
            region->mmap_size = mmap_size
            print mem info:
            guest memory region size: 0x40000000
            guest physical addr: 0x0
            guest virtual  addr: 0x7fa58be00000
            host  virtual  addr: 0x7fffa8000000
            mmap addr : 0x7fffa8000000
            mmap size : 0x40000000
            mmap align: 0x200000
            mmap off  : 0x0
    if (dev->async_copy && rte_vfio_is_enabled("vfio"))
        async_dma_map(dev, true)
            rte_vfio_container_dma_map(RTE_VFIO_DEFAULT_CONTAINER_FD, page->host_user_addr, page->host_iova, page->size)
                vfio_cfg = get_vfio_cfg_by_container_fd(container_fd)
                    container_dma_map(vfio_cfg, vaddr, iova, len)
                        vfio_dma_mem_map
                            ioctl(vfio_container_fd, VFIO_IOMMU_MAP_DMA, &dma_map) -> iova used by device(iommu same pa with mmap_addr)
    vhost_user_postcopy_register(dev, main_fd, ctx)
        vhost_user_postcopy_region_register
    for (i = 0; i < dev->nr_vring; i++)
        vring_invalidate(dev, vq) -> set null
        translate_ring_addresses(&dev, &vq)
    dump_guest_pages(dev)
    if (async_notify)
        dev->notify_ops->vring_state_changed(dev->vid, i, 1)



xilinx, librte_pmd_sfc_vdpa, sfc_efx, ef100 设备可以配置为网络设备或 vDPA 模式。添加“class=vdpa”参数有助于指定此设备将在 vDPA 模式下使用。如果未指定此参数，设备将由 net/sfc 驱动程序探测并用作网络设备, 该 PMD 使用 libefx（common/sfc_efx）代码来访问设备固件 -> AMD 现已收购 Solarflare，可提供 XtremeScale 系列网络接口控制器 -> commit: https://github.com/ssbandjl/dpdk/commit/5e7596ba7cb3b1f07a605edf6c816d355e84dc26
SFC: Solarflare Communications
RTE_INIT(sfc_efx_register_logtype)

drivers/vdpa/sfc/sfc_vdpa.c
RTE_PMD_REGISTER_PCI(net_sfc_vdpa, rte_sfc_vdpa)
sfc_vdpa_pci_probe
    logtype_main = sfc_vdpa_register_logtype(&pci_dev->addr, SFC_VDPA_LOGTYPE_MAIN_STR, RTE_LOG_NOTICE)
    sfc_vdpa_set_log_prefix(sva)
        snprintf(sva->log_prefix, sizeof(sva->log_prefix),"PMD: sfc_vdpa " PCI_PRI_FMT " : ",pci_dev->addr.domain, pci_dev->addr.bus, pci_dev->addr.devid, pci_dev->addr.function)
    sfc_vdpa_kvargs_parse(sva)
        sva->kvargs = rte_kvargs_parse(devargs->args, params)
    sfc_vdpa_adapter_lock_init(sva)
    sfc_vdpa_vfio_setup(sva)
        sva->vfio_container_fd = rte_vfio_container_create()
            for (i = 1; i < VFIO_MAX_CONTAINERS; i++) -> 64
            vfio_cfgs[i].vfio_container_fd = rte_vfio_get_container_fd()
                vfio_container_fd = open(VFIO_CONTAINER_PATH, O_RDWR) -> /dev/vfio/vfio -> primary process
                return vfio_container_fd
                else secondary process
                p->req = SOCKET_REQ_CONTAINER
                rte_mp_request_sync(&mp_req, &mp_reply, &ts) -> This function sends a request message to the peer process, and will block until receiving reply message from the peer process
                    check_input(req)
                    for secondary process, send request to the primary process only
                        mp_request_sync(eal_mp_socket_path(), req, reply, &end)
                            send_msg(dst, req, MP_REQ) -> socket from -> open_socket_fd
                                snd = sendmsg(mp_fd, &msgh, 0)
                            memcpy(&tmp[reply->nb_received], &msg, sizeof(msg))
                    for primary process, broadcast request, and collect reply 1 by 1
                        mp_dir = opendir(mp_dir_path)
                        mp_request_sync(path, req, reply, &end)
        rte_vfio_get_group_num(rte_pci_get_sysfs_path(), dev_name, &sva->iommu_group_num) -> /sys/bus/pci/devices
            snprintf(linkname, sizeof(linkname), "%s/%s/iommu_group", sysfs_base, dev_addr) -> example: /sys/bus/pci/devices/0000:06:0d.0/iommu_group
            readlink(linkname, filename, sizeof(filename))
            *iommu_group_num = strtol(group_tok, &end, 10)
        sva->vfio_group_fd = rte_vfio_container_group_bind(sva->vfio_container_fd, sva->iommu_group_num)
            vfio_cfg = get_vfio_cfg_by_container_fd(container_fd)
                return &vfio_cfgs[i]
            vfio_get_group_fd(vfio_cfg, iommu_group_num)
                vfio_group_fd = vfio_open_group_fd(iommu_group_num) -> /dev/vfio/%u
                    if (internal_conf->process_type == RTE_PROC_PRIMARY)
                        vfio_group_fd = open(filename, O_RDWR)
                    rte_mp_request_sync(&mp_req, &mp_reply, &ts) -> vfio：使用通用多进程通道，以前，vfio 使用自己的私有通道供次进程从主进程获取容器 fd 和组 fd。此补丁更改为使用通用 mp 通道 -> 向对等进程发送请求并等待回复。此函数向对等进程发送请求消息，并将阻塞直到收到对等进程的回复消息
                cur_grp->group_num = iommu_group_num
        rte_pci_map_device(dev) -> 映射PCI设备BAR资源
        sva->vfio_dev_fd = rte_intr_dev_fd_get(dev->intr_handle)
            return intr_handle->dev_fd
    sfc_vdpa_hw_init
        sfc_efx_family(sva->pdev, &mem_ebr, &sva->family)
            static const efx_pci_ops_t ops = {
                .epo_config_readd = sfc_efx_pci_config_readd, -> rte_pci_read_config
                .epo_find_mem_bar = sfc_efx_find_mem_bar,
            };
            efx_family_probe_bar(pci_dev->id.vendor_id, pci_dev->id.device_id, &espcp, &ops, family, mem_ebrp)
                case EFX_PCI_DEVID_RIVERHEAD_VF
                    rhead_pci_nic_membar_lookup
                        efx_pci_find_next_xilinx_cap_table -> epo_config_readd
                        rhead_xilinx_cap_tbl_find_ef100_locator
                            rhead_nic_xilinx_cap_tbl_read_ef100_locator
                        epo_find_mem_bar -> sfc_efx_find_mem_bar
                efx_family(venid, devid, efp, &membar)
        sfc_vdpa_mem_bar_init(sva, &mem_ebr)
        efx_nic_create(sva->family, (efsys_identifier_t *)sva, &sva->mem_bar, mem_ebr.ebr_offset, &sva->nic_lock, &enp)
            case EFX_FAMILY_MEDFORD
                enp->en_enop = &__efx_nic_medford_ops
            case EFX_FAMILY_MEDFORD2
                enp->en_enop = &__efx_nic_medford2_ops
            case EFX_FAMILY_RIVERHEAD
                enp->en_enop = &__efx_nic_riverhead_ops
        sfc_vdpa_mcdi_init(sva)
            sfc_efx_mcdi_init(&sva->mcdi, logtype,sva->log_prefix, sva->nic, &sfc_vdpa_mcdi_ops, sva)
        sfc_vdpa_nic_probe(sva)
            efx_phy_probe
                epop = &__efx_phy_ef10_ops
        efx_nic_reset(enp)
        efx_virtio_init(enp)
            evop = &__efx_virtio_rhead_ops
        sfc_vdpa_estimate_resource_limits(sva)
        efx_filter_init(enp)
    sva->ops_data = sfc_vdpa_device_init(sva, SFC_VDPA_AS_VF)
        ops_data = rte_zmalloc("vdpa", sizeof(struct sfc_vdpa_ops_data), 0)
        pci_dev = sfc_vdpa_adapter_by_dev_handle(dev_handle)->pdev
        ops_data->vdpa_dev = rte_vdpa_register_device(&pci_dev->device, &sfc_vdpa_ops)
        sfc_vdpa_get_device_features(ops_data)
    TAILQ_INSERT_TAIL(&sfc_vdpa_adapter_list, sva, next)


static struct rte_vdpa_dev_ops sfc_vdpa_ops = {
    .get_queue_num = sfc_vdpa_get_queue_num,
    .get_features = sfc_vdpa_get_features,
    .get_protocol_features = sfc_vdpa_get_protocol_features,
    .dev_conf = sfc_vdpa_dev_config, -> vdpa/sfc：支持设备配置和关闭，实现 vDPA ops dev_conf 和 dev_close 进行 DMA 映射、中断和 virtqueue 配置
        sfc_vdpa_configure(ops_data)
            sfc_vdpa_dma_map(ops_data, true)
                vfio_container_fd = sfc_vdpa_adapter_by_dev_handle(dev)->vfio_container_fd
                rte_vhost_get_mem_table(ops_data->vid, &vhost_mem)
                rte_vfio_container_dma_map(vfio_container_fd, mem_reg->host_user_addr, mem_reg->guest_phys_addr, mem_reg->size)
            efx_virtio_qcreate(nic, &vq)
                EFSYS_KMEM_ALLOC(enp->en_esip, sizeof (efx_virtio_vq_t), evvp) -> rte_zmalloc
        sfc_vdpa_start(ops_data)
            sfc_vdpa_enable_vfio_intr(ops_data) -> enable interrupts
                struct vfio_irq_set *irq_set
                irq_fd_ptr = (int *)&irq_set->data
                irq_fd_ptr[RTE_INTR_VEC_ZERO_OFFSET] = rte_intr_fd_get(pci_dev->intr_handle)
                rte_vhost_get_vhost_vring(ops_data->vid, i, &vring)
                irq_fd_ptr[RTE_INTR_VEC_RXTX_OFFSET + i] = vring.callfd -> callfd from index 1
                ioctl(vfio_dev_fd, VFIO_DEVICE_SET_IRQS, irq_set)
            rte_vhost_get_negotiated_features
            sfc_vdpa_virtq_start(ops_data, i)
                sfc_vdpa_get_vring_info(ops_data, vq_num, &vring)
                    rte_vhost_get_vhost_vring(ops_data->vid, vq_num, &vq)
                    gpa = hva_to_gpa(ops_data->vid, (uint64_t)(uintptr_t)vq.desc)
                    gpa = hva_to_gpa(ops_data->vid, (uint64_t)(uintptr_t)vq.avail)
                    gpa = hva_to_gpa(ops_data->vid, (uint64_t)(uintptr_t)vq.used)
                    rte_vhost_get_vring_base(ops_data->vid, vq_num, &vring->last_avail_idx, &vring->last_used_idx)
                efx_virtio_qstart(vq, &vq_cfg, &vq_dyncfg)
                    evop->evo_virtio_qstart(evvp, evvcp, evvdp)
                efx_virtio_get_doorbell_offset(vq,	&doorbell)
                ops_data->vq_cxt[vq_num].doorbell = (void *)(uintptr_t)doorbell
            sfc_vdpa_filter_config
                sfc_vdpa_set_mac_filter
        sfc_vdpa_setup_notify_ctrl(ops_data)
            pthread_create(&ops_data->notify_tid, NULL, sfc_vdpa_notify_ctrl, ops_data)
                rte_vhost_host_notifier_ctrl(vid, RTE_VHOST_QUEUE_ALL, true)
            ops_data->is_notify_thread_started = true
    .dev_close = sfc_vdpa_dev_close,
    .set_vring_state = sfc_vdpa_set_vring_state,
    .set_features = sfc_vdpa_set_features,
    .get_vfio_device_fd = sfc_vdpa_get_vfio_device_fd,
    .get_notify_area = sfc_vdpa_get_notify_area,
        *offset = reg.offset + (uint64_t)ops_data->vq_cxt[qid].doorbell
        pci_dev = sfc_vdpa_adapter_by_dev_handle(dev)->pdev
        doorbell = (uint8_t *)pci_dev->mem_resource[reg.index].addr + *offset
        rte_write16(qid, doorbell) -> VM 中的 virtio-net 驱动程序在 vDPA 有机会设置队列和通知区域之前发送队列通知，因此 HW 错过了这些门铃通知。由于发送重复门铃是安全的，因此从 vDPA 驱动程序发送另一个门铃作为解决此时间问题的解决方法
};


AMD Xilinx VDPA 在DPDK中的中断处理流程如下:
1. 设备通过vfio将irq_fd_ptr暴露到用户态: irq_fd_ptr[RTE_INTR_VEC_ZERO_OFFSET] = rte_intr_fd_get(pci_dev->intr_handle)
2. 将设备IRQ与callfd进行绑定: irq_fd_ptr[RTE_INTR_VEC_RXTX_OFFSET + i] = vring.callfd -> bind dev irq to callfd (vhost_target -> qemu)
3. DPDK通过ioctl将irq_set给了VFIO内核驱动: ioctl(vfio_dev_fd, VFIO_DEVICE_SET_IRQS, irq_set)
4. 硬件触发中断后, 通过VFIO中的eventfd_signal(trigger, 1)触发, 通过callfd通知到qemu, 流程如下:
hw_irq -> pci_dev->intr_handle -> irq_fd_ptr -> vring.callfd -> qemu handle this callfd and deal with this irq event (event_notifier_test_and_clear)
5. qemu这边通过handle irq做中断处理


static const efx_virtio_ops_t	__efx_virtio_rhead_ops = {
    rhead_virtio_qstart,			/* evo_virtio_qstart */
        req.emr_cmd = MC_CMD_VIRTIO_INIT_QUEUE
        efx_mcdi_execute(enp, &req)
            emtp->emt_execute(emtp->emt_context, emrp) -> sfc_efx_mcdi_execute
                efx_mcdi_request_start(mcdi->nic, emrp, B_FALSE)
                    efx_mcdi_send_request(enp, &hdr[0], hdr_len, emrp->emr_in_buf, emrp->emr_in_length)
                        emcop->emco_send_request(enp, hdrp, hdr_len, sdup, sdu_len)
                sfc_efx_mcdi_poll(mcdi, B_FALSE)
                    do
                        sfc_efx_mcdi_proxy_event_available(mcdi)
                        or efx_mcdi_request_poll
                            efx_mcdi_read_response_header
                            efx_mcdi_finish_response
                efx_mcdi_get_proxy_handle(mcdi->nic, emrp, &proxy_handle)
    rhead_virtio_qstop,			/* evo_virtio_qstop */
    rhead_virtio_get_doorbell_offset,	/* evo_get_doorbell_offset */
    rhead_virtio_get_features,		/* evo_get_features */
    rhead_virtio_verify_features,		/* evo_verify_features */
};

static const efx_phy_ops_t	__efx_phy_ef10_ops = {
    ef10_phy_power,			/* epo_power */
    NULL,				/* epo_reset */
    ef10_phy_reconfigure,		/* epo_reconfigure */
    ef10_phy_verify,		/* epo_verify */
    ef10_phy_oui_get,		/* epo_oui_get */
    ef10_phy_link_state_get,	/* epo_link_state_get */
#if EFSYS_OPT_PHY_STATS
    ef10_phy_stats_update,		/* epo_stats_update */
#endif	/* EFSYS_OPT_PHY_STATS */
#if EFSYS_OPT_BIST
    ef10_bist_enable_offline,	/* epo_bist_enable_offline */
    ef10_bist_start,		/* epo_bist_start */
    ef10_bist_poll,			/* epo_bist_poll */
    ef10_bist_stop,			/* epo_bist_stop */
#endif	/* EFSYS_OPT_BIST */
};



static const struct sfc_efx_mcdi_ops sfc_vdpa_mcdi_ops = {
    .dma_alloc	= sfc_vdpa_mcdi_dma_alloc,
        mz = rte_memzone_reserve_aligned(mz_name, mcdi_buff_size, numa_node, RTE_MEMZONE_IOVA_CONTIG, PAGE_SIZE)
        rte_vfio_container_dma_map(sva->vfio_container_fd, (uint64_t)mz->addr, mcdi_iova, mcdi_buff_size)
    .dma_free	= sfc_vdpa_mcdi_dma_free,
    .sched_restart  = sfc_vdpa_mcdi_sched_restart,
    .mgmt_evq_poll  = sfc_vdpa_mcdi_mgmt_evq_poll,
};

static const efx_nic_ops_t	__efx_nic_medford_ops = {
    ef10_nic_probe,			/* eno_probe */
    medford_board_cfg,		/* eno_board_cfg */
    ef10_nic_set_drv_limits,	/* eno_set_drv_limits */
    ef10_nic_reset,			/* eno_reset */
    ef10_nic_init,			/* eno_init */
    ef10_nic_get_vi_pool,		/* eno_get_vi_pool */
    ef10_nic_get_bar_region,	/* eno_get_bar_region */
    ef10_nic_hw_unavailable,	/* eno_hw_unavailable */
    ef10_nic_set_hw_unavailable,	/* eno_set_hw_unavailable */
#if EFSYS_OPT_DIAG
    ef10_nic_register_test,		/* eno_register_test */
#endif	/* EFSYS_OPT_DIAG */
    ef10_nic_fini,			/* eno_fini */
    ef10_nic_unprobe,		/* eno_unprobe */
};


shm, rte_eth_dev_attach_secondary
rte_eth_dev_pci_allocate



socket_path, bdf
cmdline_parse_inst_t cmd_create_vdpa_port -> cmd_create_vdpa_port_parsed
    rte_strscpy(vports[devcnt].ifname, res->socket_path, MAX_PATH_LEN)
    dev = rte_vdpa_find_device_by_name(res->bdf) -> find vdpa dev by bdf
    vports[devcnt].dev = dev
    start_vdpa(&vports[devcnt])
    devcnt++ -> for next device




static struct rte_vdpa_dev_ops ifcvf_net_ops = {
    .get_queue_num = ifcvf_get_queue_num,
    .get_features = ifcvf_get_vdpa_features,
    .get_protocol_features = ifcvf_get_protocol_features,
    .dev_conf = ifcvf_dev_config,
        rte_atomic32_set(&internal->dev_attached, 1)
        update_datapath(internal)
        rte_vhost_host_notifier_ctrl
    .dev_close = ifcvf_dev_close,
    .set_vring_state = ifcvf_set_vring_state,
        hw->vring[vring].enable = enable
        unset_notify_relay(internal)
        vdpa_enable_vfio_intr(internal, false)
            irq_set->index = VFIO_PCI_MSIX_IRQ_INDEX
            fd_ptr = (int *)&irq_set->data -> arry -> __u8	data[]
            fd_ptr[RTE_INTR_VEC_ZERO_OFFSET] = rte_intr_fd_get(internal->pdev->intr_handle)
                return intr_handle->fd
            fd_ptr[RTE_INTR_VEC_RXTX_OFFSET + i] = vring.callfd -> bind vfio irq to callfd
            fd = eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC)
            internal->intr_fd[i] = fd
            ret = ioctl(internal->vfio_dev_fd, VFIO_DEVICE_SET_IRQS, irq_set) -> then kernel trigger irq by ->  eventfd_signal(trigger, 1) -> qemu handle irq by -> event_notifier_test_and_clear
        ifcvf_config_vring(internal, vring)
            if (hw->vring[vring].enable)
                rte_vhost_get_vhost_vring(vid, vring, &vq)
                gpa = hva_to_gpa(vid, (uint64_t)(uintptr_t)vq.desc)
                hw->vring[vring].desc = gpa
                gpa = hva_to_gpa(vid, (uint64_t)(uintptr_t)vq.avail)
                hw->vring[vring].avail = gpa
                gpa = hva_to_gpa(vid, (uint64_t)(uintptr_t)vq.used)
                hw->vring[vring].used = gpa
                rte_vhost_get_vring_base(vid, vring, &hw->vring[vring].last_avail_idx, &hw->vring[vring].last_used_idx)
                ifcvf_enable_vring_hw(&internal->hw, vring)
                    ifcvf_enable_mq(hw)
                    IFCVF_WRITE_REG16(i, &cfg->queue_select)
                    io_write64_twopart(hw->vring[i].desc, &cfg->queue_desc_lo, &cfg->queue_desc_hi)
                    hw->notify_addr[i] = (void *)((u8 *)hw->notify_base + notify_off * hw->notify_off_multiplier)
                    IFCVF_WRITE_REG16(1, &cfg->queue_enable)
        setup_notify_relay(internal)
        rte_vhost_host_notifier_ctrl(vid, vring, enable)
    .set_features = ifcvf_set_features,
    .migration_done = NULL,
    .get_vfio_group_fd = ifcvf_get_vfio_group_fd,
        list->internal->vfio_group_fd
    .get_vfio_device_fd = ifcvf_get_vfio_device_fd,
        list->internal->vfio_dev_fd
    .get_notify_area = ifcvf_get_notify_area,
    .get_dev_type = ifcvf_get_device_type,
};


static struct rte_vdpa_dev_ops ifcvf_blk_ops = {
    .get_queue_num = ifcvf_get_queue_num,
        *queue_num = list->internal->max_queues
    .get_features = ifcvf_get_vdpa_features,
        *features = list->internal->features
    .set_features = ifcvf_set_features,
        if (internal->sw_lm) -> net/ifc：支持 SW 辅助 VDPA 实时迁移, 在 SW 辅助实时迁移模式下，驱动程序将停止设备并设置一个中介 virtio 环来中继 virtio 驱动程序和 VDPA 设备之间的通信。此数据路径干预将允许 SW 帮助客户机记录实时迁移的脏页。此 SW 回退是事件驱动的中继线程，因此当网络吞吐量较低时，此 SW 回退将占用很少的 CPU 资源，但当吞吐量上升时，中继线程的 CPU 使用率将相应上升。用户在选择实时迁移支持模式时需要考虑所有因素，包括 CPU 使用率、客户机性能下降等。
            ifcvf_sw_fallback_switchover(internal)
                stop the direct IO data path
                unset_notify_relay
                vdpa_ifcvf_stop
                unset_intr_relay
                vdpa_disable_vfio_intr
                rte_vhost_host_notifier_ctrl(vid, RTE_VHOST_QUEUE_ALL, false) -> 内部函数
                    vfio_device_fd = vdpa_dev->ops->get_vfio_device_fd(vid)
                    if (enable)
                        vdpa_dev->ops->get_notify_area(vid, i, &offset, &size) < 0)
                        vhost_user_slave_set_vring_host_notifier(dev, i,vfio_device_fd, offset, size)
                            .request.slave = VHOST_USER_SLAVE_VRING_HOST_NOTIFIER_MSG
                vdpa_enable_vfio_intr(internal, true)
                config the VF
                m_ifcvf_start
                setup_vring_relay
                    rte_ctrl_thread_create(&internal->tid, name, NULL, vring_relay, (void *)internal)
                        epfd = epoll_create(IFCVF_MAX_QUEUES * 2)
                        epoll_ctl(epfd, EPOLL_CTL_ADD, vring.kickfd, &ev
                        nfds = epoll_wait(epfd, events, q_num * 2, -1)
                        update_used_ring(internal, qid)
                            rte_vhost_vring_call(internal->vid, qid) -> Notify the guest that used descriptors have been added to the vring. This function acts as a memory barrier.
                        for (;;)
                            nfds = epoll_wait(epfd, events, q_num * 2, -1)
                            nbytes = read(fd, &buf, 8)
                            qid = events[i].data.u32 >> 1
                            if (events[i].data.u32 & 1)
                                update_used_ring(internal, qid)
                            else
                                ifcvf_notify_queue(&internal->hw, qid)
                                    IFCVF_WRITE_REG16(qid, hw->notify_addr[qid])
                rte_vhost_host_notifier_ctrl(vid, RTE_VHOST_QUEUE_ALL, true)
        else
            rte_vhost_get_log_base(vid, &log_base, &log_size)
            rte_vfio_container_dma_map(internal->vfio_container_fd, log_base, IFCVF_LOG_BASE, log_size)
            ifcvf_enable_logging(&internal->hw, IFCVF_LOG_BASE, log_size)
    .get_protocol_features = ifcvf_blk_get_protocol_features,
        *features |= VDPA_BLK_PROTOCOL_FEATURES
    .dev_conf = ifcvf_dev_config,
    .dev_close = ifcvf_dev_close,
    .set_vring_state = ifcvf_set_vring_state,
    .migration_done = NULL,
    .get_vfio_group_fd = ifcvf_get_vfio_group_fd,
    .get_vfio_device_fd = ifcvf_get_vfio_device_fd,
    .get_notify_area = ifcvf_get_notify_area,
        ret = ioctl(internal->vfio_dev_fd, VFIO_DEVICE_GET_REGION_INFO, &reg)
        *offset = ifcvf_get_queue_notify_off(&internal->hw, qid) + reg.offset
            return (u8 *)hw->notify_addr[qid] - (u8 *)hw->mem_resource[hw->notify_region].addr
        *size = 0x1000
    .get_config = ifcvf_blk_get_config,
    .get_dev_type = ifcvf_get_device_type,
        if (internal->hw.device_type == IFCVF_BLK)
            *type = RTE_VHOST_VDPA_DEVICE_TYPE_BLK
};


intel, vdpa, ifcvf_vdpa.c -> net/ifcvf：添加 ifcvf vDPA 驱动程序，IFCVF vDPA（vhost 数据路径加速）驱动程序为 Intel FPGA 100G VF（IFCVF）提供支持。IFCVF 的数据路径与 virtio 环兼容，它作为 HW vhost 后端工作，可以通过 DMA 直接向/从 virtio 发送/接收数据包。不同的 VF 设备服务于位于不同 VM 中的不同 virtio 前端，因此每个 VF 都需要有自己的 DMA 地址转换服务。在驱动程序探测期间，将创建一个新容器，使用此容器 vDPA 驱动程序可以使用 VM 的内存区域信息对 DMA 重映射表进行编程。实现的关键 vDPA 驱动程序操作：- ifcvf_dev_config：使用 vhost lib 提供的 virtio 信息启用 VF 数据路径，包括 IOMMU 编程以启用 VF DMA 到 VM 的内存，VFIO 中断设置以将 HW 中断路由到 virtio 驱动程序，创建通知中继线程以将 virtio 驱动程序的踢动转换为 MMIO 写入 HW，HW 队列配置。- ifcvf_dev_close：撤销 ifcvf_dev_config 中的所有设置。实时迁移功能由 IFCVF 支持，此驱动程序启用此功能。对于脏页日志记录，VF 有助于记录数据包缓冲区写入，驱动程序有助于在设备停止时将使用的环设为脏的。由于 vDPA 驱动程序需要设置 MSI-X 向量来中断客户机，因此目前仅支持 vfio-pci
RTE_PMD_REGISTER_PCI(net_ifcvf, rte_ifcvf_vdpa)
static struct rte_pci_driver rte_ifcvf_vdpa = {
    .id_table = pci_id_ifcvf_map,
    .drv_flags = 0,
    .probe = ifcvf_pci_probe,
    .remove = ifcvf_pci_remove,
};
ifcvf_pci_probe(struct rte_pci_driver *pci_drv __rte_unused, struct rte_pci_device *pci_dev)
    kvlist = rte_kvargs_parse(pci_dev->device.devargs->args, ifcvf_valid_arguments)
    rte_kvargs_count(kvlist, IFCVF_VDPA_MODE)
    rte_kvargs_process(kvlist, IFCVF_VDPA_MODE, &open_int, &vdpa_mode)
    list = rte_zmalloc("ifcvf", sizeof(*list), 0)
    internal = rte_zmalloc("ifcvf", sizeof(*internal), 0)
    internal->pdev = pci_dev
    ifcvf_vfio_setup(internal)
        rte_vfio_get_group_num(rte_pci_get_sysfs_path(), devname, &iommu_group_num)
        internal->vfio_container_fd = rte_vfio_container_create()
        internal->vfio_group_fd = rte_vfio_container_group_bind(internal->vfio_container_fd, iommu_group_num)
        rte_pci_map_device(dev) -> map hw pcie bar to userspace
            pci_vfio_map_resource -> RTE_PCI_KDRV_VFIO
                pci_vfio_map_resource_primary
                    rte_vfio_setup_device(rte_pci_get_sysfs_path(), pci_addr, &vfio_dev_fd, &device_info)
                        rte_vfio_get_group_num
                        rte_vfio_get_group_fd
                        ioctl(vfio_group_fd, VFIO_GROUP_GET_STATUS, &group_status);
                        ioctl(vfio_group_fd, VFIO_GROUP_SET_CONTAINER,
                        vfio_group_device_count
                        vfio_set_iommu_type
                        rte_mem_event_callback_register(VFIO_MEM_EVENT_CLB_NAME, vfio_mem_event_callback, NULL)
                            msl = rte_mem_virt2memseg_list(addr)
                                virt2memseg_list
                            vfio_dma_mem_map
                            rte_mem_virt2memseg
                        rte_eal_vfio_get_vf_token
                    vfio_res = rte_zmalloc("VFIO_RES", sizeof(*vfio_res), 0)
                    pci_vfio_get_msix_bar(vfio_dev_fd, &vfio_res->msix_table)
                        if (cap_id != PCI_CAP_ID_MSIX)
                    pci_vfio_msix_is_mappable(vfio_dev_fd, vfio_res->msix_table.bar_index)
                        pci_vfio_get_region_info(vfio_dev_fd, &info, msix_region)
                            ioctl(vfio_dev_fd, VFIO_DEVICE_GET_REGION_INFO, ri)
                        pci_vfio_info_cap(info, RTE_VFIO_CAP_MSIX_MAPPABLE)
                    for (i = 0; i < vfio_res->nb_maps; i++)
                        pci_vfio_get_region_info(vfio_dev_fd, &reg, i)
                        pci_vfio_is_ioport_bar(vfio_dev_fd, i)
                        pci_map_addr = pci_find_max_end_va()
                        pci_vfio_mmap_bar(vfio_dev_fd, vfio_res, i, 0)
                    pci_rte_vfio_setup_device(dev, vfio_dev_fd)
                        pci_vfio_setup_interrupts
                    pci_vfio_enable_notifier(dev, vfio_dev_fd)
        internal->hw.mem_resource[i].addr = internal->pdev->mem_resource[i].addr;
        internal->hw.mem_resource[i].phys_addr = internal->pdev->mem_resource[i].phys_addr;
        internal->hw.mem_resource[i].len = internal->pdev->mem_resource[i].len;
    ifcvf_init_hw(&internal->hw, internal->pdev)
        ret = PCI_READ_CONFIG_BYTE(dev, &pos, PCI_CAPABILITY_LIST)
        PCI_READ_CONFIG_RANGE(dev, (u32 *)&cap, sizeof(cap), pos)
            rte_pci_read_config(dev, val, size, where)
                case RTE_PCI_KDRV_IGB_UIO
                case RTE_PCI_KDRV_UIO_GENERIC
                    pci_uio_read_config(intr_handle, buf, len, offset)
                case RTE_PCI_KDRV_VFIO
                    pci_vfio_read_config(intr_handle, buf, len, offset)
                        int vfio_dev_fd = rte_intr_dev_fd_get(intr_handle)
                        pread64(vfio_dev_fd, buf, len, VFIO_GET_REGION_ADDR(VFIO_PCI_CONFIG_REGION_INDEX) + offs)
                default
                    rte_pci_device_name(&device->addr, devname, RTE_DEV_NAME_MAX_LEN)
        case IFCVF_PCI_CAP_COMMON_CFG
            hw->common_cfg = get_cap_addr(hw, &cap)
                hw->mem_resource[bar].addr + offset
        case IFCVF_PCI_CAP_NOTIFY_CFG
            hw->notify_base = get_cap_addr(hw, &cap)
        case IFCVF_PCI_CAP_ISR_CFG
            hw->isr = get_cap_addr(hw, &cap) -> 产生int#x中断, 如果设备不支持msi-x capability的话，在设备配置变化或者需要进行队列kick通知时，就需要用到isr capability
        case IFCVF_PCI_CAP_DEVICE_CFG
            hw->dev_cfg = get_cap_addr(hw, &cap)
        hw->lm_cfg = hw->mem_resource[4].addr -> live migration
    features = ifcvf_get_features(&internal->hw)
        features_hi = IFCVF_READ_REG32(&cfg->device_feature) -> read features low and high
    device_id = ifcvf_pci_get_device_type(pci_dev)
    if (device_id == VIRTIO_ID_NET)
        internal->max_queues = MIN(IFCVF_MAX_QUEUES, queue_pairs)
    else if (device_id == VIRTIO_ID_BLOCK) -> when set device_id?
        internal->hw.device_type = IFCVF_BLK
        internal->max_queues = MIN(IFCVF_MAX_QUEUES, internal->hw.blk_cfg->num_queues)
        DRV_LOG(DEBUG, "capacity  : %"PRIu64"G", capacity >> 21)
        DRV_LOG(DEBUG, "size_max  : 0x%08x", internal->hw.blk_cfg->size_max);
        DRV_LOG(DEBUG, "seg_max   : 0x%08x", internal->hw.blk_cfg->seg_max);
        DRV_LOG(DEBUG, "blk_size  : 0x%08x", internal->hw.blk_cfg->blk_size);
        DRV_LOG(DEBUG, "    cylinders: %u", internal->hw.blk_cfg->geometry.cylinders);
        DRV_LOG(DEBUG, "    heads    : %u", internal->hw.blk_cfg->geometry.heads);
        DRV_LOG(DEBUG, "    sectors  : %u", internal->hw.blk_cfg->geometry.sectors);
        DRV_LOG(DEBUG, "num_queues: 0x%08x",internal->hw.blk_cfg->num_queues);
    if (rte_kvargs_count(kvlist, IFCVF_SW_FALLBACK_LM)) -> sw-live-migration
        rte_kvargs_process(kvlist, IFCVF_SW_FALLBACK_LM, &open_int, &sw_fallback_lm)
    TAILQ_INSERT_TAIL(&internal_list, list, next)
    internal->vdev = rte_vdpa_register_device(&pci_dev->device, dev_info[internal->hw.device_type].ops) -> ifcvf_blk_ops | ifcvf_net_ops
        dev = __vdpa_find_device_by_name(rte_dev->name)
        dev = rte_zmalloc(NULL, sizeof(*dev), 0)
        dev->ops = ops
        ops->get_dev_type(dev, &dev->type)
        TAILQ_INSERT_TAIL(&vdpa_device_list, dev, next)
    rte_atomic32_set(&internal->started, 1)
    update_datapath(internal) -> need check state
        ifcvf_dma_map(internal, true)
            rte_vhost_get_mem_table(internal->vid, &mem)
            rte_vfio_container_dma_map(vfio_container_fd, reg->host_user_addr, reg->guest_phys_addr, reg->size)
        vdpa_enable_vfio_intr(internal, false) -> enable irq
        vdpa_ifcvf_start(internal)
            ifcvf_start_hw(&internal->hw)
                ifcvf_hw_enable(hw)
                    IFCVF_WRITE_REG16(0, &cfg->msix_config)
                    ifcvf_enable_mq(hw)
                    IFCVF_WRITE_REG16(i, &cfg->queue_select);
                    IFCVF_WRITE_REG16(i + 1, &cfg->queue_msix_vector)
                    hw->notify_addr[i] = (void *)((u8 *)hw->notify_base + notify_off * hw->notify_off_multiplier)
                    IFCVF_WRITE_REG16(1, &cfg->queue_enable)
                ifcvf_add_status(hw, IFCVF_CONFIG_STATUS_DRIVER_OK)
                    ifcvf_set_status(hw, status)
                        IFCVF_WRITE_REG8(status, &hw->common_cfg->device_status)
                    ifcvf_get_status(hw)
        setup_notify_relay(internal)
            rte_ctrl_thread_create(&internal->tid, name, NULL, notify_relay, (void *)internal)
                epfd = epoll_create(IFCVF_MAX_QUEUES * 2)
                rte_vhost_get_vhost_vring(internal->vid, qid, &vring)
                epoll_ctl(epfd, EPOLL_CTL_ADD, vring.kickfd, &ev) -> when qemu kick vhost, wakup epoll
                for (;;)
                    nfds = epoll_wait(epfd, events, q_num, -1)
                    ifcvf_notify_queue(hw, qid)
        setup_intr_relay(internal)
            rte_ctrl_thread_create(&internal->intr_tid, name, NULL, intr_relay, (void *)internal) -> 创建具有给定名称和属性的控制线程。新线程的亲和性基于调用 rte_eal_init() 时检索到的 CPU 亲和性，然后排除数据平面和服务 lcore。如果设置线程名称失败，则忽略错误并记录调试消息 -> vdpa/ifc：为配置空间添加中断处理，创建一个线程来轮询和中继配置空间更改中断。使用 VHOST_USER_SLAVE_CONFIG_CHANGE_MSG 通知 QEMU
                csc_epfd = epoll_create(1)
                ev.data.fd = rte_intr_fd_get(internal->pdev->intr_handle)
                for (;;)
                    csc_val = epoll_wait(csc_epfd, &csc_event, 1, -1)
                    nbytes = read(csc_event.data.fd, &buf, 8)
                    virtio_interrupt_handler(internal)
                        rte_vhost_slave_config_change(vid, 1)
                            vhost_user_slave_config_change(dev, need_reply)
                                .request.slave = VHOST_USER_SLAVE_CONFIG_CHANGE_MSG -> change notifications -> kernel virtio_config_changed
                                send_vhost_slave_message(dev, &ctx)
                                process_slave_message_reply(dev, &ctx)
    rte_kvargs_free(kvlist)



new_connection(int vid)
    vhost_session_install_rte_compat_hooks
        rte_vhost_extern_callback_register(vsession->vid, &g_spdk_extern_vhost_ops, NULL)
struct rte_vhost_user_extern_ops g_spdk_extern_vhost_ops = {
    .pre_msg_handle = extern_vhost_pre_msg_handler,
        rte_vhost_get_ifname(vid, path, PATH_MAX)
        ctrlr = vhost_blk_ctrlr_find(path)
        vhost_blk_get_config(ctrlr->bdev, msg->payload.cfg.region, msg->payload.cfg.size)
            struct virtio_blk_config blkcfg
            blkcfg.blk_size = blk_size
            blkcfg.capacity = (blkcnt * blk_size) / 512
            memcpy(config, &blkcfg, RTE_MIN(len, sizeof(blkcfg)))
    .post_msg_handle = extern_vhost_post_msg_handler,
};
extern_vhost_post_msg_handler(int vid, void *_msg)
    vsession = vhost_session_find_by_vid(vid)
    if (msg->request == VHOST_USER_SET_MEM_TABLE)
        vhost_register_memtable_if_required(vsession, vid)
            vhost_get_mem_table(vid, &new_mem) -> vhost：如果未发生改变，则注册一次 memtable，将 memtable 寄存器移出 start_device，移入 vhost-msg SET_MEMTABLE 的 post_handler 中；在 destroy_connection 中而不是 destroy_device 中注销 memtable，如果 memtable 信息在 msg 中未发生改变，则无需多次注册
                 rte_vhost_get_mem_table(vid, mem) -> 将在内部分配一个 rte_vhost_vhost_memory 对象，用于保存客户机内存区域。应用程序应在 destroy_device() 回调中释放它
                    m = malloc(sizeof(struct rte_vhost_memory) + size)
                    memcpy(m->regions, dev->mem->regions, size)
            vhost_session_mem_register(vsession->mem)
                for (i = 0; i < mem->nregions; i++)
                    vhost_session_mem_region_calc(&previous_start, &start, &end, &len, &mem->regions[i])
                        *start = FLOOR_2MB(region->mmap_addr)
                        *end = CEIL_2MB(region->mmap_addr + region->mmap_size)
                    spdk_mem_register((void *)start, len)
                        spdk_mem_map_translate(g_mem_reg_map, (uint64_t)seg_vaddr, NULL)
                        spdk_mem_map_set_translation(g_mem_reg_map, (uint64_t)vaddr, VALUE_2MB, seg_len == 0 ? REG_MAP_REGISTERED | REG_MAP_NOTIFY_START : REG_MAP_REGISTERED)
            if (vhost_memory_changed(new_mem, vsession->mem))
                vhost_session_mem_register(vsession->mem)
    switch (msg->request)
    case VHOST_USER_SET_FEATURES:
        vhost_get_negotiated_features(vid, &vsession->negotiated_features) -> rte_vhost_get_negotiated_features(vid, negotiated_features)
    case VHOST_USER_SET_VRING_CALL
        set_device_vq_callfd
            q = &vsession->virtqueue[qid]
            q->used_req_cnt += 1
    case VHOST_USER_SET_VRING_KICK
        enable_device_vq
            rte_vhost_get_vhost_vring(vsession->vid, qid, &q->vring)
            rte_vhost_get_vhost_ring_inflight
            rte_vhost_get_vring_base
            rc = backend->alloc_vq_tasks(vsession, qid) -> alloc_vq_task_pool
                vq->tasks = spdk_zmalloc
            spdk_interrupt_mode_is_enabled
                backend->register_vq_interrupt(vsession, q)
        g_spdk_vhost_ops.new_device(vid)




static __rte_used cmdline_parse_ctx_t main_ctx[] = {
    &cmd_help,
    &cmd_list,
    &cmd_create,
    &cmd_stats,
    &cmd_quit,
    NULL
};

static cmdline_parse_inst_t cmd_list = {
    .f = cmd_list_parsed,
        RTE_DEV_FOREACH(dev, "class=vdpa", &dev_iter)
            rte_vdpa_find_device_by_name
                __vdpa_find_device_by_name
                    TAILQ_FOREACH(dev, vdpa_device_list, next) <- TAILQ_INSERT_TAIL(vdpa_device_list, dev, next)
        rte_vdpa_get_queue_num
        rte_vdpa_get_features
        cmdline_printf(cl, "%s\t\t%" PRIu32 "\t\t0x%" PRIx64 "\n", rte_dev_name(dev), queue_num, features)
    .data = NULL,
    .help_str = "list all available vdpa devices",
    .tokens = {
        (void *)&cmd_list_list_tok,
        NULL,
    }
};

static cmdline_parse_inst_t cmd_create = {
    .f = cmd_create_parsed,
        rte_strscpy(vports[devcnt].ifname, res->socket_path, MAX_PATH_LEN) -> devcnt auto ++
        dev = rte_vdpa_find_device_by_name(res->bdf) -> find vdpa dev by bus device function addr
            __vdpa_find_device_by_name(name)
        start_vdpa(&vports[devcnt])
    .data = NULL,
    .help_str = "create a new vdpa port",
    .tokens = {
        (void *)&cmd_create_create_tok,
        (void *)&cmd_create_socket_path_tok,
        (void *)&cmd_create_bdf_tok,
        NULL,
    }
};




rte_eal_hotplug_add("pci", pf_name, "vdpa=2")



log:
RTE_LOG(NOTICE, VDPA, "%s is a blk device\n", socket_path);





ctrl_thread_start


vhost stack:
OVS
                       netdev_dpdk_vhost_construct
                        (struct netdev *netdev)
                                   |
                                   |
DPDK                               V
                      rte_vhost_driver_register
                        (const char *path, uint64_t flags)
                                   |
             -----------------------------------------------
             |                                             |
             V                                             |
vhost_user_create_server                                   |
  (struct vhost_user_socket *vsocket)                      |
             |                                             |
             V                                             V
vhost_user_server_new_connection                     vhost_user_create_client                     vhost_user_client_reconnect
(int fd, void *dat, int *remove __rte_unused)          (struct vhost_user_socket *vsocket)          (void *arg __rte_unused)
             |                                             |                                                  |
             V                                             V                                                  V
             --------------------------------------------------------------------------------------------------
                                                           |
                                                           V
                                               vhost_user_add_connection
                                                 (int fd, struct vhost_user_socket *vsocket)
                                                           |
                                                           V
                                               vhost_user_read_cb
                                                (int connfd, void *dat, int *remove)
                                                           |
                                                           V
                                                vhost_user_msg_handler



modern_setup_queue
    notify_off = rte_read16(&hw->common_cfg->queue_notify_off)
    vq->notify_addr = (void *)((uint8_t *)hw->notify_base + notify_off * hw->notify_off_multiplier)








rte_eal_intr_init
    rte_ctrl_thread_create(&intr_thread, "eal-intr-thread", NULL, eal_intr_thread_main, NULL) -> create the host thread to wait/handle the interrupt
        TAILQ_FOREACH(src, &intr_sources, next) <- TAILQ_INSERT_TAIL(&intr_sources, src, next)
            ev.data.fd = rte_intr_fd_get(src->intr_handle) -> example -> kickfd
            epoll_ctl(pfd, EPOLL_CTL_ADD, rte_intr_fd_get(src->intr_handle), &ev)
        eal_intr_handle_interrupts(pfd, numfds)
            for
                nfds = epoll_wait(pfd, events, totalfds, EAL_INTR_EPOLL_WAIT_FOREVER)
                eal_intr_process_interrupts(events, nfds)
                    rte_intr_fd_get(src->intr_handle)
                    bytes_read = read(events[n].data.fd, &buf, bytes_read)
                    active_cb.cb_fn(active_cb.cb_arg) -> call irq cb <- rte_intr_callback_register



static const struct eth_dev_ops virtio_eth_dev_ops = {
    .dev_configure           = virtio_dev_configure,
    .dev_start               = virtio_dev_start,
    .dev_stop                = virtio_dev_stop,
    .dev_close               = virtio_dev_close,
    .promiscuous_enable      = virtio_dev_promiscuous_enable,
    .promiscuous_disable     = virtio_dev_promiscuous_disable,
    .allmulticast_enable     = virtio_dev_allmulticast_enable,
    .allmulticast_disable    = virtio_dev_allmulticast_disable,
    .mtu_set                 = virtio_mtu_set,
    .dev_infos_get           = virtio_dev_info_get,
    .stats_get               = virtio_dev_stats_get,
    .xstats_get              = virtio_dev_xstats_get,
    .xstats_get_names        = virtio_dev_xstats_get_names,
    .stats_reset             = virtio_dev_stats_reset,
    .xstats_reset            = virtio_dev_stats_reset,
    .link_update             = virtio_dev_link_update,
    .vlan_offload_set        = virtio_dev_vlan_offload_set,
    .rx_queue_setup          = virtio_dev_rx_queue_setup,
    .rx_queue_intr_enable    = virtio_dev_rx_queue_intr_enable,
        virtqueue_enable_intr
            virtqueue_enable_intr_split
                vq->vq_split.ring.avail->flags &= (~VRING_AVAIL_F_NO_INTERRUPT)
    .rx_queue_intr_disable   = virtio_dev_rx_queue_intr_disable,
    .tx_queue_setup          = virtio_dev_tx_queue_setup,
    .rss_hash_update         = virtio_dev_rss_hash_update,
    .rss_hash_conf_get       = virtio_dev_rss_hash_conf_get,
    .reta_update             = virtio_dev_rss_reta_update,
    .reta_query              = virtio_dev_rss_reta_query,
    /* collect stats per queue */
    .queue_stats_mapping_set = virtio_dev_queue_stats_mapping_set,
    .vlan_filter_set         = virtio_vlan_filter_set,
    .mac_addr_add            = virtio_mac_addr_add,
    .mac_addr_remove         = virtio_mac_addr_remove,
    .mac_addr_set            = virtio_mac_addr_set,
    .get_monitor_addr        = virtio_get_monitor_addr,
};




mlx5 将 lwm（Limit WaterMark）字段添加到 Rxq 对象，该字段指示 HW 用于向用户发出 LWM 事件的 RX 队列大小百分比。允许在modify_rq 命令中设置 LWM。通过添加 RDY2RDY 状态更改允许动态配置 LWM



mlx5_dev_interrupt_handler_devx



const struct eth_dev_ops mlx5_dev_ops = {
    .dev_configure = mlx5_dev_configure,
    .dev_start = mlx5_dev_start,
        mlx5_txpp_start
        mlx5_txpp_create
            mlx5_txpp_start_service
                sh->txpp.intr_handle = mlx5_os_interrupt_handler_create(RTE_INTR_INSTANCE_F_SHARED, false, fd, mlx5_txpp_interrupt_handler, sh)
    .dev_stop = mlx5_dev_stop,
    .dev_set_link_down = mlx5_set_link_down,
    .dev_set_link_up = mlx5_set_link_up,
    .dev_close = mlx5_dev_close,
    .promiscuous_enable = mlx5_promiscuous_enable,
    .promiscuous_disable = mlx5_promiscuous_disable,
    .allmulticast_enable = mlx5_allmulticast_enable,
    .allmulticast_disable = mlx5_allmulticast_disable,
    .link_update = mlx5_link_update,
    .stats_get = mlx5_stats_get,
    .stats_reset = mlx5_stats_reset,
    .xstats_get = mlx5_xstats_get,
    .xstats_reset = mlx5_xstats_reset,
    .xstats_get_names = mlx5_xstats_get_names,
    .fw_version_get = mlx5_fw_version_get,
    .dev_infos_get = mlx5_dev_infos_get,
    .representor_info_get = mlx5_representor_info_get,
    .read_clock = mlx5_txpp_read_clock,
    .dev_supported_ptypes_get = mlx5_dev_supported_ptypes_get,
    .vlan_filter_set = mlx5_vlan_filter_set,
    .rx_queue_setup = mlx5_rx_queue_setup,
    .rx_queue_avail_thresh_set = mlx5_rx_queue_lwm_set,
    .rx_queue_avail_thresh_query = mlx5_rx_queue_lwm_query,
    .rx_hairpin_queue_setup = mlx5_rx_hairpin_queue_setup,
    .tx_queue_setup = mlx5_tx_queue_setup,
    .tx_hairpin_queue_setup = mlx5_tx_hairpin_queue_setup,
    .rx_queue_release = mlx5_rx_queue_release,
    .tx_queue_release = mlx5_tx_queue_release,
    .rx_queue_start = mlx5_rx_queue_start,
    .rx_queue_stop = mlx5_rx_queue_stop,
    .tx_queue_start = mlx5_tx_queue_start,
    .tx_queue_stop = mlx5_tx_queue_stop,
    .flow_ctrl_get = mlx5_dev_get_flow_ctrl,
    .flow_ctrl_set = mlx5_dev_set_flow_ctrl,
    .mac_addr_remove = mlx5_mac_addr_remove,
    .mac_addr_add = mlx5_mac_addr_add,
    .mac_addr_set = mlx5_mac_addr_set,
    .set_mc_addr_list = mlx5_set_mc_addr_list,
    .mtu_set = mlx5_dev_set_mtu,
    .vlan_strip_queue_set = mlx5_vlan_strip_queue_set,
    .vlan_offload_set = mlx5_vlan_offload_set,
    .reta_update = mlx5_dev_rss_reta_update,
    .reta_query = mlx5_dev_rss_reta_query,
    .rss_hash_update = mlx5_rss_hash_update,
    .rss_hash_conf_get = mlx5_rss_hash_conf_get,
    .flow_ops_get = mlx5_flow_ops_get,
    .rxq_info_get = mlx5_rxq_info_get,
    .txq_info_get = mlx5_txq_info_get,
    .rx_burst_mode_get = mlx5_rx_burst_mode_get,
    .tx_burst_mode_get = mlx5_tx_burst_mode_get,
    .rx_queue_intr_enable = mlx5_rx_intr_enable,
    .rx_queue_intr_disable = mlx5_rx_intr_disable,
    .is_removed = mlx5_is_removed,
    .udp_tunnel_port_add  = mlx5_udp_tunnel_port_add,
    .get_module_info = mlx5_get_module_info,
    .get_module_eeprom = mlx5_get_module_eeprom,
    .hairpin_cap_get = mlx5_hairpin_cap_get,
    .mtr_ops_get = mlx5_flow_meter_ops_get,
    .hairpin_bind = mlx5_hairpin_bind,
    .hairpin_unbind = mlx5_hairpin_unbind,
    .hairpin_get_peer_ports = mlx5_hairpin_get_peer_ports,
    .hairpin_queue_peer_update = mlx5_hairpin_queue_peer_update,
    .hairpin_queue_peer_bind = mlx5_hairpin_queue_peer_bind,
    .hairpin_queue_peer_unbind = mlx5_hairpin_queue_peer_unbind,
    .get_monitor_addr = mlx5_get_monitor_addr,
};



vfio:
modprobe vfio-pci
readlink /sys/bus/pci/devices/0000:03:00.0/iommu_group
ls -alh  /sys/bus/pci/devices/0000:03:00.0/
lspci -n -s 0000:03:00.0
echo 0000:06:0d.0 > /sys/bus/pci/devices/0000:06:0d.0/driver/unbind
echo 1102 0002 > /sys/bus/pci/drivers/vfio-pci/new_id
$ ls -l /sys/bus/pci/devices/0000:03:00.0/iommu_group/devices


vdpa:
#get network businfo
lshw -c network -businfo

#set hugepage
echo 2048 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
#echo 2048 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages
cat /proc/meminfo | grep -i huge

#bind vfio driver
modprobe vfio-pci
./usertools/dpdk-devbind.py -b vfio-pci 0000:03:00.0

#get vfio status
./usertools/dpdk-devbind.py -s

#start dpdk-vdpa
cd build/examples/
./dpdk-vdpa -a 0000:03:00.1,class=vdpa --log-level=pmd,info -- -i



vfio-pci如何将硬件设备的寄存器和bar空间映射到用户空间的？[转载],背景：  如果一块新的网卡，要对其开发DPDK驱动支持，DPDK的框架已经搭建好了，需要我们作的主要实现用户态的驱动。能够支持用户态驱动的关键是能够将硬件设备的寄存器，（pcie设备的话）bar空间，中断等映射到用户空间。  目前实现有两个UIO和VFIO两种方式，VFIO是比较新的方式，支持虚拟化和隔离，需要有IOMMU(SMMU)硬件的支持。如果没有IOMMU硬件，但是想用DPDK需要改用UI，否则报错-22 No such device  vfio-pci如何将硬件寄存器和bar空间映射到用户空间的呢？  rte_pci_map_device->pci_vfio_map_resource->pci_vfio_map_resource_primary->pci_vfio_mmap_bar->mmap  使用dpdk的程序（如ovs）调用rte_dev_probe向dpdk注册一个设备，rte_dev_probe的核心处理函数为local_dev_probe，这个函数主要包含了设备总线的匹配，pci设备的bar空间映射，以及最终为设备添加ixgbe驱动。  local_dev_probe的plug最终调用pci_plug，然后遍历bus上的所有驱动为设备匹配驱动，匹配驱动的函数为rte_pci_match，从这个函数可以看出，其实就是通过匹配驱动的id_table里的信息是否能匹配上设备的pci信息，以ixgbe为例，这里的id_table一开始就定义好的，然后存放在struct rte_pci_driver rte_ixgbe_pmd里，最终通过RTE_PMD_REGISTER_PCI将ixgbe_pmd驱动注册到pci总线上。  为设备找到驱动后，接下来一步比较重要的是为设备映射资源信息，如果使用vfio驱动，调用pci_vfio_map_resource，这个函数一开始先通过rte_vfio_setup_device为设备分配vfio_container_id、vfio_group_id，同时设置iommu_type，然后调用dma_map_func将rte_eal_get_configuration()->mem_config的内存信息进行dma映射，这里的mem_config表示dpdk管理的内存信息（从这里看，dpdk应该是一开始会将所有内存都进行dma映射？后面驱动的rx_ring、tx_ring分配dma地址的时候，貌似也没有进一步的dma映射，而是直接使用这里分配好的iova地址。）。 完成dma映射后，通过VFIO_DEVICE_GET_INFO获取设备的信息（主要是pci的bar个数信息及中断信息）。   获取到pci的bar个数信息后，先通过pci_vfio_get_region_info获取每个bar region的地址偏移及大小信息，然后再通过pci_vfio_mmap_bar将其映射到用户空间，并将映射后的bar地址信息存放在rte_pci_device->mem_resource。  接下来主要是调用驱动的probe函数，初始化设备信息，如ixgbe，最终调用eth_ixgbe_pci_probe，该probe函数主要是调用rte_eth_dev_create  参考：  https://www.twblogs.net/a/5efa9d0533599de3c62bb3aa/?lang=zh-cn  https://www.jianshu.com/p/4a240af0e5c6  https://docs.kernel.org/driver-api/vfio.html  https://cwshu.github.io/arm_virt_notes/notes/vfio/vfio_core2.html


cmdline_parse_inst_t cmd_create_vdpa_port
    cmd_create_vdpa_port_parsed
        dev = rte_vdpa_find_device_by_name(res->bdf)
        start_vdpa(&vports[devcnt]




static struct rte_vdpa_dev_ops ifcvf_net_ops = {
    .get_queue_num = ifcvf_get_queue_num,
    .get_features = ifcvf_get_vdpa_features,
    .get_protocol_features = ifcvf_get_protocol_features,
    .dev_conf = ifcvf_dev_config,
    .dev_close = ifcvf_dev_close,
    .set_vring_state = ifcvf_set_vring_state,
    .set_features = ifcvf_set_features,
    .migration_done = NULL,
    .get_vfio_group_fd = ifcvf_get_vfio_group_fd,
    .get_vfio_device_fd = ifcvf_get_vfio_device_fd,
    .get_notify_area = ifcvf_get_notify_area,
    .get_dev_type = ifcvf_get_device_type,
};


static struct rte_vdpa_dev_ops ifcvf_blk_ops = {
    .get_queue_num = ifcvf_get_queue_num,
    .get_features = ifcvf_get_vdpa_features,
    .set_features = ifcvf_set_features,
    .get_protocol_features = ifcvf_blk_get_protocol_features,
    .dev_conf = ifcvf_dev_config,
    .dev_close = ifcvf_dev_close,
    .set_vring_state = ifcvf_set_vring_state,
    .migration_done = NULL,
    .get_vfio_group_fd = ifcvf_get_vfio_group_fd,
    .get_vfio_device_fd = ifcvf_get_vfio_device_fd,
    .get_notify_area = ifcvf_get_notify_area,
    .get_config = ifcvf_blk_get_config,
    .get_dev_type = ifcvf_get_device_type,
};


intel, vdpa,
RTE_PMD_REGISTER_PCI(net_ifcvf, rte_ifcvf_vdpa)
ifcvf_pci_probe
    kvlist = rte_kvargs_parse(pci_dev->device.devargs->args, ifcvf_valid_arguments)
    rte_kvargs_process(kvlist, IFCVF_VDPA_MODE, &open_int, &vdpa_mode)
    list = rte_zmalloc("ifcvf", sizeof(*list), 0)
    internal = rte_zmalloc("ifcvf", sizeof(*internal), 0)
    ifcvf_vfio_setup(internal)
        rte_vfio_get_group_num(rte_pci_get_sysfs_path(), devname, &iommu_group_num)
        internal->vfio_container_fd = rte_vfio_container_create()
        internal->vfio_group_fd = rte_vfio_container_group_bind(internal->vfio_container_fd, iommu_group_num)
        internal->hw.mem_resource[i].addr = internal->pdev->mem_resource[i].addr;
        internal->hw.mem_resource[i].phys_addr = internal->pdev->mem_resource[i].phys_addr;
        internal->hw.mem_resource[i].len = internal->pdev->mem_resource[i].len;
    ifcvf_init_hw(&internal->hw, internal->pdev)
        ret = PCI_READ_CONFIG_BYTE(dev, &pos, PCI_CAPABILITY_LIST)
        PCI_READ_CONFIG_RANGE(dev, (u32 *)&cap, sizeof(cap), pos)
        case IFCVF_PCI_CAP_COMMON_CFG
        case IFCVF_PCI_CAP_NOTIFY_CFG
        case IFCVF_PCI_CAP_ISR_CFG
        case IFCVF_PCI_CAP_DEVICE_CFG
        hw->lm_cfg = hw->mem_resource[4].addr
    rte_kvargs_process(kvlist, IFCVF_SW_FALLBACK_LM, &open_int, &sw_fallback_lm)
    TAILQ_INSERT_TAIL(&internal_list, list, next)
    internal->vdev = rte_vdpa_register_device(&pci_dev->device, dev_info[internal->hw.device_type].ops) -> ifcvf_blk_ops | ifcvf_net_ops


new_connection(int vid)
    vhost_session_install_rte_compat_hooks
        rte_vhost_extern_callback_register(vsession->vid, &g_spdk_extern_vhost_ops, NULL)
struct rte_vhost_user_extern_ops g_spdk_extern_vhost_ops = {
    .pre_msg_handle = extern_vhost_pre_msg_handler,
        rte_vhost_get_ifname(vid, path, PATH_MAX)
        ctrlr = vhost_blk_ctrlr_find(path)
        vhost_blk_get_config(ctrlr->bdev, msg->payload.cfg.region, msg->payload.cfg.size)
    .post_msg_handle = extern_vhost_post_msg_handler,
};
extern_vhost_post_msg_handler(int vid, void *_msg)
    vsession = vhost_session_find_by_vid(vid)
    if (msg->request == VHOST_USER_SET_MEM_TABLE)
        vhost_register_memtable_if_required(vsession, vid)
            vhost_get_mem_table(vid, &new_mem) -> vhost：如果未发生改变，则注册一次 memtable，将 memtable 寄存器移出 start_device，移入 vhost-msg SET_MEMTABLE 的 post_handler 中；在 destroy_connection 中而不是 destroy_device 中注销 memtable，如果 memtable 信息在 msg 中未发生改变，则无需多次注册
                 rte_vhost_get_mem_table(vid, mem) -> 将在内部分配一个 rte_vhost_vhost_memory 对象，用于保存客户机内存区域。应用程序应在 destroy_device() 回调中释放它
                    m = malloc(sizeof(struct rte_vhost_memory) + size)
                    memcpy(m->regions, dev->mem->regions, size)
            vhost_session_mem_register(vsession->mem)
                for (i = 0; i < mem->nregions; i++)
                    vhost_session_mem_region_calc(&previous_start, &start, &end, &len, &mem->regions[i])
                        *start = FLOOR_2MB(region->mmap_addr)
                        *end = CEIL_2MB(region->mmap_addr + region->mmap_size)
                    spdk_mem_register((void *)start, len)
                        spdk_mem_map_translate(g_mem_reg_map, (uint64_t)seg_vaddr, NULL)
                        spdk_mem_map_set_translation(g_mem_reg_map, (uint64_t)vaddr, VALUE_2MB, seg_len == 0 ? REG_MAP_REGISTERED | REG_MAP_NOTIFY_START : REG_MAP_REGISTERED)
            if (vhost_memory_changed(new_mem, vsession->mem))
                vhost_session_mem_register(vsession->mem)
    switch (msg->request)
    case VHOST_USER_SET_FEATURES:
        vhost_get_negotiated_features(vid, &vsession->negotiated_features) -> rte_vhost_get_negotiated_features(vid, negotiated_features)
    case VHOST_USER_SET_VRING_CALL
        set_device_vq_callfd
            q = &vsession->virtqueue[qid]
            q->used_req_cnt += 1
    case VHOST_USER_SET_VRING_KICK
        enable_device_vq
            rte_vhost_get_vhost_vring(vsession->vid, qid, &q->vring)
            rte_vhost_get_vhost_ring_inflight
            rte_vhost_get_vring_base
            rc = backend->alloc_vq_tasks(vsession, qid) -> alloc_vq_task_pool
                vq->tasks = spdk_zmalloc
            spdk_interrupt_mode_is_enabled
                backend->register_vq_interrupt(vsession, q)
        g_spdk_vhost_ops.new_device(vid)





/* EAL defines */
#define RTE_MAX_HEAPS 32
#define RTE_MAX_MEMSEG_LISTS 128
#define RTE_MAX_MEMSEG_PER_LIST 8192
#define RTE_MAX_MEM_MB_PER_LIST 32768
#define RTE_MAX_MEMSEG_PER_TYPE 32768
#define RTE_MAX_MEM_MB_PER_TYPE 65536
#define RTE_MAX_MEMZONE 2560
#define RTE_MAX_TAILQ 32
#define RTE_LOG_DP_LEVEL RTE_LOG_INFO
#define RTE_BACKTRACE 1
#define RTE_MAX_VFIO_CONTAINERS 64



malloc_heap_find_external_seg
    rte_memseg_list_walk_thread_unsafe(extseg_walk, &wa)




malloc_heap_free
    rte_spinlock_lock(&(heap->lock))
    malloc_elem_free
        elem = malloc_elem_join_adjacent_free(elem)
        join_elem(elem, elem->next)
            if (elem1->pad)
                struct malloc_elem *inner = RTE_PTR_ADD(elem1, elem1->pad)
                inner->size = elem1->size - elem1->pad
        malloc_elem_free_list_insert(elem)
        memset(ptr, MALLOC_POISON, data_len)


rte_zmalloc_socket
    rte_malloc_socket
    malloc_elem_from_data


static const struct eth_dev_ops ops = {
	.dev_start              = tap_dev_start,
	.dev_stop               = tap_dev_stop,
	.dev_close              = tap_dev_close,
	.dev_configure          = tap_dev_configure,
	.dev_infos_get          = tap_dev_info,
	.rx_queue_setup         = tap_rx_queue_setup,
	.tx_queue_setup         = tap_tx_queue_setup,
	.rx_queue_start         = tap_rx_queue_start,
	.tx_queue_start         = tap_tx_queue_start,
	.rx_queue_stop          = tap_rx_queue_stop,
	.tx_queue_stop          = tap_tx_queue_stop,
	.rx_queue_release       = tap_rx_queue_release,
	.tx_queue_release       = tap_tx_queue_release,
	.flow_ctrl_get          = tap_flow_ctrl_get,
	.flow_ctrl_set          = tap_flow_ctrl_set,
	.link_update            = tap_link_update,
	.dev_set_link_up        = tap_link_set_up,
	.dev_set_link_down      = tap_link_set_down,
	.promiscuous_enable     = tap_promisc_enable,
	.promiscuous_disable    = tap_promisc_disable,
	.allmulticast_enable    = tap_allmulti_enable,
	.allmulticast_disable   = tap_allmulti_disable,
	.mac_addr_set           = tap_mac_set,
	.mtu_set                = tap_mtu_set,
	.set_mc_addr_list       = tap_set_mc_addr_list,
	.stats_get              = tap_stats_get,
	.stats_reset            = tap_stats_reset,
	.dev_supported_ptypes_get = tap_dev_supported_ptypes_get,
	.rss_hash_update        = tap_rss_hash_update,
	.flow_ops_get           = tap_dev_flow_ops_get,
};


tap_dev_flow_ops_get
	static const struct rte_flow_ops tap_flow_ops = {
		.validate = tap_flow_validate,
		.create = tap_flow_create,
			priv_flow_process
				rss_enable
					tap_flow_bpf_rss_map_create
						BPF_MAP_CREATE
		.destroy = tap_flow_destroy,
		.flush = tap_flow_flush,
		.isolate = tap_flow_isolate,
	};


/* BPF commands types */
enum bpf_cmd {
	BPF_MAP_CREATE,
	BPF_MAP_LOOKUP_ELEM,
	BPF_MAP_UPDATE_ELEM,
	BPF_MAP_DELETE_ELEM,
	BPF_MAP_GET_NEXT_KEY,
	BPF_PROG_LOAD,
};



RTE_PMD_REGISTER_VDEV(net_tap, pmd_tap_drv);
RTE_PMD_REGISTER_VDEV(net_tun, pmd_tun_drv);
rte_pmd_tun_probe
	eth_dev_tap_create



