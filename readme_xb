源码分析-大页初始化: https://zzqcn.github.io/opensource/dpdk/code-analysis/mem.html
UIO原理和流程简析: https://blog.csdn.net/ApeLife/article/details/100751359

git remote add upstream https://github.com/DPDK/dpdk
git fetch upstream
git merge upstream/master



mlx5driver:
driver source code: https://blog.csdn.net/leiyanjie8995/article/details/121341828
struct rte_eth_dev { -> 与每个以太网设备关联的通用数据结构。 指向突发数据包接收和发送功能的指针位于该结构的开头，以及指向特定设备的所有数据元素存储在共享内存中的位置的指针。 这种分割允许每个进程使用函数指针和驱动程序数据，而设备的实际配置数据是共享的。

与框架相关的比较重要的，收发报文的接口是rx_pkt_burst和tx_pkt_burst。还有与网卡相关的初始化、配置等接口都在eth_dev_ops里。还有网卡设备的私有数据，带有硬件相关的各项参数和数据，记录在rte_eth_dev_data结构里，包括网卡名称、收发队列个数及列表、mac地址等等
值得注意的是，为了representor的概念，mellanox在rte_eth_dev_data结构里添加了一个名为representor_id的参数，用作representor设备的id

mellanox的驱动在drivers/common/mlx5和drivers/net/mlx5目录下。common目录下是通用pcie相关，包括pcie驱动、与硬件交互的接口封装；net目录下是更上层的接口，包括eth设备、representor相关的一系列操作






Longest Prefix matching: lib\lpm\rte_lpm.h


rte_distributor_process -> 处理一组数据包并将其分发给worker


testpmd>


ptp presision timing protocol


查看大页: cat /proc/meminfo |grep -i huge

配置大页:
echo 16 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
hugepage:
大页信息初始化
eal_hugepage_info_init(void) -> 当我们初始化大页信息时，默认情况下所有内容都会转到套接字 0。 稍后将按内存初始化过程排序. mem：共享主要和次要的大页信息，因为我们需要在主要和辅助进程中映射大页，所以我们需要知道应该在哪里寻找hugetlbfs挂载点。 因此，与辅助进程共享这些，并将它们映射到 init 上
    eal_get_internal_configuration
    hugepage_info_init
        const char dirent_start_text[] = "hugepages-";
        dir = opendir(sys_dir_path) -> static const char sys_dir_path[] = "/sys/kernel/mm/hugepages";
        for (dirent = readdir(dir); dirent != NULL; dirent = readdir(dir))
            rte_str_to_size(&dirent->d_name[dirent_start_len]) -> 大页目录名转整数
            get_hugepage_dir
            get_num_hugepages
            calc_num_pages
    create_shared_memory(eal_hugepage_info_path(),
    memcpy
    munmap

读取 /sys/kernel/mm/hugepages 中的“hugepages-XXX”目录，最多读取 3个。比如读取到 hugepages-2048kB ，将其中的 2048kB 转换为2048*1024，存入internal_config.hugepage_info[num_sizes].hugepage_sz， num_sizes<3
打开 /proc/meminfo 文件，读取 Hugepagesize 项的值，做为大页默认大小。打开 /proc/mounts 文件，找到类似 hugetlbfs /dev/hugepages hugetlbfs rw,seclabel,relatime 0 0 或 nodev /mnt/huge hugetlbfs rw,relatime 0 0 的行，根据选项(rw,relatime)中出现的 pagesize= 项的值(如果有的话)，来返回对应的大页文件系统挂载路径，如 /dev/hugepages 或 /mnt/huge ，将其存入internal_config.hugepage_info[num_sizes].hugedir
锁定hugedir(flock)
打开 sys/kernel/mm/hugepages/hugepages-XXX 目录下面的 resv_hugepages 和 free_hugepages 文件，计算可用大页数量， 存入internal_config.hugepange_info->num_pages[0]，这个0是socket id，在支持NUMA的系统中先在socket 0上进行操作
internal_config.num_hugepage_sizes数设置为num_sizes数，不大于3
将上述过程发现的所有num_sizes个大页信息按从大到小排序，并检查至少有一个可用大页尺寸




大页内存初始化
rte_eal_memory_init
    rte_eal_memseg_init
    eal_memalloc_init
    rte_eal_hugepage_init rte_eal_hugepage_attach
        map_all_hugepages
            eal_get_hugefile_path(hf->filepath, sizeof(hf->filepath), -> // 拼接文件名 /dev/hugepages/rte_hugepage_%s
            ...
            fd = open(hf->filepath, O_CREAT | O_RDWR, 0600);
            ...
            virtaddr = mmap(NULL, hugepage_sz, PROT_READ | PROT_WRITE,
                            MAP_SHARED | MAP_POPULATE, fd, 0);





uio:
dpdk-devbind.py



receive pkt:
rte_eth_rx_burst
    eth_igb_recv_pkts
        if (! (staterr & rte_cpu_to_le_32(E1000_RXD_STAT_DD))) -> check dma dd flag



send pkt:
rte_eth_tx_burst
    tx_queues -> rte_eth_tx_burst



how huge page init?


//主进程创建/var/run/.rte_config文件
mem_cfg_fd = open(pathname, O_RDWR | O_CREAT, 0660);
//主进程映射/var/run/.rte_config到主进程空间
rte_mem_cfg_addr = mmap(rte_mem_cfg_addr, sizeof(*rte_config.mem_config),PROT_READ | PROT_WRITE, MAP_SHARED, mem_cfg_fd, 0);

static void rte_config_init(void)
{
    switch (rte_config.process_type)
    {
        case RTE_PROC_PRIMARY:
             //主进程创建共享内存配置
            rte_eal_config_create();
            break;
        case RTE_PROC_SECONDARY:
            //从进程打开共享内存配置后，映射到从进程自己的地址空间
            rte_eal_config_attach();
            //睡眠等待主进程设置完成共享内存配置
            rte_eal_mcfg_wait_complete(rte_config.mem_config);
            //从进程重新映射共享内存配置
            rte_eal_config_reattach();
    }
}

//从进程打开/var/run/.rte_config文件
mem_cfg_fd = open(pathname, O_RDWR);
//从进程将/var/run/.rte_config文件内容映射到从进程空间
rte_mem_cfg_addr = mmap(NULL, sizeof(*rte_config.mem_config),PROT_READ | PROT_WRITE, MAP_SHARED, mem_cfg_fd, 0

思考个问题，dpdk如何保证在主从进程模式下，物理地址相同，对应的主从进程的虚拟地址也相同呢？答案是主进程mmap映射后，主进程会将mmap映射后的虚拟地址放到共享内存中rte_config.mem_config。从进程会进行2次共享内存映射，第一次调用mmap进行映射时，第一个参数为空，表示由内核选择一个虚拟地址空间，从进程将会映射到这个由内核选择的虚拟地址空间中，此时从进程就可以从共享内存中获取到主进程mmap后的虚拟地址。之后从进程第二次调用mmap进行映射，传递的第一个参数不为空了，而是主进程mmap映射后的虚拟地址，相当于从进程直接从这个虚拟地址开始映射，从而保证了主从进程的虚拟地址空间一样，对应的物理空间也一样。主从进程的映射逻辑，都在rte_config_init函数中
原文链接：https://blog.csdn.net/ApeLife/article/details/99700882



rte_malloc


eth_igb_dev_init
read register or write:
E1000_PCI_REG_ADDR
E1000_READ_REG
E1000_WRITE_REG

e1000_init_nvm_ops_generic
e1000_init_mbx_ops_generic -> mailbox

cb:
rx_pkt_burst



core:
rte_flow_create


从 find_physaddr() 中提取 rte_mem_virt2phy()。 该函数允许获取映射到调用该函数的当前进程的任何虚拟地址的物理地址。 请注意，此函数非常慢，不应在初始化后调用，以避免性能瓶颈
#define RTE_BAD_PHYS_ADDR ((phys_addr_t)-1)
#define RTE_BAD_IOVA ((rte_iova_t)-1)

rte_mem_virt2phy(const void *virtaddr) -> 获取当前进程中任意映射虚拟地址的物理地址, 整理了DPDK中实现在用户态分配巨页和获取巨页物理地址的代码，可以作为参考，需要简化代码
    page_size = getpagesize()


rte_iova_t rte_mem_virt2iova(const void *virt);

内存缓冲区
struct rte_mbuf {
    ...
    buf_iova -> 段缓冲区的物理地址。 如果构建配置为仅使用虚拟地址作为 IOVA（即 RTE_IOVA_AS_PA 为 0），则该字段未定义。 强制对齐到 8 字节，以确保 32 位和 64 位具有完全相同的 mbuf cacheline0 布局。 这使得矢量驱动程序的工作变得更加容易. buf_iova 是给设备用的地址，在 iova_mod=PA 的情况，buf_iova 就是物理地址
    ...
}


rte_mempool_obj_iter(mp, rte_pktmbuf_init, NULL) -> mbuf：使用配置中的默认内存池处理程序，默认情况下，用于 mbuf 分配的内存池操作是多生产者和多消费者环。 我们可以想象一个提供硬件辅助池机制的目标（也许是一些网络处理器？）。 在这种情况下，该架构的默认配置将包含不同的 RTE_MBUF_DEFAULT_MEMPOOL_OPS 值
    rte_pktmbuf_priv_size
    rte_pktmbuf_data_room_size
    rte_mbuf_iova_set(m, rte_mempool_virt2iova(m) + mbuf_size) -> mbuf：添加帮助程序来获取/设置 IOVA 地址，添加 API rte_mbuf_iova_set 和 rte_mbuf_iova_get 分别用于设置和获取 mbuf 的物理地址。 更新了应用程序和库以使用相同的


rte_mempool_populate_default


struct hugepage_info {

test:
examples\helloworld\main.c
main(int argc, char **argv)
rte_eal_init
    rte_eal_using_phys_addrs -> eal：根据PA可用性计算IOVA模式，目前，如果总线选择IOVA作为PA，则在缺乏对物理地址的访问时，内存初始化可能会失败。 对于普通用户来说，这可能很难理解出了什么问题，因为这是默认行为。 通过验证物理地址可用性，在 eal init 中尽早发现这种情况，或者在没有表达明确的偏好时选择 IOVA。 总线代码已更改，以便它在不关心 IOVA 模式时进行报告，并让 eal init 决定。 在Linux实现中，重新设计rte_eal_using_phys_addrs()，以便可以更早地调用它，但仍然避免与rte_mem_virt2phys()的循环依赖。 在 FreeBSD 实现中，rte_eal_using_phys_addrs() 始终返回 false，因此检测部分保持原样。 如果编译了librte_kni并加载了KNI kmod， - 如果总线请求VA，如果物理地址可用，则强制使用PA，就像之前所做的那样， - 否则，将iova保留为VA，KNI init稍后将失败
    ...
    eal_hugepage_info_init
    ...
rte_eal_remote_launch(lcore_hello, NULL, lcore_id)
    lcore_id = rte_lcore_id()
    printf("hello from core %u\n", lcore_id)
lcore_hello
rte_eal_mp_wait_lcore
rte_eal_cleanup

./dpdk-helloworld -l 0-3 -n 4



user hugepage:
app\test-pmd\testpmd.c
setup_extmem
create_extmem
alloc_mem



eal_get_virtual_area

echo 16 >/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
cat /proc/meminfo |grep -i huge

helloworld -> examples/helloworld/main.c
main(int argc, char **argv)
rte_eal_init(int argc, char **argv)
    rte_eal_get_configuration
    eal_get_internal_configuration
    rte_cpu_is_supported
        RTE_COMPILE_TIME_CPUFLAGS -> #define RTE_COMPILE_TIME_CPUFLAGS RTE_CPUFLAG_SSE,RTE_CPUFLAG_SSE2,RTE_CPUFLAG_SSE3,RTE_CPUFLAG_SSSE3,RTE_CPUFLAG_SSE4_1,RTE_CPUFLAG_SSE4_2,RTE_CPUFLAG_AES,RTE_CPUFLAG_AVX,RTE_CPUFLAG_AVX2,RTE_CPUFLAG_AVX512BW,RTE_CPUFLAG_AVX512CD,RTE_CPUFLAG_AVX512DQ,RTE_CPUFLAG_AVX512F,RTE_CPUFLAG_AVX512VL,RTE_CPUFLAG_PCLMULQDQ,RTE_CPUFLAG_RDRAND,RTE_CPUFLAG_RDSEED
        rte_cpu_get_flag_enabled
            rte_cpu_feature_table
            __get_cpuid_max(feat->leaf & 0x80000000, NULL)
            const struct feature_entry rte_cpu_feature_table[]
    __atomic_compare_exchange_n
    eal_reset_internal_config
        internal_cfg->iova_mode = RTE_IOVA_DC -> default memory mode
    eal_log_level_parse -> set_log demo: ./app/test-pmd --log-level='pmd\.i40e.*,8' -> 现在正确设置 --log-level=7 不会打印来自 rte_eal_cpu_init() 例程的消息
        eal_parse_common_option
            eal_service_cores_parsed
            rte_eal_parse_coremask
            ...
    eal_save_args -> Connecting to /var/run/dpdk/rte/dpdk_telemetry.v2
        handle_eal_info_request
            rte_tel_data_start_array
            rte_tel_data_add_array_string
    eal_create_runtime_dir
    eal_adjust_config
    rte_eal_cpu_init -> eal：不要对CPU检测感到恐慌，可能没有办法优雅地恢复，但是应该通知应用程序发生了故障，而不是完全中止。 这允许用户继续使用“慢路径”类型的解决方案。 进行此更改后，EAL CPU NUMA 节点解析步骤不再发出 rte_panic。 这与 rte_eal_init 中的代码一致，该代码期望失败返回错误代码 -> 使用物理和逻辑处理器的数量填充配置 此函数是 EAL 专用的。 解析 /proc/cpuinfo 以获取计算机上的物理和逻辑处理器的数量, /sys/devices/system/cpu
        eal_cpu_socket_id

    eal_parse_args
        eal_usage
            eal_common_usage
        eal_check_common_options
    eal_plugins_init
    ...
    rte_config_init
    rte_eal_using_phys_addrs
        rte_eal_has_hugepages -> no_hugetlbfs -> default use hugepage
        rte_mem_virt2phy
            return RTE_BAD_IOVA
    rte_bus_get_iommu_class
    if (internal_conf->no_hugetlbfs == 0)
        hugepage_info_init
        create_shared_memory
    ...
    RTE_LOG(DEBUG, EAL, "IOMMU is not available, selecting IOVA as PA mode.\n")
    rte_eal_get_configuration
    eal_hugepage_info_init
    eal_log_init
    rte_eal_vfio_setup
    rte_eal_memzone_init
    eal_hugedirs_unlock
    rte_eal_malloc_heap_init
    rte_eal_tailqs_init
    rte_eal_timer_init
    eal_check_mem_on_local_socket
    rte_thread_set_affinity_by_id
    eal_thread_dump_current_affinity
    RTE_LCORE_FOREACH_WORKER(i)
        eal_worker_thread_create(i)
    rte_eal_mp_remote_launch sync_func
    rte_eal_mp_wait_lcore
    rte_service_init
	if (rte_bus_probe())
    rte_vfio_is_enabled
    rte_service_start_with_defaults
    eal_clean_runtime_dir
    rte_log_register_type_and_pick_level
    rte_telemetry_init
    eal_mcfg_complete
rte_eal_remote_launch(lcore_hello, NULL, lcore_id)
lcore_hello


map_all_hugepages


args/option:
eal_usage
eal_common_usage(void)
{
	printf("[options]\n\n"
	       "EAL common options:\n"
	       "  -c COREMASK         Hexadecimal bitmask of cores to run on\n"
	       "  -l CORELIST         List of cores to run on\n"
	       "                      The argument format is <c1>[-c2][,c3[-c4],...]\n"
	       "                      where c1, c2, etc are core indexes between 0 and %d\n"
	       "  --"OPT_LCORES" COREMAP    Map lcore set to physical cpu set\n"
	       "                      The argument format is\n"
	       "                            '<lcores[@cpus]>[<,lcores[@cpus]>...]'\n"
	       "                      lcores and cpus list are grouped by '(' and ')'\n"
	       "                      Within the group, '-' is used for range separator,\n"
	       "                      ',' is used for single number separator.\n"
	       "                      '( )' can be omitted for single element group,\n"
	       "                      '@' can be omitted if cpus and lcores have the same value\n"
	       "  -s SERVICE COREMASK Hexadecimal bitmask of cores to be used as service cores\n"
	       "  --"OPT_MAIN_LCORE" ID     Core ID that is used as main\n"
	       "  --"OPT_MBUF_POOL_OPS_NAME" Pool ops name for mbuf to use\n"
	       "  -n CHANNELS         Number of memory channels\n" -> 内存通道是内存单元和 CPU 之间用于数据移动的走线。 内存通道的数量充当以更快的速率传输数据的路径。 对于 OVS-DPDK，参数 OVSDpdkMemoryChannels 保存活动使用的通道数, 内存通道 获取正确的内存通道数（-n 参数）很棘手，因为它取决于系统主板支持的通道数、内存芯片的数量和类型以及它们在系统中的物理安装方式，并且没有简单或简单的方法 甚至可以通过可靠的方式来判断正在运行的系统。 此信息应该在 BIOS 内存检查阶段的启动期间可用，在运行时 dmidecode 可以帮助至少做出有根据的猜测。 除非已经安装，否则您现在需要这样做： # yum install dmidecode 这通常会提供足够的信息来查找系统和/或主板手册，以了解主板支持什么以及在哪些配置中内存插槽数量 对于多通道支持至关重要： # dmidecode -t system # dmidecode -t baseboard 这会输出系统上已填充的内存插槽： # dmidecode -t memory | grep 'Size: [0-9]' 如果只有一个，则不能使用多通道内存。 如果有两个或其倍数，则可能是双通道，如果有三个或其倍数，则可能是三通道，如果有四个或其倍数，则可能是四通道。 或者双通道...此外，内存设备的定位器字段中可能还有进一步的提示，例如 ChannelA-DIMM0 和 ChannelB-DIMM0： grep 定位器：
	       "  -m MB               Memory to allocate (see also --"OPT_SOCKET_MEM")\n"
	       "  -r RANKS            Force number of memory ranks (don't detect)\n"
	       "  -b, --block         Add a device to the blocked list.\n"
	       "                      Prevent EAL from using this device. The argument\n"
	       "                      format for PCI devices is <domain:bus:devid.func>.\n"
	       "  -a, --allow         Add a device to the allow list.\n"
	       "                      Only use the specified devices. The argument format\n"
	       "                      for PCI devices is <[domain:]bus:devid.func>.\n"
	       "                      This option can be present several times.\n"
	       "                      [NOTE: " OPT_DEV_ALLOW " cannot be used with "OPT_DEV_BLOCK" option]\n"
	       "  --"OPT_VDEV"              Add a virtual device.\n"
	       "                      The argument format is <driver><id>[,key=val,...]\n"
	       "                      (ex: --vdev=net_pcap0,iface=eth2).\n"
	       "  --"OPT_IOVA_MODE"   Set IOVA mode. 'pa' for IOVA_PA\n"
	       "                      'va' for IOVA_VA\n"
	       "  -d LIB.so|DIR       Add a driver or driver directory\n"
	       "                      (can be used multiple times)\n"
	       "  --"OPT_VMWARE_TSC_MAP"    Use VMware TSC map instead of native RDTSC\n"
	       "  --"OPT_PROC_TYPE"         Type of this process (primary|secondary|auto)\n"
#ifndef RTE_EXEC_ENV_WINDOWS
	       "  --"OPT_SYSLOG"            Set syslog facility\n"
#endif
	       "  --"OPT_LOG_LEVEL"=<level> Set global log level\n"
	       "  --"OPT_LOG_LEVEL"=<type-match>:<level>\n"
	       "                      Set specific log level\n"
	       "  --"OPT_LOG_LEVEL"=help    Show log types and levels\n"
#ifndef RTE_EXEC_ENV_WINDOWS
	       "  --"OPT_TRACE"=<regex-match>\n"
	       "                      Enable trace based on regular expression trace name.\n"
	       "                      By default, the trace is disabled.\n"
	       "		      User must specify this option to enable trace.\n"
	       "  --"OPT_TRACE_DIR"=<directory path>\n"
	       "                      Specify trace directory for trace output.\n"
	       "                      By default, trace output will created at\n"
	       "                      $HOME directory and parameter must be\n"
	       "                      specified once only.\n"
	       "  --"OPT_TRACE_BUF_SIZE"=<int>\n"
	       "                      Specify maximum size of allocated memory\n"
	       "                      for trace output for each thread. Valid\n"
	       "                      unit can be either 'B|K|M' for 'Bytes',\n"
	       "                      'KBytes' and 'MBytes' respectively.\n"
	       "                      Default is 1MB and parameter must be\n"
	       "                      specified once only.\n"
	       "  --"OPT_TRACE_MODE"=<o[verwrite] | d[iscard]>\n"
	       "                      Specify the mode of update of trace\n"
	       "                      output file. Either update on a file can\n"
	       "                      be wrapped or discarded when file size\n"
	       "                      reaches its maximum limit.\n"
	       "                      Default mode is 'overwrite' and parameter\n"
	       "                      must be specified once only.\n"
#endif /* !RTE_EXEC_ENV_WINDOWS */
	       "  -v                  Display version information on startup\n"
	       "  -h, --help          This help\n"
	       "  --"OPT_IN_MEMORY"   Operate entirely in memory. This will\n"
	       "                      disable secondary process support\n"
	       "  --"OPT_BASE_VIRTADDR"     Base virtual address\n"
	       "  --"OPT_TELEMETRY"   Enable telemetry support (on by default)\n"
	       "  --"OPT_NO_TELEMETRY"   Disable telemetry support\n"
	       "  --"OPT_FORCE_MAX_SIMD_BITWIDTH" Force the max SIMD bitwidth\n"
	       "\nEAL options for DEBUG use only:\n"
	       "  --"OPT_HUGE_UNLINK"[=existing|always|never]\n"
	       "                      When to unlink files in hugetlbfs\n"
	       "                      ('existing' by default, no value means 'always')\n"
	       "  --"OPT_NO_HUGE"           Use malloc instead of hugetlbfs\n"
	       "  --"OPT_NO_PCI"            Disable PCI\n"
	       "  --"OPT_NO_HPET"           Disable HPET\n"
	       "  --"OPT_NO_SHCONF"         No shared config (mmap'd files)\n"
	       "\n", RTE_MAX_LCORE);
}



CPU特性
const struct feature_entry rte_cpu_feature_table[] = {
	FEAT_DEF(SSE3, 0x00000001, 0, RTE_REG_ECX,  0)
	FEAT_DEF(PCLMULQDQ, 0x00000001, 0, RTE_REG_ECX,  1)
	FEAT_DEF(DTES64, 0x00000001, 0, RTE_REG_ECX,  2)
	FEAT_DEF(MONITOR, 0x00000001, 0, RTE_REG_ECX,  3)
	FEAT_DEF(DS_CPL, 0x00000001, 0, RTE_REG_ECX,  4)
	FEAT_DEF(VMX, 0x00000001, 0, RTE_REG_ECX,  5)
	FEAT_DEF(SMX, 0x00000001, 0, RTE_REG_ECX,  6)
	FEAT_DEF(EIST, 0x00000001, 0, RTE_REG_ECX,  7)
	FEAT_DEF(TM2, 0x00000001, 0, RTE_REG_ECX,  8)
	FEAT_DEF(SSSE3, 0x00000001, 0, RTE_REG_ECX,  9)
	FEAT_DEF(CNXT_ID, 0x00000001, 0, RTE_REG_ECX, 10)
	FEAT_DEF(FMA, 0x00000001, 0, RTE_REG_ECX, 12)
	FEAT_DEF(CMPXCHG16B, 0x00000001, 0, RTE_REG_ECX, 13)
	FEAT_DEF(XTPR, 0x00000001, 0, RTE_REG_ECX, 14)
	FEAT_DEF(PDCM, 0x00000001, 0, RTE_REG_ECX, 15)
	FEAT_DEF(PCID, 0x00000001, 0, RTE_REG_ECX, 17)
	FEAT_DEF(DCA, 0x00000001, 0, RTE_REG_ECX, 18)
	FEAT_DEF(SSE4_1, 0x00000001, 0, RTE_REG_ECX, 19)
	FEAT_DEF(SSE4_2, 0x00000001, 0, RTE_REG_ECX, 20)
	FEAT_DEF(X2APIC, 0x00000001, 0, RTE_REG_ECX, 21)
	FEAT_DEF(MOVBE, 0x00000001, 0, RTE_REG_ECX, 22)
	FEAT_DEF(POPCNT, 0x00000001, 0, RTE_REG_ECX, 23)
	FEAT_DEF(TSC_DEADLINE, 0x00000001, 0, RTE_REG_ECX, 24)
	FEAT_DEF(AES, 0x00000001, 0, RTE_REG_ECX, 25)
	FEAT_DEF(XSAVE, 0x00000001, 0, RTE_REG_ECX, 26)
	FEAT_DEF(OSXSAVE, 0x00000001, 0, RTE_REG_ECX, 27)
	FEAT_DEF(AVX, 0x00000001, 0, RTE_REG_ECX, 28)
	FEAT_DEF(F16C, 0x00000001, 0, RTE_REG_ECX, 29)
	FEAT_DEF(RDRAND, 0x00000001, 0, RTE_REG_ECX, 30)
	FEAT_DEF(HYPERVISOR, 0x00000001, 0, RTE_REG_ECX, 31)

	FEAT_DEF(FPU, 0x00000001, 0, RTE_REG_EDX,  0)
	FEAT_DEF(VME, 0x00000001, 0, RTE_REG_EDX,  1)
	FEAT_DEF(DE, 0x00000001, 0, RTE_REG_EDX,  2)
	FEAT_DEF(PSE, 0x00000001, 0, RTE_REG_EDX,  3)
	FEAT_DEF(TSC, 0x00000001, 0, RTE_REG_EDX,  4)
	FEAT_DEF(MSR, 0x00000001, 0, RTE_REG_EDX,  5)
	FEAT_DEF(PAE, 0x00000001, 0, RTE_REG_EDX,  6)
	FEAT_DEF(MCE, 0x00000001, 0, RTE_REG_EDX,  7)
	FEAT_DEF(CX8, 0x00000001, 0, RTE_REG_EDX,  8)
	FEAT_DEF(APIC, 0x00000001, 0, RTE_REG_EDX,  9)
	FEAT_DEF(SEP, 0x00000001, 0, RTE_REG_EDX, 11)
	FEAT_DEF(MTRR, 0x00000001, 0, RTE_REG_EDX, 12)
	FEAT_DEF(PGE, 0x00000001, 0, RTE_REG_EDX, 13)
	FEAT_DEF(MCA, 0x00000001, 0, RTE_REG_EDX, 14)
	FEAT_DEF(CMOV, 0x00000001, 0, RTE_REG_EDX, 15)
	FEAT_DEF(PAT, 0x00000001, 0, RTE_REG_EDX, 16)
	FEAT_DEF(PSE36, 0x00000001, 0, RTE_REG_EDX, 17)
	FEAT_DEF(PSN, 0x00000001, 0, RTE_REG_EDX, 18)
	FEAT_DEF(CLFSH, 0x00000001, 0, RTE_REG_EDX, 19)
	FEAT_DEF(DS, 0x00000001, 0, RTE_REG_EDX, 21)
	FEAT_DEF(ACPI, 0x00000001, 0, RTE_REG_EDX, 22)
	FEAT_DEF(MMX, 0x00000001, 0, RTE_REG_EDX, 23)
	FEAT_DEF(FXSR, 0x00000001, 0, RTE_REG_EDX, 24)
	FEAT_DEF(SSE, 0x00000001, 0, RTE_REG_EDX, 25)
	FEAT_DEF(SSE2, 0x00000001, 0, RTE_REG_EDX, 26)
	FEAT_DEF(SS, 0x00000001, 0, RTE_REG_EDX, 27)
	FEAT_DEF(HTT, 0x00000001, 0, RTE_REG_EDX, 28)
	FEAT_DEF(TM, 0x00000001, 0, RTE_REG_EDX, 29)
	FEAT_DEF(PBE, 0x00000001, 0, RTE_REG_EDX, 31)

	FEAT_DEF(DIGTEMP, 0x00000006, 0, RTE_REG_EAX,  0)
	FEAT_DEF(TRBOBST, 0x00000006, 0, RTE_REG_EAX,  1)
	FEAT_DEF(ARAT, 0x00000006, 0, RTE_REG_EAX,  2)
	FEAT_DEF(PLN, 0x00000006, 0, RTE_REG_EAX,  4)
	FEAT_DEF(ECMD, 0x00000006, 0, RTE_REG_EAX,  5)
	FEAT_DEF(PTM, 0x00000006, 0, RTE_REG_EAX,  6)

	FEAT_DEF(MPERF_APERF_MSR, 0x00000006, 0, RTE_REG_ECX,  0)
	FEAT_DEF(ACNT2, 0x00000006, 0, RTE_REG_ECX,  1)
	FEAT_DEF(ENERGY_EFF, 0x00000006, 0, RTE_REG_ECX,  3)

	FEAT_DEF(FSGSBASE, 0x00000007, 0, RTE_REG_EBX,  0)
	FEAT_DEF(BMI1, 0x00000007, 0, RTE_REG_EBX,  3)
	FEAT_DEF(HLE, 0x00000007, 0, RTE_REG_EBX,  4)
	FEAT_DEF(AVX2, 0x00000007, 0, RTE_REG_EBX,  5)
	FEAT_DEF(SMEP, 0x00000007, 0, RTE_REG_EBX,  7)
	FEAT_DEF(BMI2, 0x00000007, 0, RTE_REG_EBX,  8)
	FEAT_DEF(ERMS, 0x00000007, 0, RTE_REG_EBX,  9)
	FEAT_DEF(INVPCID, 0x00000007, 0, RTE_REG_EBX, 10)
	FEAT_DEF(RTM, 0x00000007, 0, RTE_REG_EBX, 11)
	FEAT_DEF(AVX512F, 0x00000007, 0, RTE_REG_EBX, 16)
	FEAT_DEF(AVX512DQ, 0x00000007, 0, RTE_REG_EBX, 17)
	FEAT_DEF(RDSEED, 0x00000007, 0, RTE_REG_EBX, 18)
	FEAT_DEF(AVX512IFMA, 0x00000007, 0, RTE_REG_EBX, 21)
	FEAT_DEF(AVX512CD, 0x00000007, 0, RTE_REG_EBX, 28)
	FEAT_DEF(AVX512BW, 0x00000007, 0, RTE_REG_EBX, 30)
	FEAT_DEF(AVX512VL, 0x00000007, 0, RTE_REG_EBX, 31)

	FEAT_DEF(AVX512VBMI, 0x00000007, 0, RTE_REG_ECX,  1)
	FEAT_DEF(WAITPKG, 0x00000007, 0, RTE_REG_ECX,  5)
	FEAT_DEF(AVX512VBMI2, 0x00000007, 0, RTE_REG_ECX,  6)
	FEAT_DEF(GFNI, 0x00000007, 0, RTE_REG_ECX,  8)
	FEAT_DEF(VAES, 0x00000007, 0, RTE_REG_ECX,  9)
	FEAT_DEF(VPCLMULQDQ, 0x00000007, 0, RTE_REG_ECX, 10)
	FEAT_DEF(AVX512VNNI, 0x00000007, 0, RTE_REG_ECX, 11)
	FEAT_DEF(AVX512BITALG, 0x00000007, 0, RTE_REG_ECX, 12)
	FEAT_DEF(AVX512VPOPCNTDQ, 0x00000007, 0, RTE_REG_ECX, 14)
	FEAT_DEF(CLDEMOTE, 0x00000007, 0, RTE_REG_ECX, 25)
	FEAT_DEF(MOVDIRI, 0x00000007, 0, RTE_REG_ECX, 27)
	FEAT_DEF(MOVDIR64B, 0x00000007, 0, RTE_REG_ECX, 28)

	FEAT_DEF(AVX512VP2INTERSECT, 0x00000007, 0, RTE_REG_EDX,  8)

	FEAT_DEF(LAHF_SAHF, 0x80000001, 0, RTE_REG_ECX,  0)
	FEAT_DEF(LZCNT, 0x80000001, 0, RTE_REG_ECX,  4)

	FEAT_DEF(SYSCALL, 0x80000001, 0, RTE_REG_EDX, 11)
	FEAT_DEF(XD, 0x80000001, 0, RTE_REG_EDX, 20)
	FEAT_DEF(1GB_PG, 0x80000001, 0, RTE_REG_EDX, 26)
	FEAT_DEF(RDTSCP, 0x80000001, 0, RTE_REG_EDX, 27)
	FEAT_DEF(EM64T, 0x80000001, 0, RTE_REG_EDX, 29)

	FEAT_DEF(INVTSC, 0x80000007, 0, RTE_REG_EDX,  8)
};

